{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ElasticSearch 연동 및 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Wilshere_Elasticsearch',\n",
       " 'cluster_name': 'Wilshere_ES',\n",
       " 'cluster_uuid': 'e8ZntsY7R7CEZnj8Odv-Rw',\n",
       " 'version': {'number': '7.12.1',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'tar',\n",
       "  'build_hash': '3186837139b9c6b6d23c3200870651f10d3343b7',\n",
       "  'build_date': '2021-04-20T20:56:39.040728659Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.8.0',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = Elasticsearch('http://13.209.84.139:9200')\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ElasticSearch 에 데이터 적재 및 색인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-1). ElasticSearch 에 index 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 색인화하기 위한 함수\n",
    "def indexing(es, index_name):\n",
    "    # 이미 존재할 경우 삭제하고 다시 만들기\n",
    "    if es.indices.exists(index=index_name):\n",
    "        es.indices.delete(index=index_name)\n",
    "\n",
    "    # 인덱스 생성\n",
    "    print(es.indices.create(index=index_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'en_wiki_5'}\n"
     ]
    }
   ],
   "source": [
    "# 인덱스명을 정하기 (자유롭게)\n",
    "index_name = 'en_wiki_5'\n",
    "#index_name = 'temp1'\n",
    "indexing(es, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 생성 : en_wiki_1~5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-2_. ElasticSearch 에 데이터 적재(인덱싱)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "./en_wiki_dump/enwiki-20210520_1_result/AA~AD || AA/AB/AC/AD\n",
    "./en_wiki_dump/enwiki-20210520_2_result/AA~AF || AA/AB/AC/AD/AE/AF\n",
    "./en_wiki_dump/enwiki-20210520_3_result/AA~AE || AA/AB/AC/AD/AE\n",
    "./en_wiki_dump/enwiki-20210520_4_result/AA~AF || AA/AB/AC/AD/AE/AF\n",
    "./en_wiki_dump/enwiki-20210520_5_result/AA~AF || AA/AB/AC/AD/AE/AF\n",
    "여기까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'total': 2, 'successful': 1, 'failed': 0}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import json, codecs\n",
    "import xmltodict\n",
    "from collections import OrderedDict\n",
    "\n",
    "path = './en_wiki_dump/enwiki-20210520_5_result/AF/'\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "for i in range(len(file_list)):\n",
    "    fr = open(path + file_list[i],'r', encoding=\"utf-8\")\n",
    "    data = fr.readlines()\n",
    "    \n",
    "    for j in range(len(data)):\n",
    "        es.index(index=index_name, doc_type='string', body=data[j])\n",
    "\n",
    "    fr.close()\n",
    "es.indices.refresh(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Elasticsearch 데이터 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"en_wiki*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 11.89215 source: {'id': '145569', 'revid': '39662122', 'url': 'https://en.wikipedia.org/wiki?curid=145569', 'title': 'Kenya Airways', 'text': 'Kenya Airways Ltd., more commonly known as Kenya Airways, is the flag carrier airline of Kenya. The company was founded in 1977, after the dissolution of East African Airways. Its head office is located in Embakasi, Nairobi, with its hub at Jomo Kenyatta International Airport.\\nThe airline was owned by the Government of Kenya until , and it was privatised in 1996, becoming the first African flag carrier to successfully do so. Kenya Airways is currently a public-private partnership. The largest shareholder is the Government of Kenya (48.9%), 38.1% is owned by KQ Lenders Company 2017 Ltd. (in turn owned by a consortium of banks), followed by KLM, which has a 7.8% stake in the company. The rest of the shares are held by private owners; shares are traded on the Nairobi Stock Exchange, the Dar es Salaam Stock Exchange, and the Uganda Securities Exchange.\\nThe airline became a member of SkyTeam in , and is also a member of the African Airlines Association since 1977.\\nHistory.\\nEarly years.\\nKenya Airways was established by the Kenyan government on , following the break-up of the East African Community and the consequent demise of East African Airways (EAA). On , two Boeing 707-321s leased from British Midland Airways inaugurated operations, serving the Nairobi–Frankfurt–London route. On internal and regional flights, the carrier deployed aircraft formerly operated by the EAA consortium, such as one Douglas DC-9-52 and three Fokker F-27-200s. In late 1977, three Boeing 707s were acquired from Northwest Orient. The following year, the company formed a charter subsidiary named Kenya Flamingo Airlines, which leased aircraft from the parent airline in order to operate international passenger and cargo services. Aer Lingus provided the company with technical and management support in the early years.\\n1980s–1990s: Expansion and privatisation.\\nIn the airline had 2,100 employees and a fleet of three Boeing 707-320Bs, one Boeing 720B, one DC-9-30 and three Fokker F-27-200s. At this time, Addis Ababa, Athens, Bombay, Cairo, Copenhagen, Frankfurt, Jeddah, Kampala, Karachi, Khartoum, London, Lusaka, Mauritius, Mogadishu, Rome, Salisbury, Seychelles and Zurich were among the airline\\'s international destinations, whereas domestic routes radiated from Nairobi to Kisumu, Malindi, Mombasa and Mumias. A Nairobi–Bombay nonstop route was launched in 1982 using Boeing 707-320Bs. A year later, the company commenced serving Tanzania. Flights to Burundi, Malawi and Rwanda were launched in 1984. Capacity on the European routes was boosted in with the incorporation of an Airbus A310-200 leased from Condor. Kilimanjaro was first served in . That year, the airline ordered two Airbus A310-300s. Kenya Airways was the first African carrier to acquire the type, and they were the first wide-bodies ordered by the company. Funded with a loan, the delivery of these two aircraft took place in and . They flew on the Kenya–Europe corridor, and permitted Kenya Airways to return the A310-200 to the lessor. In early 1988, the carrier ordered two Fokker 50s; for domestic routes, the airline received the first of these aircraft at the end of the year. Also in 1988, the lease of a third A310-300 was arranged with the International Lease Finance for a ten-year period; the aircraft joined the fleet in . Leased from Ansett Worldwide, the first Boeing 757-200 was received in , whereas a third Fokker 50 was acquired in the same year. By late 1991, two Boeing 737-200s had been leased from Guinness Peat Aviation.\\nIn 1986, \"Sessional Paper Number 1\" was published by the Government of Kenya, outlining the country\\'s need for economic development and growth. The document stressed the government opinion that the airline would be better off privately owned, thus resulting in the first privatisation attempt. The government named Philip Ndegwa as chairman of the board in 1991, with specific orders to make the airline a privately owned company. In 1992, the \"Public Enterprise Reform\" paper was published, giving Kenya Airways priority among national companies in Kenya to be privatised. Ndegwa was succeeded by Isaac Omolo Okero. In , Brian Davies, was appointed as the new managing director of the company. Davies had been previously hired to carry out a study of viability on privatisation, working for British Airways\\' Speedwing consulting arm. Swissair was the first company to provide Kenya Airways with privatisation advice. In the fiscal year 1993 to 1994, the airline produced its first profit since the start of commercialisation. In 1994, the International Finance Corporation was appointed to provide assistance in the privatisation process, which effectively began in 1995. A large aviation industry partner was sought to acquire 40% of the shares, with another 40% reserved for private investors and the government keeping the remaining stake. The government would absorb almost \\xa0million in debts and would convert another \\xa0million it provided in loans into equity; after reorganisation, the company would have a debt of approximate \\xa0million. British Airways, KLM, Lufthansa and South African Airways were among the airlines expressing interest in taking a stake in Kenya Airways.\\nKLM was awarded the privatisation of the company, which restructured its debts and made a master corporation agreement with KLM, which bought 26% of the shares, becoming the largest single shareholder since then. Shares were floated to the public in , and the airline started trading on the Nairobi Stock Exchange. The Government of Kenya kept a 23% stake in the company, and offered the remaining 51% to the public; however, non-Kenyan shareholders could hold a maximum 49% share of the airline. Despite 40% of the shares being kept by foreign investors following privatisation (including KLM 26% stake), top management positions were held by Kenyans. Following the takeover, the government of Kenya capitalised \\xa0million, while the airline was awarded a \\xa0million loan from the International Finance Corporation to modernise its fleet. In a deal worth \\xa0million, two Boeing 737-300s were ordered in .\\n2000s–2010s.\\nIn , the airline experienced its first fatal accident when an Airbus A310 that had been bought new in 1986 crashed off Ivory Coast, shortly after taking off from Abidjan. By the same year, the fleet consisted of four Airbus A310-300s, two Boeing 737-200 Advanced and four Boeing 737-300s. At this time the company had a staff of 2,780, including 400 engineers, 146 flight crew and 365 cabin crew. From its main hub at Jomo Kenyatta International Airport, scheduled services were operated to Abidjan, Addis Ababa, Amsterdam, Bujumbura, Cairo, Copenhagen, Dar es Salaam, Douala, Dubai, Eldoret, Entebbe/Kampala, Harare, Johannesburg, Karachi, Khartoum, Kigali, Kinshasa, Lagos, Lilongwe, Lokichoggio, London, Lusaka, Mahe Island, Malindi, Mombasa, Mumbai, and Zanzibar. In 2002, an order for Boeing 777-200ERs was placed with Boeing; an additional aircraft of the type was acquired in . In , Boeing 787-8s were ordered; the first two examples would be delivered in and the rest in . The original Boeing 787 order was amended months later to include more aircraft of the type. The first Embraer 190 joined the fleet in December 2010.\\nIn the company announced the issuance of rights worth KSh20\\xa0billion, aimed at increasing capital to support expansion plans. Following the allocation of shares, KLM increased their stake in the company from 26% to 26.73%, while the Kenyan government boosted their participation into the company from 23% to 29.8%, becoming the largest shareholder. In , the airline launched a plan named \"Project Mawingu\" (the Swahili word meaning \"Clouds\") to add 24 destinations by 2021, including the start of services to Australia and North and South America, and expanding its presence in Asia as well. In , the airline stated that it will add six new destinations every year, following the delivery of Boeing 777s and 787s the carrier has on order.\\nOperational results for fiscal years 2015 and 2016 showed substantial losses. The rapid expansion of the fleet and routes (dubbed \"Project Mawingu\") was cited as the primary cause of the downturn. Fuel-price hedging and the 1996 agreement with KLM, considered intrusive in the running of the flag carrier, took secondary blame. Corrective measures were taken to improve the financial and operational position of the airline and avert insolvency. \\nThe route partnership with KLM was deemed profitable thus, kept. However, the parties agreed to amend some features of the deal that had a negative effect on KQ -IATA code for Kenya Airways. Two Boeing B737-700 were sold and five newer, leased airliners were sub-leased to improve cash flow. Efforts to financially re-position the carrier were successful at the end of 2017. In a complex deal, stakeholders agreed to convert close to half a billion US dollars in loans to equity, changing the ownership structure. The government of Kenya, the biggest lender, saw its holdings rise from 29.8% to 48.9% while that of KLM was diluted from 26.7% down to 7.8%. A consortium of local banks, through a special-purpose vehicle called: \"KQ Lenders Company 2017 Ltd.\", ended up with 38.1%. The latter entity is obligated with a loan from the above local banks in the amount of US$225 million; this amount, in turn, is guaranteed by the government. The airline\\'s employees, through a shareholding scheme, and others own the remaining 5.2%. The Government of Kenya issued a guarantee for a further US$525\\xa0million debt owed to Import-Export Bank of the United States, financier of the newer Boeing planes of its fleet. \\nIn a bid to recover their exposure, syndicated leaseholders and banks unsuccessfully fought these measures to restructure the carrier\\'s ownership.\\nAn outline of a plan to restore profitability was disclosed in a March 2018 interview given by the CEO and the chairman of the company. The turnaround operation will include route expansion, pursuing the high-end segment of the market and, on partnerships and joint ventures with other airlines. The carrier plans to add up to twenty new destinations in Africa, Europe and Asia in the next five years. Five sub-leased aircraft are to re-join the fleet by the end of 2019 to facilitate this move. Preparations are underway to roll out an economy-plus class to target the business and high-end leisure travelers. Direct flights to luxury-tourism destinations in the Indian Ocean are also planned. Talks are underway with South African Airways regarding route-sharing and aircraft-maintenance collaboration; this is the other focus of the turnaround scheme. In December 2018 Kenya Airways revealed plans to start flights between Nairobi and Windhoek, Namibia.\\nCorporate affairs.\\nSubsidiaries and associates.\\nLow-cost carrier Jambojet, created in 2013, and African Cargo Handling Limited are both wholly owned subsidiaries of Kenya Airways.\\nPartly owned companies include Kenya Airfreight Handling Limited, dedicated to the cargo handling of perishable goods (51%-owned) and Tanzanian carrier Precision Air (41.23%-owned).\\nBusiness trends.\\nThe key trends for the Kenya Airways group over recent years are shown below (to 31 March until 2017; periods ending 31 December thereafter):\\nKey people.\\n, Michael Joseph is the airline chairman. Joseph is the former CEO of Safaricom, the leading telecom operator in Kenya.\\n, Sebastian Mikosz became Kenya Airways Group\\'s managing director and chief executive officer (CEO). Mikosz was formerly CEO of LOT Polish Airlines, and took office on 1 June 2017.\\n, Allan Kilavuka was appointed as Kenya Airways Group\\'s Acting Chief Executive Officer. He was subsequently confirmed in the substantive role.\\nDestinations.\\nKenya Airways serves 53 destinations in 41 countries, .\\nAlliances.\\nKLM sponsored Kenya Airways\\' SkyTeam candidacy process in mid-2005. In , Kenya Airways became one of the first official SkyTeam Associate Airline and achieved full membership in . The alliance provides Kenya Airways\\' passengers with access to the member airlines\\' worldwide network and passenger facilities.\\nCodeshare agreements.\\nKenya Airways has codeshare agreements with the following airlines:\\nFleet.\\nCurrent fleet.\\n, the Kenya Airways fleet consists of the following aircraft:\\nRecent developments and future plans.\\nThe first of four converted Boeing 737-300s was delivered to the company in ; Kenya Airways planned to fly this aircraft on African routes served by the Embraer 190s, in order to boost cargo capacity. The company took delivery of its first Boeing 777-300ER in October 2013.\\nKenya Airways had nine Boeing 787 Dreamliners on order as of April 2011, although the company considered cancelling the order after systematic delays with the delivery dates. The handover of the first Boeing 787 took place on 4 April 2014. Two days later, Nairobi–Paris became the first route to be served by the Boeing 787.\\nKenya Airways phased out its Boeing 777s in May 2015 after the airline made losses and incurred debts in the previous financial year. The Boeing 777-300ER fleet was leased to Turkish Airlines in May 2016.\\nHistorical fleet.\\nThe company has previously operated the following aircraft:\\nLivery.\\nIn 2005, Kenya Airways changed its livery. The four stripes running all through the length of the fuselage were replaced by the company slogan \"Pride of Africa\", whereas the \"KA\" tail logo was replaced by a styled \"K\" encircled with a \"Q\" to evoke the airline\\'s IATA airline code.\\nServices.\\nFrequent flyer programmes.\\nFormer Kenya Airways\\' frequent flyer programme \"Msafiri\" was merged with KLM\\'s \"Flying Dutchman\" in 1997, which was in turn merged with that of Air France and rebranded as Flying Blue in 2005, following the fusion of both companies. Gold Elite and Platinum Elite members of the Flying Blue programme are offered the \"JV Lounge\". This service is provided to Kenya Airways passengers, and to passengers flying with its partner airlines as well. \"Simba Lounge\" is a service provided to Kenya Airways Business passengers only. Both lounges are located at Jomo Kenyatta International Airport.\\nIn-flight entertainment.\\nDifferent in-flight entertainment is available depending upon the aircraft and the class travelled. The airline\\'s in-flight magazine is called \"Msafiri\", and is distributed among the passengers in all aircraft, irrespective of the class.\\nPremier World entertainment is AVOD; NVOD is offered in Economy class.\\nOverhead screens in both classes, plus eight channels of audio offered.\\nIndividual in-seat touchscreens.\\nAccidents and incidents.\\n, Kenya Airways has had two fatal accidents and two hull-loss accidents.'}\n",
      "score: 11.392608 source: {'id': '149154', 'revid': '1005449', 'url': 'https://en.wikipedia.org/wiki?curid=149154', 'title': 'Richard Leakey', 'text': 'Richard Erskine Frere Leakey FRS (born 19 December 1944) is a Kenyan palaeoanthropologist, conservationist and politician. Leakey has held a number of official positions in Kenya, mostly in institutions of archaeology and wildlife conservation. He has been Director of the National Museum of Kenya, founded the NGO WildlifeDirect and is the chairman of the Kenya Wildlife Service.\\nEarly life.\\nEarliest years.\\nAs a small boy, Leakey lived in Nairobi with his parents, Louis Leakey, curator of the Coryndon Museum, and Mary Leakey, director of the Leakey excavations at Olduvai, and his two brothers, Jonathan and Philip. The Leakey brothers had a very active childhood. All the boys had ponies and belonged to the Langata Pony Club. They participated in jumping and steeplechase competitions but often rode for fun across the plains to the Ngong Hills, chasing and playing games with the animals. Sometimes the whole club were guests at the Leakeys\\' for holidays and vacations. Leakey\\'s parents founded the Dalmatian Club of East Africa and won a prize in 1957. Dogs and many other pets shared the Leakey home. The Leakey boys participated in games conducted by both adults and children, in which they tried to imitate early humans, catching springhare and small antelope by hand on the Serengeti. They drove lions and jackals from the kill to see if they could do it.\\nFractured skull.\\nIn 1956, at age 11, Leakey fell from his horse, fracturing his skull and lying near death. Incidentally, it was this incident that saved his parents\\' marriage. Louis was seriously considering leaving Mary for his secretary, Rosalie Osborn. As the battle with Mary raged in the household, Leakey begged his father from his sickbed not to leave. That was the deciding factor. Louis broke up with Rosalie and the family lived in happy harmony for a few years more.\\nTeenage entrepreneur.\\nLeakey chose to support himself, borrowed 500 pounds from his parents for a Land Rover and went into the trapping and skeleton supply business with Kamoya Kimeu. Already a skilled horseman, outdoorsman, Land Rover mechanic, amateur archaeologist, and expedition leader, he learned to identify bones, skills which all pointed to a path he did not yet wish to take, simply because his father was on it.\\nThe bone business turned into a safari business in 1961. In 1962 he obtained a private aeroplane pilot license and took tours to Olduvai. It was from a casual aerial survey that he noted the potential of Lake Natron\\'s shores for palaeontology. He went looking for fossils in a Land Rover, but could find none, until his parents assigned Glynn Isaac to go with him. Louis was so impressed with their finds that he gave them National Geographic money for a month\\'s expedition. They explored in the vicinity of Peninj near the lake, where Leakey was in charge of the administrative details. Bored, he returned to Nairobi temporarily, but at that moment, Kamoya Kimeu discovered a fossil of \"Australopithecus boisei\". A second expedition left Leakey feeling that he was being excluded from the most significant part of the operation, the scientific analysis.\\nMarriage.\\nIn 1964 on his second Lake Natron expedition, Leakey met an archaeologist named Margaret Cropper. When Margaret returned to England, Leakey decided to follow suit to study for a degree and become better acquainted with her. He completed his high school requirements in six months; meanwhile Margaret obtained her degree at the University of Edinburgh. He passed the entrance exams for admission to college, but in 1965 he and Margaret decided to get married and return to Kenya. His father offered him a job at Centre for Prehistory and Palaeontology. He worked for it, excavating at Lake Baringo and continued his photographic safari business, making enough money to buy a house in Karen, a pleasant suburb of Nairobi. Their daughter Anna was born in 1969, the same year that Leakey and Margaret divorced. He married his colleague Meave Epps in 1970 and they had two daughters, Louise (born 1972) and Samira (1974).\\nPalaeontology.\\nLeakey\\'s career as a palaeoanthropologist did not begin with a date-able event or a sudden decision, as did that of Louis; he was with his parents on every excavation, was taught every skill and was given responsible work even as a boy. It is not surprising that his independent decision-making led him into conflict with his father, who had always tried to instil in him that very trait. After he gave some fossils to Tanzania and set Margaret to inventory Louis\\' collections, Louis suggested in 1967 that Richard find work elsewhere.\\nRichard formed the Kenya Museum Associates (now Kenya Museum Society) with influential Kenyans in that year. They aimed to \"Kenyanise\" and improve the National Museum. They offered the museum 5000 pounds, one-third of its yearly budget, if it would place Leakey in a responsible position, and he became an observer on the board of directors. Joel Ojal, the government official in charge of the museum, and a member of the Associates, directed the chairman of the board to start placing Kenyans on it.\\nThe Omo.\\nPlans for the museum had not matured when Louis, intentionally or not, found a way to remove his confrontational son from the scene. Louis attended a lunch with Emperor Haile Selassie and President Jomo Kenyatta. The conversation turned to fossils, and the Emperor wanted to know why none had been found in Ethiopia. Louis developed this inquiry into permission to excavate on the Omo River.\\nThe expedition consisted of three contingents: French, under Camille Arambourg, American, under Clark Howell, and Kenyan, led by Leakey. Louis could not go because of his arthritis. Crossing the Omo in 1967, Leakey\\'s contingent was attacked by crocodiles, which destroyed their wooden boat. Expedition members barely escaped with their lives. Leakey radioed Louis for a new, aluminium boat, which the National Geographic Society was happy to supply.\\nOn site, Kamoya Kimeu found a hominid fossil. Leakey took it to be \"Homo erectus\", but Louis identified it as \"Homo sapiens\". It was the oldest of the species found at that time, dating to 160,000 years, and was the first find contemporaneous with \"Homo neanderthalensis\". During the identification process, Leakey came to feel that the college men were patronising him.\\nKoobi Fora.\\nDuring the Omo expedition of 1967, Leakey visited Nairobi and on the return flight the pilot flew over Lake Rudolph (renamed Lake Turkana from 1975) to avoid a thunderstorm. The map led Leakey to expect volcanic rock below him but he saw sediments. Visiting the region with Howell by helicopter, he saw tools and fossils everywhere. In his mind, he started formulating a new enterprise.\\nIn 1968 Louis and Leakey attended a meeting of the Research and Exploration Committee of the National Geographic Society to ask for money for Omo. Catching Louis by surprise, Leakey asked the committee to divert the $25,000 intended for Omo to new excavations to be conducted under his leadership at Koobi Fora. Leakey won, but chairman Leonard Carmichael told him he\\'d better find something or never \"come begging at our door again\". Louis graciously congratulated Leakey.\\nBy then the board of the National Museum was packed with Kenyan supporters of Leakey. They appointed him administrative director. The curator, Robert Carcasson, resigned in protest, and Leakey was left with the museum at his command, which he, like Louis before him, used as a base of operations. Although there was friendly rivalry and contention between Louis and Leakey, relations remained good. Each took over for the other when one was busy with something else or incapacitated, and Leakey continued to inform his father immediately of hominid finds.\\nIn the first expedition to Allia Bay on Lake Turkana, where the Koobi Fora camp came to be located, Leakey hired only graduate students in anthropology, as he did not want any questioning of his leadership. The students were John Harris and Bernard Wood. Also present was a team of Africans under Kamoya: a geochemist, Paul Abel, and a photographer, Bob Campbell. Margaret was the archaeologist. Leakey took to smoking a pipe to enhance his status, as did Kamoya. There were no leadership problems. In contrast to his father, Leakey ran a disciplined and tidy camp, although, in order to find fossils, he did push the expedition harder than it wished.\\nIn 1969 the discovery of a cranium of \"Paranthropus boisei\" caused great excitement. A \"Homo rudolfensis\" skull (KNM ER 1470) and a \"Homo erectus\" skull (KNM ER 3733), discovered in 1972 and 1975, respectively, were among the most significant finds of Leakey\\'s earlier expeditions. In 1978 an intact cranium of \"Homo erectus\" (KNM ER 3883) was discovered.\\nLeakey was diagnosed with a terminal kidney disease in 1969. Ten years later he became seriously ill but received a kidney transplant from his brother, Philip, and recovered to full health.\\nLeakey and Donald Johanson were at the time considered the most famous palaeoanthropologists, and scientifically their views on human evolution were differing - a scientific rivalry that gained public attention. This culminated at the \"Cronkite\\'s Universe\" talk-show hosted by Walter Cronkite in New York in 1981, where Leakey and Johanson held a fierce debate on live TV-show.\\nWest Turkana.\\nTurkana Boy, discovered by Kamoya Kimeu, a member of the Leakeys\\' team, in 1984, was the nearly complete skeleton of a \"Homo ergaster\" (though some, including Leakey, call it \"erectus\") who died 1.6 million years ago at about age 9–12. Leakey and Roger Lewin describe the experience of this find and their interpretation of it, in their book \"Origins Reconsidered\" (1992). Shortly after the discovery of Turkana Boy, Leakey and his team made the discovery of a skull (KNM WT 17000, known as \"Black Skull\") of a new species, \"Australopithecus aethiopicus\" (or \"Paranthropus aethiopicus\").\\nRichard shifted away from palaeontology in 1989, but his wife Meave Leakey and daughter Louise Leakey continue to conduct palaeontological research in Northern Kenya.\\nConservation.\\nIn 1989 Richard Leakey was appointed the head of the Wildlife Conservation and Management Department (WMCD) by President Daniel Arap Moi in response to the international outcry over the poaching of elephants and the impact it was having on the wildlife of Kenya. The department was replaced by the Kenya Wildlife Service (KWS) in 1990, and Leakey became its first chairman. With characteristically bold steps Leakey created special, well-armed anti-poaching units that were authorised to shoot poachers on sight. The poaching menace was dramatically reduced. Impressed by Leakey\\'s transformation of the Kenya Wildlife Service, the World Bank approved grants worth $140 million. Richard Leakey, President Moi and the WMCD made the international news headlines when a stock pile of 12 tons of ivory was burned in 1989 in Nairobi National Park.\\nRichard Leakey\\'s confrontational approach to the issue of human–wildlife conflict in national parks did not win him friends. His view was that parks were self-contained ecosystems that had to be fenced in and the humans kept out. Leakey\\'s bold and incorruptible nature also offended many local politicians.\\nIn 2016 Richard Leakey achieved The Perfect World Foundation Award The Conservationist of the year 2016 &amp; Prize \"The Fragile Rhino\" at the Elephant Ball in Gothenburg, Sweden.\\nPlane crash.\\nIn 1993, a small propeller-driven plane piloted by Richard Leakey crashed, crushing his lower legs, both of which were later amputated. Sabotage was suspected but never proved. While in the hospital, Leakey told President Moi, a religious man, not to pray for him, but act on matters pending for the Kenya Wildlife Service. Since then, Richard Leakey has walked on artificial limbs. Around this time the Kenyan government announced that a secret probe had found evidence of corruption and mismanagement in the Kenya Wildlife Service. An annoyed Leakey resigned publicly in a press conference in January 1994. He was replaced by David Western as the head of the Kenya Wildlife Service.\\nRichard Leakey wrote about his experiences at the Kenya Wildlife Service in his book \"\" (2001).\\nPolitics.\\nIn May 1995, Richard Leakey joined some Kenyan intellectuals in launching a new political party – the Safina Party, which in Swahili means \"Noah\\'s Ark\". \"If KANU and Mr Moi will do something about the deterioration of public life, corruption and mismanagement, I\\'d be happy to fight alongside them. If they won\\'t, I want somebody else to do it,\" announced Richard Leakey. The Safina party was routinely harassed and even its application to become an official political party was not approved until 1997.\\nIn 1997, international donor institutions froze their aid to Kenya because of widespread corruption. To placate the donors, Moi appointed Richard Leakey as Cabinet Secretary and head of the civil service in 1999. Leakey\\'s second stint in the civil service lasted two years. He sacked 25,000 civil servants and obtained £250 million of funds from the International Monetary Fund and the World Bank. However, Leakey found himself sidelined after the money arrived, and his reforms were blocked in the courts. He was sacked from his cabinet post in 2001.\\nAmerica.\\nLeakey left Kenya for America in 2002, becoming a professor of anthropology at Stony Brook University, New York. He was also Chair of the Turkana Basin Institute. In 2004, Leakey founded and chaired WildlifeDirect, a Kenya-based charitable organisation. The charity was established to provide support to conservationists in Africa directly on the ground via the use of blogs. This enables individuals anywhere to play a direct and interactive role in the survival of some of the world\\'s most precious species. The organisation played a significant role in the saving of Congo\\'s mountain gorillas in Virunga National Park in January 2007 after a rebel uprising threatened to eliminate the highly vulnerable population.\\nIn April 2007, he was appointed interim chairman of Transparency International Kenya branch. The same year, Leakey was elected a Fellow of the Royal Society and received the Golden Plate Award of the American Academy of Achievement. In June 2013, Leakey was awarded the Isaac Asimov Science Award from the American Humanist Association.\\nReturn to Kenya.\\nIn 2015, President Uhuru Kenyatta appointed Leakey chairman of the board of the Kenya Wildlife Service. Although he was chairman rather than director, Leakey played an active role in KWS policies. He brokered a deal on the extension of the Mombasa–Nairobi Standard Gauge Railway, allowing the railway to pass over Nairobi National Park on an 18 m tall viaduct. Leakey felt that the viaduct would set an example for the rest of Africa in balancing economic development with environmental protection. However, other Kenyan conservationists have opposed railway construction in the park.\\nAngelina Jolie was to direct a bio-pic about Leakey\\'s life, with Leakey in early 2016 expressing his confidence that the film would be shot in Kenya.\\nBeliefs.\\nLeakey stated that he is an atheist and a humanist.\\nBibliography.\\nLeakey\\'s early published works include: \"Origins\" and \"The People of the Lake\" (both with Roger Lewin as co-author); \"The Illustrated Origin of Species\"; and \"The Making of Mankind\" (1981).'}\n",
      "score: 11.218966 source: {'id': '45349', 'revid': '9784415', 'url': 'https://en.wikipedia.org/wiki?curid=45349', 'title': 'List of national parks of Kenya', 'text': 'The national park system of Kenya is maintained by the Kenya Wildlife Service. There are two main types of terrestrial protected areas in Kenya: national parks, and national reserves; there are also marine parks and marine reserves.'}\n",
      "score: 11.218379 source: {'id': '72874', 'revid': '9748448', 'url': 'https://en.wikipedia.org/wiki?curid=72874', 'title': 'Rift Valley Province', 'text': \"Rift Valley Province () of Kenya, bordering Uganda, was one of Kenya's eight provinces, before the Kenyan general election, 2013.\\nRift Valley Province was the largest and one of the most economically important provinces in Kenya. It was dominated by the Kenya Rift Valley which passes through it and gives the province its name. According to the 2009 Census, the former province covered an area of and would have had a population of 10,006,805, making it the largest and most populous province in the country. The bulk of the provincial population inhabited a strip between former Nairobi and Nyanza Province. The capital was the town of Nakuru.\\nCounties.\\nAs of March 2013 after the Kenyan general election, 2013, the Province was partitioned into counties and Rift Valley Province was dissolved.\\nGeography.\\nThe Great Rift Valley runs south through Kenya from Lake Turkana in the north and has several unique geographical features, including the Elgeyo escarpment which is a popular tourist attraction.\\nApart from the Rift Valley itself, the area has other important geographic features such as: the extinct volcanoes Mount Longonot and Mount Suswa and Lake Baringo, Lake Bogoria, Lake Magadi, Lake Nakuru, Lake Naivasha, the Suguta Valley, and Lake Turkana.\\nGeology.\\nA large part of Kenya is underlain by Precambrian basement, while the Kenya rift basin (a typical extensional basin) hosts Tertiary volcanics that cover Mesozoic sediments (Recently these sediments have been considered for oil exploration). The sedimentary basins evolved along the Anza trough during the Late Paleozoic to Early Tertiary times through extension tectonics during the major Gondwanaland breakup. In the Miocene Period the region underwent intermittent uplift and subsidence along major boundary faults accompanied by the large outpouring of lava flows. The Anza trough intersects the modern rift valley in the area of Lake Turkana. Rifting still continues today; primarily in the north, where active volcanoes are more plentiful.\\nEconomy.\\nThe highlands provide adequate rainfall for farming and agriculture which is the economic base of the residents of the Rift Valley. Tea from the highlands in the Kericho district enjoy a worldwide reputation, but horticulture is an important part of the district's economy and cattle raising is also practised to a large extent.\\nThe full economic potential of the Rift Valley region is, however, far from fully exploited, though the current growth in population and improved education may change this in a near future. People in the province are still mostly rural, but urbanisation is gradually increasing; new cities and towns contain the rural-urban migration and, provided the right policies are instituted, the Rift Valley province will be able to emerge as a national economic and cultural hub.\\nEthnicity.\\nThe Rift Valley is home to various communities. The people of the Rift Valley are a mesh work of different ethnic identities, and the Kalenjin and the Maasai are two of the best known ethnic groups. Most of Kenya's top runners come from the Kalenjin community. The Maasai people have the most recognizable cultural identity, both nationally and internationally, and serve as Kenya's international cultural symbol.\"}\n",
      "score: 11.084081 source: {'id': '843815', 'revid': '9784415', 'url': 'https://en.wikipedia.org/wiki?curid=843815', 'title': 'Telkom Kenya', 'text': 'Telkom Kenya is an integrated telecommunications provider in Kenya. It was previously a part of the Kenya Posts and Telecommunications Corporation (KPTC) which was the sole provider of both postal and telecommunication services. The company was established as a telecommunications operator in April 1999, after the split of KPTC into the Communications Commission of Kenya (CCK), the Postal Corporation of Kenya (POSTA) and Telkom Kenya. The company is 60 per cent owned by Helios Investment Partners, with the remaining stake held by Kenyans through the Government of Kenya.\\nServices.\\nTelkom Kenya provides integrated telecommunications solutions to individuals, Small and Medium-sized Enterprises (SMEs), Government and large corporates in Kenya, drawing from a diverse solutions suite that includes voice, data, mobile money as well as network services. Powered by its vast fibre optic infrastructure, it is also a major provider of wholesale, carrier-to-carrier traffic within the country and the region.\\nThe company operates and maintains the infrastructure over which Kenya\\'s various internet service providers operate. As of 2004, most internet service was provided via dial-up service. Jambonet, an important Kenyan ISP, is a subsidiary of Telkom Kenya. It also offers mobile GSM voice and high speed internet services under the Orange Kenya brand, in which it is the 3rd in market share after Safaricom and AirtelKenya. In March 2018, the company resumed a mobile-money service that it had dropped in 2017. Referred to as T-kash, the service is a direct competitor to the M-pesa service, offered by market-leader Safaricom.\\nHistory.\\nIn 2007 France Télécom (now Orange S.A.) acquired 51% of Telkom Kenya\\'s shares at a cost of US$390 million. In November 2012, the shareholding structure changed due to a decision by the Kenyan government to convert its shareholder loans at that time, into equity in order to ease Telkom Kenya\\'s debt burden. It was subsequently confirmed that the Kenya government would retain 40% shareholding down from 49% with the remaining shares held by France Télécom. In January 2013, France Télécom increased its stake in Telkom Kenya to 70% as a consequence of the government\\'s failure to provide its full portion of the 2012 funding. In June 2017, the firm was re-branded from \"Orange Kenya\" to \"Telkom Kenya\".\\nPast shareholding.\\nOn November 9, 2015, Helios Investment Partners announced that they were going to purchase France Télécom\\'s entire stake in Telkom Kenya.\\nCurrent shareholding.\\nSubsequent to the agreement to buy, Helios negotiated with the Kenyan government to own 40 percent in the new joint venture, with the investment firm retaining 60 percent. In June 2016, final regulatory approval was received for the deal to proceed. The current shareholding is as depicted in the table below.'}\n"
     ]
    }
   ],
   "source": [
    "results = es.search(index=index_name, body={'from':0, 'size':5, 'query':{'match':{'text':'Kenya'}}})\n",
    "for result in results['hits']['hits']:\n",
    "    print('score:', result['_score'], 'source:', result['_source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 질의 응답 시스템 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로드하는 항목들\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import collections\n",
    "import json\n",
    "import logging\n",
    "import transformers\n",
    "import easydict\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets, load_metric\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from typing import Optional, Tuple\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Huggingface transformers의 토크나이저, 설정파일, 최적화 함수 등 사용\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from utils_qa import postprocess_qa_predictions\n",
    "from transformers.utils import check_min_version\n",
    "\n",
    "# huggingface dataset 추가\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 명령행 인터페이스, 사전 훈련된 모델을 로드하여 훈련하고 평가하는데까지 필요한 인자를 사용자 정의 인자로 설정하는 내용을 담고 있음.\n",
    "## 사용법: python example.py --dataset_name squad.json .... \n",
    "## 주피터 환경을 위한 인자 설정법 \n",
    "import easydict\n",
    "import string\n",
    "import re\n",
    "def easydict_args():\n",
    "    \n",
    "    args = easydict.EasyDict({\n",
    "        \n",
    "        \"dataset_name\": 'squad',\n",
    "        \"dataset_config_name\": 'plain_text',\n",
    "        \"train_file\": None,        \n",
    "        \"preprocessing_num_workers\": 4,\n",
    "        \"validation_file\": 'final_test_sample.json',\n",
    "        \"test_file\": 'final_test_no_context.json',\n",
    "        \"test_save_path\" : \"result/final_test.csv\",\n",
    "        \"max_seq_length\": 384,\n",
    "        \"pad_to_max_length\": None,\n",
    "        \"model_name_or_path\": 'pytorch_model.bin',\n",
    "        \"config_name\": 'bert-base-cased',\n",
    "        \"tokenizer_name\": 'bert-base-cased',\n",
    "        \"use_slow_tokenizer\": None,\n",
    "        \"per_device_train_batch_size\": 4,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 3e-5,\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"max_train_steps\": None,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"lr_scheduler_type\": 'linear',\n",
    "        \"num_warmup_steps\":0,\n",
    "        \"output_dir\": './result/',\n",
    "        \"seed\": 42,\n",
    "        \"doc_stride\": 128,\n",
    "        \"n_best_size\": 20,\n",
    "        \"null_score_diff_threshold\": 0.0,\n",
    "        \"version_2_with_negative\": False,\n",
    "        \"max_answer_length\": 30,\n",
    "        \"max_train_samples\": None,\n",
    "        \"max_eval_samples\": None,\n",
    "        \"overwrite_cache\": False,\n",
    "        \"max_predict_samples\": '2000',\n",
    "    })\n",
    "\n",
    "    ## 무결성 체크\n",
    "    ## 인자로 전달한 데이터셋 명이 잘못되었거나 확장자 오류가 있을 경우 raise 이하의 메세지를 출력\n",
    "    if (\n",
    "        args.dataset_name is None\n",
    "        and args.train_file is None\n",
    "        and args.validation_file is None\n",
    "        and args.test_file is None\n",
    "    ):\n",
    "        raise ValueError(\"Need either a dataset name or a training/validation/test file.\")\n",
    "    else:\n",
    "        if args.train_file is not None:\n",
    "            extension = args.train_file.split(\".\")[-1]\n",
    "            assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "        if args.validation_file is not None:\n",
    "            extension = args.validation_file.split(\".\")[-1]\n",
    "            assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
    "        if args.test_file is not None:\n",
    "            extension = args.test_file.split(\".\")[-1]\n",
    "            assert extension in [\"csv\", \"json\"], \"`test_file` should be a csv or a json file.\"\n",
    "\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    ## 위에서 인자로 전달받은 값을 args 변수에 담아 반환\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_name': 'squad', 'dataset_config_name': 'plain_text', 'train_file': None, 'preprocessing_num_workers': 4, 'validation_file': 'final_test_sample.json', 'test_file': 'final_test_no_context.json', 'test_save_path': 'result/final_test.csv', 'max_seq_length': 384, 'pad_to_max_length': None, 'model_name_or_path': 'pytorch_model.bin', 'config_name': 'bert-base-cased', 'tokenizer_name': 'bert-base-cased', 'use_slow_tokenizer': None, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 4, 'learning_rate': 3e-05, 'weight_decay': 0.0, 'num_train_epochs': 3, 'max_train_steps': None, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'linear', 'num_warmup_steps': 0, 'output_dir': './result/', 'seed': 42, 'doc_stride': 128, 'n_best_size': 20, 'null_score_diff_threshold': 0.0, 'version_2_with_negative': False, 'max_answer_length': 30, 'max_train_samples': None, 'max_eval_samples': None, 'overwrite_cache': False, 'max_predict_samples': '2000'}\n"
     ]
    }
   ],
   "source": [
    "print(easydict_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'datasets' from 'c:\\\\users\\\\donghwan lee\\\\appdata\\\\local\\\\programs\\\\python\\\\python38\\\\lib\\\\site-packages\\\\datasets\\\\__init__.py'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\Donghwan Lee\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\6b6c4172d0119c74515f44ea0b8262efe4897f2ddb6613e5e915840fdc309c16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 87599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "})\n",
      "<class 'datasets.dataset_dict.DatasetDict'>\n"
     ]
    }
   ],
   "source": [
    "print(datasets)\n",
    "\n",
    "raw_datasets = load_dataset('squad', 'plain_text')\n",
    "print(raw_datasets)\n",
    "print(type(raw_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQuAD 평가를 위한 전처리 함수\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main():\n",
    "    args = easydict_args()\n",
    "\n",
    "    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
    "    ## 가속화 모드를 지원하며, 해당 모드로 병목이 발생하는 메서드 부분을 대체해서 사용가능.\n",
    "    accelerator = Accelerator()\n",
    "    # Make one log on every process with the configuration for debugging.\n",
    "    logging.basicConfig(\n",
    "        format = \"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt = \"%m/%d/%Y %H:%M:%S\",\n",
    "        level = logging.INFO,\n",
    "    )\n",
    "    logger.info(accelerator.state)\n",
    "\n",
    "    # Setup logging, we only want one process per machine to log things on the screen.\n",
    "    # accelerator.is_local_main_process is only True for one process per machine.\n",
    "    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
    "    if accelerator.is_local_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    # If passed along, set the training seed now.\n",
    "    if args.seed is not None:\n",
    "        set_seed(args.seed)\n",
    "\n",
    "    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
    "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
    "    # (the dataset will be downloaded automatically from the datasets Hub).\n",
    "    #\n",
    "    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n",
    "    # 'text' is found. You can easily tweak this behavior (see below).\n",
    "    #\n",
    "    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "    # download the dataset.\n",
    "    \n",
    "    ## Huggingface의 datasets에 존재하는 데이터셋 명을 사용하는 경우에는 저장된 형식에 맞추어서 로드 (위의 링크에서 리스트 참조)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if args.dataset_name is not None:\n",
    "        # Downloading and loading a dataset from the hub.\n",
    "        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n",
    "\n",
    "        \n",
    "    ## 사용자 정의의 데이터셋을 사용하는 경우에는 아래의 조건문 실행. \n",
    "    else:\n",
    "        data_files = {}\n",
    "        print(args)\n",
    "        \n",
    "        if args.train_file is not None:\n",
    "            data_files[\"train\"] = args.train_file\n",
    "            #print(data_files[\"train\"])\n",
    "            \n",
    "        if args.validation_file is not None:\n",
    "            data_files[\"validation\"] = args.validation_file\n",
    "            #print(data_files[\"validation\"])\n",
    "            \n",
    "        if args.test_file is not None:\n",
    "            data_files[\"test\"] = args.test_file\n",
    "            #print(data_files[\"test\"])\n",
    "            \n",
    "        extension = args.test_file.split(\".\")[-1]\n",
    "        raw_datasets = load_dataset(extension, data_files=data_files, field=\"data\")\n",
    "        raw_datasets = load_dataset(extension, data_files=data_files, field=\"data\")\n",
    "\n",
    "\n",
    "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
    "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    #\n",
    "    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "\n",
    "    ## 지정한 모델의 설정에 맞추어서 환경 구성 요소 로드\n",
    "    ## ex) BERT --> BERTConfig...\n",
    "    if args.config_name:\n",
    "        config = AutoConfig.from_pretrained(args.config_name)\n",
    "    elif args.model_name_or_path:\n",
    "        config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "    else:\n",
    "        config = CONFIG_MAPPING[args.model_type]()\n",
    "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "    \n",
    "    ## 지정한 모델의 설정에 맞추어서 토크나이저 로드\n",
    "    ## ex) BERT --> BERTTokenizer ... 30,000개의 vocab와 word piece tokenzier를 곁들인...\n",
    "    if args.tokenizer_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=True)\n",
    "    elif args.model_name_or_path:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "        )\n",
    "\n",
    "    ## 지정한 모델 로드하기.\n",
    "    ## 여기서는 사전 훈련된 모델 상단에 질의응답 TASK를 위한 레이어를 추가하고 미세조정훈련을 하기위해 ForQuestionAnswering을 불러온다.\n",
    "    if args.model_name_or_path:\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            from_tf = bool(\".ckpt\" in args.model_name_or_path),\n",
    "            config = config,\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"Training new model from scratch\")\n",
    "        model = AutoModelForQuestionAnswering.from_config(config)\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # Preprocessing is slighlty different for training and evaluation.\n",
    "    \n",
    "    ## 전처리하기 \n",
    "#    column_names = raw_datasets[\"test\"].column_names\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "    print(column_names)\n",
    "    \n",
    "    \n",
    "    question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n",
    "    context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
    "    answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n",
    "    \n",
    "    \n",
    "    # Padding side determines if we do (question|context) or (context|question).\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "    ## 인자로 전달한 최대 입력 시퀀스 길이가 모델의 최대 입력 임베딩 길이보다 길 경우 에러 반환\n",
    "    if args.max_seq_length > tokenizer.model_max_length:\n",
    "        logger.warning(\n",
    "            f\"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the\"\n",
    "            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "\n",
    "    max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "\n",
    "    # prediction preprocessing\n",
    "    def prepare_validation_features(examples):\n",
    "        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "        # in one example possible giving several features when a context is long, each of those features having a\n",
    "        # context that overlaps a bit the context of the previous feature.\n",
    "        tokenized_examples = tokenizer(\n",
    "            examples[question_column_name if pad_on_right else context_column_name],\n",
    "            examples[context_column_name if pad_on_right else question_column_name],\n",
    "            truncation = \"only_second\" if pad_on_right else \"only_first\",\n",
    "            max_length = max_seq_length,\n",
    "            stride = args.doc_stride,\n",
    "            return_overflowing_tokens = True,\n",
    "            return_offsets_mapping = True,\n",
    "            padding = \"max_length\" if args.pad_to_max_length else False,\n",
    "        )\n",
    "\n",
    "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "        # its corresponding example. This key gives us just that.\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "        # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
    "        # corresponding example_id and we will store the offset mappings.\n",
    "        tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "        for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "            context_index = 1 if pad_on_right else 0\n",
    "\n",
    "            # One example can give several spans, this is the index of the example containing this span of text.\n",
    "            sample_index = sample_mapping[i]\n",
    "            tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "            # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "            # position is part of the context or not.\n",
    "            tokenized_examples[\"offset_mapping\"][i] = [\n",
    "                (o if sequence_ids[k] == context_index else None)\n",
    "                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "            ]\n",
    "\n",
    "        return tokenized_examples\n",
    "\n",
    "    \n",
    "\n",
    "    #################\n",
    "    ## 사용자 정의 검증 데이터를 입력 인자로 사용하는 경우, --do_predict을 입력 인자로 반드시 사용해야함.\n",
    "\n",
    "    if \"train\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_predict requires a test dataset\")\n",
    "    #eval_examples = raw_datasets[\"test\"]\n",
    "\n",
    "    \n",
    "    if args.max_eval_samples is not None:\n",
    "        # We will select sample from whole data\n",
    "        eval_examples = eval_examples.select(range(args.max_eval_samples))\n",
    "    # Validation Feature Creation\n",
    "    eval_dataset = eval_examples.map(\n",
    "        prepare_validation_features,\n",
    "        batched = True,\n",
    "        num_proc = args.preprocessing_num_workers,\n",
    "        remove_columns = column_names,\n",
    "        load_from_cache_file = not args.overwrite_cache,\n",
    "    )\n",
    "\n",
    "    if args.max_eval_samples is not None:\n",
    "        # During Feature creation dataset samples might increase, we will select required samples again\n",
    "        eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n",
    "\n",
    "\n",
    "\n",
    "    #################\n",
    "    ## 입력 인코딩을 완료한 데이터셋에 대해서 batch_size 만큼 로드하기 (pytorch 프레임워크의 장점)\n",
    "    # DataLoaders creation:\n",
    "    if args.pad_to_max_length:\n",
    "        # If padding was already done ot max length, we use the default data collator that will just convert everything\n",
    "        # to tensors.\n",
    "        data_collator = default_data_collator\n",
    "    else:\n",
    "        # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n",
    "        # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n",
    "        # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
    "        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None))\n",
    "\n",
    "    ## 검증 데이터셋에 대한 로더\n",
    "    eval_dataset_for_model = eval_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset_for_model, collate_fn = data_collator, batch_size = args.per_device_eval_batch_size\n",
    "    )\n",
    "\n",
    "\n",
    "    ## huggingface에서 제공하는 간편하게 평가 결과를 반환하도록 하는 함수 \n",
    "    ## 깃헙에서 다운로드 받은 utils_qa.py를 참조\n",
    "    # Post-processing:\n",
    "    def post_processing_function(examples, features, predictions, stage=\"eval\"):\n",
    "        # Post-processing: we match the start logits and end logits to answers in the original context.\n",
    "        predictions = postprocess_qa_predictions(\n",
    "            examples = examples,\n",
    "            features = features,\n",
    "            predictions = predictions,\n",
    "            version_2_with_negative = args.version_2_with_negative,\n",
    "            n_best_size = args.n_best_size,\n",
    "            max_answer_length = args.max_answer_length,\n",
    "            null_score_diff_threshold = args.null_score_diff_threshold,\n",
    "            output_dir = args.output_dir,\n",
    "            prefix = stage,\n",
    "        )\n",
    "        # Format the result to the format the metric expects.\n",
    "        if args.version_2_with_negative:\n",
    "            formatted_predictions = [\n",
    "                {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n",
    "            ]\n",
    "        else:\n",
    "            #formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "            formatted_predictions = [{\"id\": normalize_answer(k), \"prediction_text\": normalize_answer(v)} for k, v in predictions.items()]\n",
    "            df_pred = pd.DataFrame(formatted_predictions)\n",
    "            df_pred.to_csv(args.test_save_path , index=False, encoding='utf-8')\n",
    "\n",
    "        references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in examples]\n",
    "        return EvalPrediction(predictions = formatted_predictions, label_ids = references)\n",
    "\n",
    "    ## F1 score, Exact match\n",
    "    metric = load_metric(\"squad_v2\" if args.version_2_with_negative else \"squad\")\n",
    "\n",
    "    #################\n",
    "    # Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n",
    "    def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n",
    "        \"\"\"\n",
    "        Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n",
    "        Args:\n",
    "            start_or_end_logits(:obj:`tensor`):\n",
    "                This is the output predictions of the model. We can only enter either start or end logits.\n",
    "            eval_dataset: Evaluation dataset\n",
    "            max_len(:obj:`int`):\n",
    "                The maximum length of the output tensor. ( See the model.eval() part for more details )\n",
    "        \"\"\"\n",
    "\n",
    "        step = 0\n",
    "        # create a numpy array and fill it with -100.\n",
    "        ## pad에 해당하는 logit 값을 -100으로 지정\n",
    "        ## 효율적인 배치 계산을 위한 방법으로 지정한 max sequence length에 메모리 효율을 올리는 방법\n",
    "        logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n",
    "        # Now since we have create an array now we will populate it with the outputs gathered using accelerator.gather\n",
    "        for i, output_logit in enumerate(start_or_end_logits):  # populate columns\n",
    "            # We have to fill it such that we have to take the whole tensor and replace it on the newly created array\n",
    "            # And after every iteration we have to change the step\n",
    "\n",
    "            batch_size = output_logit.shape[0]\n",
    "            cols = output_logit.shape[1]\n",
    "\n",
    "            if step + batch_size < len(dataset):\n",
    "                logits_concat[step : step + batch_size, :cols] = output_logit\n",
    "            else:\n",
    "                logits_concat[step:, :cols] = output_logit[: len(dataset) - step]\n",
    "\n",
    "            step += batch_size\n",
    "\n",
    "        return logits_concat\n",
    "\n",
    "    # Prediction\n",
    "    all_start_logits = []\n",
    "    all_end_logits = []\n",
    "    for step, batch in enumerate(tqdm_notebook(eval_dataloader)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            start_logits = outputs.start_logits # 모든 토큰 중에서 가장 probability가 높은 시작점\n",
    "            end_logits = outputs.end_logits# 모든 토큰 중에서 가장 probability가 높은 끝점\n",
    "                \n",
    "            if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered\n",
    "                start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n",
    "                end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n",
    "\n",
    "            all_start_logits.append(accelerator.gather(start_logits).cpu().numpy())\n",
    "            all_end_logits.append(accelerator.gather(end_logits).cpu().numpy())\n",
    "\n",
    "    max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor\n",
    "\n",
    "    # concatenate the numpy array\n",
    "    start_logits_concat = create_and_fill_np_array(all_start_logits, eval_dataset, max_len)\n",
    "    end_logits_concat = create_and_fill_np_array(all_end_logits, eval_dataset, max_len)\n",
    "\n",
    "    # delete the list of numpy arrays\n",
    "    del all_start_logits\n",
    "    del all_end_logits\n",
    "\n",
    "    outputs_numpy = (start_logits_concat, end_logits_concat)\n",
    "    prediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)\n",
    "    eval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n",
    "    logger.info(f\"Evaluation metrics: {eval_metric}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
