{"id": "6321", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=6321", "title": "Channel 4", "text": "Channel 4 is a British free-to-air public-service television network with a remit to produce \"high quality and distinctive programming\". Its headquarters are in London, a National HQ in Leeds and creative hubs in Glasgow and Bristol.\nThe channel was established to provide a fourth television service to the United Kingdom in addition to the licence-funded BBC One and BBC Two, and the single commercial broadcasting network ITV.\nIt began transmission on 2 November 1982, the day after Welsh language broadcaster S4C's launch. Although largely commercially self-funded, it is ultimately publicly owned; originally a subsidiary of the Independent Broadcasting Authority (IBA), the station is now owned and operated by Channel Four Television Corporation, a public corporation of the Department for Digital, Culture, Media and Sport, which was established in 1990 and came into operation in 1993. In 2010, Channel 4 extended service into Wales and became a UK-wide television channel.\nHistory.\nConception.\nBefore Channel 4 and S4C, Britain had three terrestrial television services: BBC1, BBC2, and ITV. The Broadcasting Act 1980 began the process of adding a fourth; Channel 4 was formally created, along with its Welsh counterpart, by an Act of Parliament in 1982. After some months of test broadcasts, it began scheduled transmissions on 2 November 1982.\nThe notion of a second commercial broadcaster in the United Kingdom had been around since the inception of ITV in 1954 and its subsequent launch in 1955; the idea of an \"ITV2\" was long expected and pushed for. Indeed, television sets sold throughout the 1970s and early 1980s had a spare tuning button labelled \"ITV/IBA 2\". Throughout ITV's history and until Channel 4 finally became a reality, a perennial dialogue existed between the GPO, the government, the ITV companies and other interested parties, concerning the form such an expansion of commercial broadcasting would take. Most likely, politics had the biggest impact in leading to a delay of almost three decades before the second commercial channel became a reality.\nOne clear benefit of the \"late arrival\" of the channel was that its frequency allocations at each transmitter had already been arranged in the early 1960s, when the launch of an ITV2 was highly anticipated. This led to very good coverage across most of the country and few problems of interference with other UK-based transmissions; a stark contrast to the problems associated with Channel 5's launch almost 15 years later. \"ITV2\" is not to be confused with ITV's digital television channel launched in 1998.\nWales.\nAt the time the fourth service was being considered, a movement in Wales lobbied for the creation of dedicated service that would air Welsh-language programmes, then only catered for at \"off peak\" times on BBC Wales and HTV. The campaign was taken so seriously by Gwynfor Evans, former president of Plaid Cymru, that he threatened the government with a hunger strike were it not to honour the plans.\nThe result was that Channel 4 as seen by the rest of the United Kingdom would be replaced in Wales by Sianel Pedwar Cymru (S4C) (\"Channel Four Wales\"). Operated by a specially created authority, S4C would air programmes in Welsh made by HTV, the BBC and independent companies. Initially limited frequency space meant that Channel 4 could not be broadcast alongside S4C, though some Channel 4 programmes would be aired at less popular times on the Welsh variant; this practice continued until the closure of S4C's analogue transmissions in 2010, at which time S4C became a fully Welsh channel.\nWith this conversion of the Wenvoe transmitter group in Wales to digital terrestrial broadcasting on 31 March 2010, Channel 4 became a UK-wide television channel for the first time.\nSince then, carriage on digital cable, satellite and digital terrestrial has introduced Channel 4 to Welsh homes where it is now universally available.\nLaunch and IBA control.\nThe first voice heard on Channel 4's opening day of Tuesday 2 November 1982 was that of continuity announcer Paul Coia who said:\nFollowing the announcement, the channel headed into a montage of clips from its programmes set to the station's signature tune, \"Fourscore\", written by David Dundas, which would form the basis of the station's jingles for its first decade. The first programme to air on the channel was the teatime game show \"Countdown\", at 16:45 produced by Yorkshire Television. The first person to be seen on Channel 4 was Richard Whiteley with Ted Moult being the second. The first woman on the channel, contrary to popular belief, was not Whiteley's \"Countdown\" co-host Carol Vorderman but a lexicographer only ever identified as Mary. Whiteley opened the show with the words:\nOn its first day, Channel 4 also broadcast soap opera \"Brookside\", which often ran storylines thought to be controversial; this ran until 2003.\nAt its launch, Channel 4 committed itself to providing an alternative to the existing channels, an agenda in part set out by its remit which required the provision of programming to minority groups.\nIn step with its remit, the channel became well received both by minority groups and the arts and cultural worlds during this period, especially under founding chief executive Jeremy Isaacs, where the channel gained a reputation for programmes on the contemporary arts. Channel 4 co-commissioned Robert Ashley's ground-breaking television opera \"Perfect Lives\", which it premiered over several episodes in 1984. The channel often did not receive mass audiences for much of this period, however, as might be expected for a station focusing on minority interest.\nChannel 4 also began the funding of independent films, such as the Merchant Ivory docudrama \"The Courtesans of Bombay\", during this time.\nIn 1992, Channel 4 also faced its first libel case by Jani Allan, a South African journalist, who objected to her representation in Nick Broomfield's documentary \"The Leader, His Driver and the Driver's Wife\".\nIn September 1993, the channel broadcast the direct-to-TV documentary film \"Beyond Citizen Kane\", in which it displayed the dominant position of the Rede Globo television network, and discussed its influence, power and political connections in Brazil.\nChannel Four Television Corporation.\nAfter control of the station passed from the Channel Four Television Company to the Channel Four Television Corporation in 1993, a shift in broadcasting style took place. Instead of aiming for the fringes of society, it began to focus on the edges of the mainstream, and the centre of the mass market itself. It began to show many US programmes in peak viewing time, far more than it had previously done. It gave such shows as \"Friends\" and \"ER\" their UK premi\u00e8res.\nIn the early 2000s, Channel 4 began broadcasting reality formats such as \"Big Brother\" and obtained the rights to broadcast mass appeal sporting events like cricket and horse racing. This new direction increased ratings and revenues.\nIn addition, the corporation launched a number of new television channels through its new 4Ventures offshoot, including Film4, At the Races, E4 and More4.\nPartially in reaction to its new \"populist\" direction, the Communications Act 2003 directed the channel to demonstrate innovation, experimentation and creativity, appeal to the tastes and interests of a culturally diverse society, and to include programmes of an educational nature which exhibit a distinctive character.\nOn 31 December 2004, Channel 4 launched a new look and new idents in which the logo is disguised as different objects and the 4 can be seen in an angle.\nUnder the leadership of Freeview founder Andy Duncan, 2005 saw a change of direction for Channel 4's digital channels. Channel 4 made E4 free-to-air on digital terrestrial television, and launched a new free-to-air digital channel called More4. By October, Channel 4 had joined the Freeview consortium. By July 2006, Film4 had likewise become free-to-air and restarted broadcasting on digital terrestrial.\nVenturing into radio broadcasting, 2005 saw Channel 4 purchase 51% of shares in the now defunct Oneword radio station with UBC Media holding on to the remaining shares. New programmes such as the weekly, half-hour \"The Morning Report\" news programme were among some of the new content Channel 4 provided for the station, with the name 4Radio being used. As of early 2009, however, Channel 4's future involvement in radio remained uncertain.\nOn 2 November 2007, the station celebrated its 25th birthday. It showed the first episode of \"Countdown\", an anniversary \"Countdown\" special, as well as a special edition of \"The Big Fat Quiz\" and using the original multicoloured 1982\u20131996 blocks logo on presentation and idents using the Fourscore jingle throughout the day.\nIn November 2009, Channel 4 launched a week of 3D television, broadcasting selected programmes each night using stereoscopic ColorCode 3D technology. The accompanying 3D glasses were distributed through Sainsbury's supermarkets.\nOn 29 September 2015, Channel 4 revamped its presentation for a fifth time; the new branding downplayed the \"4\" logo from most on-air usage, in favour of using the shapes from the logo in various forms. Four new idents were filmed by Jonathan Glazer, which featured the shapes in various real-world scenes depicting the \"discovery\" and \"origins\" of the shapes. The full logo was still occasionally used, but primarily for off-air marketing. Channel 4 also commissioned two new corporate typefaces, \"Chadwick\", and \"Horseferry\" (a variation of Chadwick with the aforementioned shapes incorporated into its letter forms), for use across promotional material and on-air. \nOn 31 October 2017, Channel 4 introduced a new series of idents continuing the theme, this time depicting the logo shapes as having formed an anthropomorphic \"giant\" character.\nRecent history.\nBefore the digital switch-over, Channel 4 raised concerns over how it might finance its public service obligations afterward. It was announced in April 2006 that Channel 4's digital switch-over costs would be paid for by licence fee revenues.\nOn 28 March 2007, Channel 4 announced plans to launch a music channel \"4Music\" as a joint venture with British media company EMAP, which would include carriage on the Freeview platform. On 15 August 2008, 4Music was launched across the UK. Channel 4 announced interest in launching a high-definition version of Film4 on Freeview, to coincide with the launch of Channel 4 HD, However, the fourth HD slot was given to Channel 5 instead. Channel 4 has since acquired a 50% stake in EMAP's TV business for a reported \u00a328\u00a0million.\nChannel 4 was considered for privatisation by the governments of Margaret Thatcher, John Major and Tony Blair. the future of the channel was again being looked into by the government, with analysts suggesting several options for the channel's future.\nIn June 2017, it was announced that Alex Mahon would be the next chief executive, and would take over from David Abraham, who left in November 2017.\nPublic service remit.\nChannel 4 was established with, and continues to hold, a remit of public service obligations which it must fulfil. The remit changes periodically, as dictated by various broadcasting and communications acts, and is regulated by the various authorities Channel 4 has been answerable to; originally the IBA, then the ITC and now Ofcom.\nThe preamble of the remit as per the Communications Act 2003 states that:\nThe remit also involves an obligation to provide programming for schools, and a substantial amount of programming produced outside of Greater London.\nCarriage.\nChannel 4 was carried from its beginning on analogue terrestrial, which was practically the only means of television broadcast in the United Kingdom at the time. It continued to be broadcast through these means until the changeover to digital terrestrial television in the United Kingdom was complete. Since 1998, it has been universally available on digital terrestrial, and the Sky platform (initially encrypted, though encryption was dropped on 14 April 2008 and is now free of charge and available on the Freesat platform) as well as having been available from various times in various areas, on analogue and digital cable networks.\nDue to its special status as a public service broadcaster with a specific remit, it is afforded free carriage on the terrestrial platforms, in contrast with other broadcasters such as ITV.\nChannel 4 is available outside the United Kingdom where it is widely available in Ireland, the Netherlands, Belgium and Switzerland The channel is registered to broadcast within the European Union/EEA through the Luxembourg Broadcasting Regulator (ALIA). \nSince 2019, it has been offered by British Forces Broadcasting Service (BFBS) to members of HM Forces and their families around the world, BFBS Extra having previously carried a selection of Channel 4 programmes.\nThe Channel 4 website allows Internet users in the United Kingdom to watch Channel 4 live on the Internet. In the past some programmes (mostly international imports) were not shown. Channel 4 is also provided by Virgin Mobile's DAB mobile TV service which has the same restrictions as the Internet live stream had. Channel 4 is also carried by the Internet TV service TVCatchup and was previously carried by Zattoo until the operator removed the channel from its platform.\nChannel 4 also makes some of its programming available \"on demand\" via cable and the Internet through All 4.\nFunding.\nDuring the station's formative years, funding came from the ITV companies in return for their right to sell advertisements in their region on the fourth channel.\nNowadays it pays for itself in much the same way as most privately run commercial stations, i.e. through the sale of on-air advertising, programme sponsorship, and the sale of any programme content and merchandising rights it owns, such as overseas sales and video sales. For example, its total revenues were \u00a3925\u00a0million with 91% derived from sale of advertising. It also has the ability to subsidise the main network through any profits made on the corporation's other endeavours, which have in the past included subscription fees from stations such as E4 and Film4 (now no longer subscription services) and its \"video-on-demand\" sales. In practice, however, these other activities are loss-making, and are subsidised by the main network. According to Channel 4's last published accounts, for 2005, the extent of this cross-subsidy was some \u00a330\u00a0million.\nThe change in funding came about under the Broadcasting Act 1990 when the new corporation was afforded the ability to fund itself. Originally this arrangement left a \"safety net\" guaranteed minimum income should the revenue fall too low, funded by large insurance payments made to the ITV companies. Such a subsidy was never required, however, and these premiums were phased out by the government in 1998. After the link with ITV was cut, the cross-promotion which had existed between ITV and Channel 4 also ended.\nIn 2007, owing to severe funding difficulties, the channel sought government help and was granted a payment of \u00a314\u00a0million over a six-year period. The money was to have come from the television licence fee, and would have been the first time that money from the licence fee had been given to any broadcaster other than the BBC. However, the plan was scrapped by the Secretary of State for Culture, Media and Sport, Andy Burnham, ahead of \"broader decisions about the future framework of public service broadcasting\". The broadcasting regulator Ofcom released its review in January 2009 in which it suggested that Channel 4 would preferably be funded by \"partnerships, joint ventures or mergers\".\nProgramming.\nChannel 4 is a \"publisher-broadcaster\", meaning that it commissions or \"buys\" all of its programming from companies independent of itself. It was the first broadcaster in the United Kingdom to do so on any significant scale; such commissioning is a stipulation which is included in its licence to broadcast. This had the consequence of starting an industry of production companies that did not have to rely on owning an ITV licence to see their programmes air, though since Channel 4, external commissioning has become regular practice on the numerous stations that have launched since, as well as on the BBC and in ITV (where a quota of 25% minimum of total output has been imposed since the Broadcasting Act 1990 came into force). Although it was the first British broadcaster to commission all of its programmes from third parties, Channel 4 was the last terrestrial broadcaster to outsource its transmission and playout operations (to Red Bee Media), after 25 years in-house.\nThe requirement to obtain all content externally is stipulated in its licence. Additionally, Channel 4 also began a trend of owning the copyright and distribution rights of the programmes it aired, in a manner that is similar to the major Hollywood studios' ownership of television programmes that they did not directly produce. Thus, although Channel 4 does not produce programmes, many are seen as belonging to it.\nIt was established with a specific intention of providing programming to groups of minority interests, not catered for by its competitors, which at the time were only the BBC and ITV.\nChannel 4 also pioneered the concept of 'stranded programming', where seasons of programmes following a common theme would be aired and promoted together. Some would be very specific, and run for a fixed period of time; the \"4 Mation\" season, for example, showed innovative animation. Other, less specific strands, were (and still are) run regularly, such as \"T4\", a strand of programming aimed at teenagers, on weekend mornings (and weekdays during school/college holidays); \"Friday Night Comedy\", a slot where the channel would pioneer its style of comedy commissions, \"4Music\" (now a separate channel) and \"4Later\", an eclectic collection of offbeat programmes transmitted in the early hours of the morning.\nIn its earlier years, certain risqu\u00e9 art-house films (dubbed by many of Channel 4's critics as being pornographic) would be screened with a \"red triangle\" digital on-screen graphic in the upper right of the screen. Other films were broadcast under the \"Film on Four\" banner, before the \"FilmFour\" brand was launched in the late 1990s.\nMost watched programmes.\nThe following is a list of the 10 most watched shows on Channel 4 since launch, based on Live +28 data supplied by BARB, and archival data published by Channel 4.\nComedy.\nDuring the station's early days, the screenings of innovative short one-off comedy films produced by a rotating line-up of alternative comedians went under the title of \"The Comic Strip Presents\". \"The Tube\" and \"Saturday Live/Friday Night Live\" also launched the careers of a number of comedians and writers. Channel 4 broadcast a number of popular American imports, including \"Roseanne\", \"Friends\", \"Sex and the City\", \"South Park\" and \"Will &amp; Grace\". Other significant US acquisitions include \"The Simpsons\", for which the station was reported to have paid \u00a3700,000 per episode for the terrestrial television rights.\nIn April 2010, Channel 4 became the first UK broadcaster to adapt the American comedy institution of roasting to British television, with \"A Comedy Roast\".\nIn 2010, Channel 4 organised \"Channel 4's Comedy Gala\", a comedy benefit show in aid of Great Ormond Street Children's Hospital. With over 25 comedians appearing, it billed it as \"the biggest live stand up show in United Kingdom history\". Filmed live on 30 March in front of 14,000 at The O2 Arena in London, it was broadcast on 5 April. This has continued to 2016.\nFactual and current affairs.\nChannel 4 has a strong reputation for history programmes and real-life documentaries. It has also courted controversy, for example by broadcasting live the first public autopsy in the UK for 170 years, carried out by Gunther von Hagens in 2002, or the 2003 one-off stunt \"Derren Brown Plays Russian Roulette Live\".\nIts news service, \"Channel 4 News\", is supplied by ITN whilst its long-standing investigative documentary series, \"Dispatches\", attracts perennial media attention.\nFourDocs.\nFourDocs is an online documentary site provided by Channel 4. It allows viewers to upload their own documentaries to the site for others to view. It focuses on documentaries of between 3 and 5\u00a0minutes. The website also includes an archive of classic documentaries, interviews with documentary filmmakers and short educational guides to documentary-making. It won a Peabody Award in 2006. The site also includes a strand for documentaries of under 59 seconds, called \"Microdocs\".\nSchools programming.\nChannel 4 is obliged to carry schools programming as part of its remit and licence.\nITV Schools on Channel 4.\nSince 1957 ITV had produced schools programming, which became an obligation. In 1987, five years after the station was launched, the IBA afforded ITV free carriage of these programmes during Channel 4's then-unused weekday morning hours. This arrangement allowed the ITV companies to fulfil their obligation to provide schools programming, whilst allowing ITV itself to broadcast regular programmes complete with advertisements. During the times in which schools programmes were aired Central Television provided most of the continuity with play-out originating from Birmingham.\nChannel 4 Schools/4Learning.\nAfter the restructuring of the station in 1993, ITV's obligations to provide such programming on Channel 4's airtime passed to Channel 4 itself, and the new service became Channel 4 Schools, with the new corporation administering the service and commissioning its programmes, some still from ITV, others from independent producers.\nIn March 2008, the 4Learning interactive new media commission slabovia.tv was launched. The Slabplayer online media player showing TV shows for teenagers was launched on 26 May 2008.\nThe schools programming has always had elements which differ from its normal presentational package. In 1993, the Channel 4 Schools idents featured famous people in one category, with light shining on them in front of an industrial looking setting supplemented by instrumental calming music. This changed in 1996 with the circles look to numerous children touching the screen, forming circles of information then picked up by other children. The last child would produce the channel 4 logo in the form of three vertical circles, with another in the middle and to the left containing the Channel 4 logo.\nA present feature of presentation was a countdown sequence featuring, in 1993 a slide with the programme name, and afterwards an extended sequence matching the channel branding. In 1996, this was an extended ident with timer in top left corner, and in 1999 following the adoption of the squares look, featured a square with timer slowly make its way across the right of the screen with people learning and having fun while doing so passing across the screen. It finished with the Channel 4 logo box on the right of the screen and the name 'Channel 4 Schools' being shown. This was adapted in 2000 when the service's name was changed to '4Learning'.\nIn 2001, this was altered to various scenes from classrooms around the world and different parts of school life. The countdown now flips over from the top, right, bottom and left with each second, and ends with four coloured squares, three of which are aligned vertically to the left of the Channel 4 logo, which is contained inside the fourth box. The tag 'Learning' is located directly beneath the logo. The final countdown sequence lasted between 2004 and 2005 and featured a background video of current controversial issues, overlaid with upcoming programming information. the video features people in the style of graffiti enacting the overuse of CCTV cameras, fox hunting, computer viruses and pirate videos, relationships, pollution of the seas and violent lifestyles. Following 2005, no branded section has been used for Schools programmes.\nReligious programmes.\nFrom the outset, Channel 4 did not conform to the expectations of conventional religious broadcasting in the UK. John Ranelagh, first Commissioning Editor for Religion, made his priority 'broadening the spectrum of religious programming' and more 'intellectual' concerns. He also ignored the religious programme advisory structure that had been put in place by the BBC, and subsequently adopted by ITV. Ranelagh's first major commission caused a furore, a three-part documentary series called \"\". The programmes, transmitted during the Easter period of 1984, seemed to advocate the idea that the Gospels were unreliable, Jesus may have indulged in witchcraft, and that he may not have even existed. The series triggered a public outcry, and marked a significant moment in the deterioration in the relationship between the UK's broadcasting and religious institutions.\nFilm.\nNumerous genres of film-making \u2013 such as comedy, drama, documentary, adventure/action, romance and horror/thriller \u2013 are represented in the channel's schedule. From the launch of Channel 4 until 1998, film presentations on C4 would often be broadcast under the \"Film on Four\" banner.\nIn March 2005, Channel 4 screened the uncut Lars von Trier film \"The Idiots\", which includes unsimulated sexual intercourse, making it the first UK terrestrial channel to do so. The channel had previously screened other films with similar material but censored and with warnings.\nSince 1 November 1998, Channel 4 has had a digital subsidiary channel dedicated to the screening of films. This channel launched as a paid subscription channel under the name \"FilmFour\", and was relaunched in July 2006 as a free-to-air channel under the current name of \"Film4\". The Film4 channel carries a wide range of film productions, including acquired and Film4-produced projects. Channel 4's general entertainment channels E4 and More4 also screen feature films at certain points in the schedule as part of their content mix.\nWank Week.\nA season of television programmes about masturbation, called \"Wank Week\", was to be broadcast in the United Kingdom by Channel 4 in March 2007. The first show was about a Masturbate-a-thon, a public mass masturbation event, organised to raise money for the sexual health charity Marie Stopes International. Another film would have focused on compulsive male masturbators and a third was to feature the sex educator Dr Betty Dodson.\nThe series came under public attack from senior television figures, and was pulled amid claims of declining editorial standards and controversy over the channel's public service broadcasting credentials.\nGlobal warming.\nOn 8 March 2007, Channel 4 screened a highly controversial documentary, \"The Great Global Warming Swindle\". The programme states that global warming is \"a lie\" and \"the biggest scam of modern times\". The programme's accuracy has been disputed on multiple points, and several commentators have criticised it for being one-sided, noting that the mainstream position on global warming is supported by the scientific academies of the major industrialised nations. There were 246 complaints to Ofcom as of 25 April 2007, including allegations that the programme falsified data. The programme has been criticised by scientists and scientific organisations, and various scientists who participated in the documentary claimed their views had been distorted.\n\"Against Nature\": An earlier controversial Channel 4 programme made by Martin Durkin which was also critical of the environmental movement and was charged by the Independent Television Commission of the UK for misrepresenting and distorting the views of interviewees by selective editing.\n\"The Greenhouse Conspiracy\": An earlier Channel 4 documentary broadcast on 12 August 1990, as part of the \"Equinox\" series, in which similar claims were made. Three of the people interviewed (Lindzen, Michaels and Spencer) were also interviewed in \"The Great Global Warming Swindle\".\nAhmadinejad's Christmas speech.\nIn the \"Alternative Christmas address\" of 2008, a Channel 4 tradition since 1993 with a different presenter each year, Iranian President Mahmoud Ahmadinejad made a thinly veiled attack on the United States by claiming that Christ would have been against \"bullying, ill-tempered and expansionist powers\".\nThe airing courted controversy and was rebuked by several human rights activists, politicians and religious figures, including Peter Tatchell, Louise Ellman, Ron Prosor and Rabbi Aaron Goldstein. A spokeswoman for the Foreign and Commonwealth Office said: \"President Ahmadinejad has, during his time in office, made a series of appalling anti-Semitic statements. The British media are rightly free to make their own editorial choices, but this invitation will cause offence and bemusement not just at home but among friendly countries abroad\".\nHowever, some defended Channel 4. Stonewall director Ben Summerskill stated: \"In spite of his ridiculous and often offensive views, it is an important way of reminding him that there are some countries where free speech is not repressed...If it serves that purpose, then Channel 4 will have done a significant public service\". Dorothy Byrne, Channel 4's head of news and current affairs, also defended the station, saying: \"As the leader of one of the most powerful states in the Middle East, President Ahmadinejad's views are enormously influential... As we approach a critical time in international relations, we are offering our viewers an insight into an alternative world view...Channel 4 has devoted more airtime to examining Iran than any other broadcaster and this message continues a long tradition of offering a different perspective on the world around us\".\n4Talent.\n4Talent is an editorial branch of Channel 4's commissioning wing, which co-ordinates Channel 4's various talent development schemes for film, television, radio, new media and other platforms and provides a showcasing platform for new talent.\nThere are bases in London, Birmingham, Glasgow and Belfast, serving editorial hubs known respectively as 4Talent National, 4Talent Central England, 4Talent Scotland and 4Talent Northern Ireland. These four sites include features, profiles and interviews in text, audio and video formats, divided into five zones: TV, Film, Radio, New Media and Extras, which covers other arts such as theatre, music and design. 4Talent also collates networking, showcasing and professional development opportunities, and runs workshops, masterclasses, seminars and showcasing events across the UK.\n\"4Talent Magazine\".\n\"4Talent magazine\" is the creative industries magazine from 4Talent, which launched in 2005 as TEN4 magazine under the editorship of Dan Jones. \"4Talent Magazine\" is currently edited by Nick Carson. Other staff include deputy editor Catherine Bray and production editor Helen Byrne. The magazine covers rising and established figures of interest in the creative industries, a remit including film, radio, TV, comedy, music, new media and design.\nSubjects are usually UK-based, with contributing editors based in Northern Ireland, Scotland, London and Birmingham, but the publication has been known to source international content from Australia, America, continental Europe and the Middle East. The magazine is frequently organised around a theme for the issue, for instance giving half of November 2007's pages over to profiling winners of the annual 4Talent Awards.\nAn unusual feature of the magazine's credits is the equal prominence given to the names of writers, photographers, designers and illustrators, contradicting standard industry practice of more prominent writer bylines. It is also recognisable for its 'wraparound' covers, which use the front and back as a continuous canvas \u2013 often produced by guest artists.\nAlthough \"4Talent Magazine\" is technically a newsstand title, a significant proportion of its readers are subscribers. It started life as a quarterly 100-page title, but has since doubled in size and is now published bi-annually.\nPresentation.\nSince its launch in 1982, Channel 4 has used the same logo which consists of a stylised numeral \"4\" made up of nine differently shaped blocks. The logo was designed by Martin Lambie-Nairn and his partner [Colin Robinson] and was the first channel in the UK to depict an ident made using advanced computer generation (the first electronically generated ident was on BBC2 in 1979, but this was two-dimensional). It was designed in conjunction with Bo Gehring Aviation of Los Angeles and originally depicted the \"4\" in red, yellow, green, blue and purple. The music accompanying the ident was called \"Fourscore\" and was composed by David Dundas; it was later released as a single alongside a B-side, \"Fourscore Two\", although neither reached the UK charts. In November 1992, \"Fourscore\" was replaced by new music.\nIn 1996, Channel 4 commissioned Tomato Films to revamp the \"4\", which resulted in the \"Circles\" idents showing four white circles forming up transparently over various scenes, with the \"4\" logo depicted in white in one of the circles.\nIn 1999, Spin redesigned the logo to feature in a single square which sat on the right-hand side of the screen, whilst various stripes would move along from left to right, often lighting the squared \"4\" up. Like previous \"Circles\" idents from 1996 (which was made by Tomato Films), the stripes would be interspersed with various scenes potentially related to the upcoming programme.\nThe logo was made three-dimensional again in 2004 when it was depicted in filmed scenes that show the blocks forming the \"4\" logo for less than a second before the action moves away again.\nIn 2015, the logo was disassembled completely to allow the blocks to appear as parts of a nature scene, sometimes featuring a strange dancing creature and sometimes being excavated for scientific study, one being studied under a microscope and showing a tardigrade. The second wave of these idents, launched in 2017, depict a giant creature made of the \"4\" blocks (made to look almost like a person) interacting with everyday life, sometimes shouting the \"Fourscore\" theme as a foghorn.\nOn-air identity.\nThe Lambie-Nairn logo was the first logo of Channel 4, used from its launch on 2 November 1982 until 1996, lasting for fourteen years. The logo was re-introduced for one day only on 22 January 2021, to promote Channel 4's new five-part drama, It's a Sin, which focused on the 1980s AIDS crisis. It was additionally used once on 28 December 2020 as a commemoration for Lambie-Nairn, who had died three days earlier.\nRegions/International.\nRegions:&lt;br&gt;\nChannel 4 has, since its inception, broadcast identical programmes and continuity throughout the United Kingdom (excluding Wales where it did not operate on analogue transmitters). At launch this made it unique, as both the BBC and ITV had long established traditions of providing regional variations in their programming in different areas of the country. Since the launch of subsequent British television channels, Channel 4 has become typical in its lack of regional programming variations.\nA few exceptions exist to this rule for programming and continuity:\nSome of Channel 4's schools' programming (1980s/early '90s) was regionalised due to differences in curricula between different regions.\nPart of Channel 4's remit covers the commissioning of programmes from outside London. Channel 4 has a dedicated director of nations and regions, Stuart Cosgrove, who is based in a regional office in Glasgow. As his job title suggests, it is his responsibility to foster relations with independent producers based in areas of the United Kingdom (including Wales) outside London.\nAdvertising on Channel 4 does contain regular variation: prior to 1993, when ITV was responsible for selling Channel 4's advertising, each regional ITV company would provide the content of advertising breaks, covering the same transmitter area as themselves, and these breaks were often unique to that area. After Channel 4 became responsible for its own advertising, it continued to offer advertisers the ability to target particular audiences and divided its coverage area into six regions: London, South (including Wales), Midlands, North, Northern Ireland and Scotland.\nAt present, Wales does not have its own advertising region, instead its viewers receive the southern region on digital platforms intentionally broadcast to the area or the neighbouring region where terrestrial transmissions spill over into Wales. Northern Ireland also has its own advertising window referred to as 'Ulster Macro'. Channel 5 and ITV Breakfast use a similar model to Channel 4 for providing their own advertising regions, despite also having a single national output of programming.\nInternational: \nChannel 4 available within the Republic of Ireland is specifically tailored towards the Irish market with advertising and sponsorship specific to this market. The channel is registered with the broadcasting regulators in Luxembourg for terms of conduct and business within the EU/EEA while observing guidelines outlined by Ireland's BAI code. Irish advertising sales are monitored by Media Link in Dublin. Where Channel 4 does not hold broadcasting rights within the Republic of Ireland such programming is unavailable. For example, the series \"Glee\" was not available on Channel 4 on Sky in Ireland due to it broadcasting on Virgin Media One within Ireland. Currently, programming available on All 4 is available within the Republic of Ireland without restrictions. The Channel 4 available elsewhere in Europe is the UK version of the channel. \nFuture possibility of regional news.\nWith ITV plc pushing for much looser requirements on the amount of regional news and other programming it is obliged to broadcast in its ITV regions, the idea of Channel 4 taking on a regional news commitment has been considered, with the corporation in talks with Ofcom and ITV over the matter. Channel 4 believe that a scaling-back of such operations on ITV's part would be detrimental to Channel 4's national news operation, which shares much of its resources with ITV through their shared news contractor ITN. At the same time, Channel 4 also believe that such an additional public service commitment would bode well in on-going negotiations with Ofcom in securing additional funding for its other public service commitments.\nChannel 4 HD.\nIn mid-2006 Channel 4 ran a six-month closed trial of HDTV, as part of the wider Freeview HD experiment via the Crystal Palace transmitter to London and parts of the home counties, including the use of \"Lost\" and \"Desperate Housewives\" as part of the experiment, as US broadcasters such as ABC already have an HDTV back catalogue.\nOn 10 December 2007, Channel 4 launched a high definition television simulcast of Channel 4 on Sky's digital satellite platform, after Sky agreed to contribute toward the channel's satellite distribution costs. It was the first full-time high definition channel from a terrestrial UK broadcaster.\nOn 31 July 2009, Virgin Media added Channel 4 HD on channel 146 (later on channel 142, now on channel 141) as a part of the M pack. On 25 March 2010 Channel 4 HD appeared on Freeview channel 52 with a placeholding caption, ahead of a commercial launch on 30 March 2010, coinciding with the commercial launch of Freeview HD. On 19 April 2011, Channel 4 HD was added to Freesat on channel 126. As a consequence, the channel moved from being free-to-view to free-to-air on satellite during March 2011. With the closure of S4C Clirlun in Wales on 1 December 2012, on Freeview, Channel 4 HD launched in Wales on 2 December 2012.\nThe channel carries the same schedule as Channel 4, broadcasting programmes in HD when available, acting as a simulcast. Therefore, SD programming is broadcast upscaled to HD. The first true HD programme to be shown was the 1996 Adam Sandler film Happy Gilmore. From launch until 2016 the presence of the 4HD logo on screen denoted true HD content.\nOn 1 July 2014, Channel 4 +1 HD, a HD simulcast of Channel 4 +1, launched on Freeview channel 110. It closed on 22 June 2020 to help make room on COM7 following the closure of COM8 on Freeview.\nOn 20 February 2018, Channel 4 announced that Channel 4 HD and All 4 will no longer be supplied on Freesat from Thursday 22 February 2018.\nOn 22 June 2020 Channel4+1 HD and 4Seven HD were removed from Freeview.\nAll 4.\nAll 4 is a video on demand service from Channel 4, launched in November 2006 as 4oD. The service offers a variety of programmes recently shown on Channel 4, E4, More4 or from their archives, though some programmes and movies are not available due to rights issues.\nTeletext services.\n4-Tel/FourText.\nChannel 4 originally licensed an ancillary teletext service to provide schedules, programme information and features. The original service was called 4-Tel, and was produced by Intelfax, a company set up especially for the purpose. It was carried in the 400s on Oracle. In 1993, with Oracle losing its franchise to Teletext Ltd, 4-Tel found a new home in the 300s, and had its name shown in the header row. Intelfax continued to produce the service and in 2002 it was renamed FourText.\nTeletext on 4.\nIn 2003, Channel 4 awarded Teletext Ltd a ten-year contract to run the channel's ancillary teletext service, named Teletext on 4. The service closed in 2008, and Teletext is no longer available on Channel 4, ITV and Channel 5."}
{"id": "6322", "revid": "39895928", "url": "https://en.wikipedia.org/wiki?curid=6322", "title": "Carolina parakeet", "text": "The Carolina parakeet (\"Conuropsis carolinensis\"), or Carolina conure, is an extinct species of small green neotropical parrot with a bright yellow head, reddish orange face and pale beak that was native to the eastern, Midwest and plains states of the United States. It was the only indigenous parrot within its range, as well as one of only three parrot species native to the United States (the others being the thick-billed parrot, now extirpated, and the green parakeet still present in Texas; a fourth parrot species, the red-crowned amazon, is debated). It was found from southern New York and Wisconsin to Kentucky, Tennessee and the Gulf of Mexico, from the Atlantic seaboard to as far west as eastern Colorado. It lived in old-growth forests along rivers and in swamps. It was called \"puzzi la n\u00e9e\" (\"head of yellow\") or \"pot pot chee\" by the Seminole and \"kelinky\" in Chickasaw. Though formerly prevalent within its range, the bird had become rare by the middle of the 19th century. The last confirmed sighting in the wild was of the \"ludovicianus\" subspecies in 1910. The last known specimen perished in captivity at the Cincinnati Zoo in 1918 and the species was declared extinct in 1939.\nThe earliest reference to these parrots was in 1583 in Florida reported by Sir George Peckham in \"A True Report of the Late Discoveries of the Newfound Lands\" of expeditions conducted by English explorer Sir Humphrey Gilbert who notes that explorers in North America \"doe testifie that they have found in those countryes;\u00a0... parrots.\" They were first scientifically described in English naturalist Mark Catesby's two volume \"Natural History of Carolina, Florida and the Bahama Islands\" published in London in 1731 and 1743.\nCarolina parakeets were probably poisonous\u2014American naturalist and painter John J. Audubon noted that cats apparently died from eating them, and they are known to have eaten the toxic seeds of cockleburs.\nTaxonomy.\n\"Carolinensis\" is a species of the genus \"Conuropsis\", one of numerous genera of New World Neotropical parrots in family Psittacidae of true parrots.\nThe specific name \"Psittacus carolinensis\" was assigned by Swedish zoologist Carl Linnaeus in the 10th edition of Systema Naturae published in 1758. The species was given its own genus \"Conuropsis\" by Italian zoologist and ornithologist Tommaso Salvadori in 1891 in his \"Catalogue of the Birds in the British Museum\", volume\u00a020. The name is derived from the Greek-ified \"conure\" (\"parrot of the genus \"Conurus\"\" an obsolete name of genus \"Aratinga\") + \"-opsis\" (\"likeness of\") and Latinized \"Carolina\" (from Carolana, an English colonial province) + \"-ensis\" (of or \"from a place\"), therefore a bird \"like a conure from Carolina.\"\nThere are two recognized subspecies. The Louisiana subspecies of the Carolina parakeet, \"C. c. ludovicianus\", was slightly different in color than the nominate subspecies, being more bluish-green and generally of a somewhat subdued coloration, and became extinct in much the same way, but at a somewhat earlier date (early 1910s). The Appalachian Mountains separated these birds from the eastern \"C.\u00a0c.\u00a0carolinensis\".\nEvolution.\nAccording to a study of mitochondrial DNA recovered from museum specimens, their closest living relatives include some of the South American \"Aratinga\" parakeets: The Nanday parakeet, the sun parakeet, and the golden-capped parakeet. The authors note the bright yellow and orange plumage and blue wing feathers found in \"Conuropsis carolinensis\" are traits shared by another species, the jandaya parakeet (\"A.\u00a0jandaya\"), that was not sampled in the study but is generally thought to be closely related. To help resolve the divergence time a whole genome of a preserved specimen has now been sequenced. Carolinensis is in a sister clade to that of Spix's macaw. The Carolina parakeet colonized North America about 5.5\u00a0million years ago. This was well before North America and South America were joined together by the formation of the Panama land bridge about 3.5\u00a0mya. Since the Carolina parakeets' more distant relations are geographically closer to its own historic range while its closest relatives are more geographically distant to it, these data are consistent with the generally accepted hypothesis that Central and North America were colonized at different times by distinct lineages of parrots \u2013 parrots that originally invaded South America from Antarctica some time after the breakup of Gondwana, where Neotropical parrots originated approximately 50\u00a0mya.\nThe following cladogram shows the placement of the Carolina parakeet among its closest relatives, after a DNA study by Kirchman \"et al\". (2012):\nA fossil parrot, designated \"Conuropsis fratercula\", was described based on a single humerus from the Miocene Sheep Creek Formation (possibly late Hemingfordian, c.\u00a016\u00a0mya, possibly later) of Snake River, Nebraska. It was a smaller bird, three-quarters the size of the Carolina parakeet. \"The present \"species\" is of peculiar interest as it represents the first known parrot-like bird to be described as a fossil from North America.\" (Wetmore 1926; italics added) However, it is not completely certain that the species is correctly assigned to \"Conuropsis\", but some authors consider it a paleosubspecies of the Carolina parakeet.\nDescription.\nThe Carolina parakeet was a small green parrot very similar in size and coloration to the extant jenday parakeet and sun conure. The majority of the plumage was green with lighter green underparts, a bright yellow head and orange forehead and face extending to behind the eyes and upper cheeks (lores). The shoulders were yellow, continuing down the outer edge of the wings. The primary feathers were mostly green, but with yellow edges on the outer primaries. Thighs were green towards the top and yellow towards the feet. Male and female adults were identical in plumage, however males were slightly larger than females (sexually dimorphic). The legs and feet were light brown. They share the zygodactyl feet of the parrot family. The skin around the eyes was white and the beak was pale flesh colored. These birds weigh about 3.5\u00a0oz., are 13\u00a0in. long, and have wingspans of 2123\u00a0in.\nYoung Carolina parakeets differed slightly in coloration from adults. The face and entire body was green, with paler underparts. They lacked yellow or orange plumage on the face, wings, and thighs. Hatchlings were covered in mouse-gray down, until about 39\u201340 days when green wings and tails appear. Fledglings had full adult plumage at around 1 year of age. (\"Nature Serve, Conuropsis carolinensis\", 2005; Fuller, 2001; Mauler, 2001; Rising, 2004; Snyder and Russell, 2002)\nThese birds were fairly long lived, at least in captivity - a pair was kept at the Cincinnati Zoo for over 35 years.\nDistribution and habitat.\nThe Carolina parakeet had the northernmost range of any known parrot. It was found from southern New England and New York and Wisconsin to Kentucky, Tennessee and the Gulf of Mexico. It has also had a wide distribution west of the Mississippi River, as far west as eastern Colorado. Its range was described by early explorers thus: the 43rd parallel as the northern limit, the 26th as the most southern, the 73rd and 106th meridians as the eastern and western boundaries respectively, the range included all or portions of at least 28 states. Its habitats were old-growth wetland forests along rivers and in swamps especially in the Mississippi-Missouri drainage basin with large hollow trees including cypress and sycamore to use as roosting and nesting sites.\nOnly very rough estimates of the birds' former prevalence can be made: with an estimated range of 20,000 to 2.5 million km2, and population density of 0.5 to 2.0 parrots per km2, population estimates range from tens of thousands to a few million birds (though the densest populations occurred in Florida covering 170,000\u00a0km2, so there may have been hundreds of thousands of the birds in that state alone).\nThe species may have appeared as a very rare vagrant in places as far north as Southern Ontario. A few bones, including a pygostyle found at the Calvert Site in Southern Ontario, came from the Carolina parakeet. The possibility remains open that this specimen was taken to Southern Ontario for ceremonial purposes.\nBehavior and diet.\nThe bird lived in huge, noisy flocks of as many as 200\u2013300 birds. It built its nest in a hollow tree, laying two to five (most accounts say two) round white eggs.\nIt mostly ate the seeds of forest trees and shrubs including those of cypress, hackberry, beech, sycamore, elm, pine, maple, oak, and other plants such as thistles and sandspurs (\"Cenchrus\" species). It also ate fruits, including apples, grapes and figs (often from orchards by the time of its decline). It was especially noted for its predilection for cockleburs (\"Xanthium strumarium\"), a plant which contains a toxic glucoside, and it was considered to be an agricultural pest of grain crops.\nExtinction.\nThe last captive Carolina parakeet, Incas, died at the Cincinnati Zoo on February 21, 1918, in the same cage as Martha, the last passenger pigeon, who died in 1914. There are no scientific studies or surveys of this bird by American naturalists; most information about it is from anecdotal accounts and museum specimens. Therefore, details of its prevalence and decline are unverified or speculative.\nThere are extensive accounts of the pre-colonial and early colonial prevalence of this bird. The existence of flocks of gregarious, very colorful and raucous parrots could hardly have gone unnoted by European explorers, as parrots were virtually unknown in seafaring European nations in the 16th and 17th centuries. Later accounts in the latter half of the 19th century onward noted the birds' sparseness and absence.\nThe birds' range collapsed from east to west with settlement and clearing of the eastern and southern deciduous forests. John J. Audubon commented as early as 1832 on the decline of the birds. The bird was rarely reported outside Florida after 1860. The last reported sighting east of the Mississippi River (except Florida) was in 1878 in Kentucky. By the turn of the century it was restricted to the swamps of central Florida. The last known wild specimen was killed in Okeechobee County, Florida, in 1904, and the last captive bird died at the Cincinnati Zoo on February 21, 1918. This was the male specimen, called \"Incas,\" who died within a year of his mate, \"Lady Jane.\" Additional reports of the bird were made in Okeechobee County, Florida, until the late 1920s, but these are not supported by specimens. It was not until 1939, however, that the American Ornithologists' Union declared that the Carolina parakeet had become extinct. The IUCN has listed the species as extinct since 1920.\nIn 1937, three parakeets resembling this species were sighted and filmed in the Okefenokee Swamp of Georgia. However, the American Ornithologists' Union analyzed the film and concluded that they had probably filmed feral parakeets. A year later, in 1938, a flock of parakeets was apparently sighted by a group of experienced ornithologists in the swamps of the Santee River basin in South Carolina. However, this sighting was doubted by most other ornithologists. The birds were never seen again after this sighting, and shortly after a portion of the area was destroyed to make way for power lines, making the species' continued existence unlikely.\nAbout 720 skins and 16 skeletons are housed in museums around the world and analyzable DNA has been extracted from them.\nReasons for extinction.\nThe evidence is indicative that humans had at least a contributory role in the extinction of the Carolina parakeet, through a variety of means. Chief was deforestation in the 18th and 19th centuries. Hunting played a significant role, both for decorative use of their colorful feathers, for example, adornment of women's hats, and for reduction of crop predation. This was partially offset by the recognition of their value in controlling invasive cockleburs. Minor roles were played by capture for the pet trade and, as noted in \"Pacific Standard\", by the introduction for crop pollination of European honeybees that competed for nest sites.\nA factor that exacerbated their decline to extinction was the flocking behavior that led them to return to the vicinity of dead and dying birds (e.g., birds downed by hunting), enabling wholesale slaughter.\nThe final extinction of the species in the early years of the 20th century is somewhat of a mystery, as it happened so rapidly. Vigorous flocks with many juveniles and reproducing pairs were noted as late as 1896, and the birds were long-lived in captivity, but they had virtually disappeared by 1904. Sufficient nest sites remained intact, so deforestation was not the final cause. American ornithologist Noel F. Snyder speculates that the most likely cause seems to be that the birds succumbed to poultry disease, although no recent or historical records exist of New World parrot populations being afflicted by domestic poultry diseases. The modern poultry scourge Newcastle disease was not detected until 1926 in Indonesia, and only a subacute form of it was reported in the United States in 1938."}
{"id": "6324", "revid": "1022843719", "url": "https://en.wikipedia.org/wiki?curid=6324", "title": "Collective trauma", "text": "The term collective trauma is a psychological reaction to a traumatic event shared by any group of people and can affect even an entire society. \nTrauma typically refers to the impact that a traumatic incident has had on an individual or a few people, with lasting repercussions on the physical and mental health of that individual(s). However, according to Thomas H\u00fcbl, it is impossible to separate personal trauma at the individual level and collective trauma at the group level, as the very physiology of our nervous system is created in interaction with the nervous system of other people from the moment that we are conceived. Multiple international scientific studies have shown how the emotional states of a mother has a direct impact on the developing nervous system of their child and the ensuing development of their brain systems over time. \nA study conducted in the aftermath of the Six day war in Israel in 1967 for example, found that women who were pregnant during the wars occurrence were statistically more likely to have had children with schizophrenia. What happened at the collective level of the country, was directly reflected in the individual neurobiological systems of the infants in the womb. Due to the direct correlation/connection between the nervous system and every other organ in our bodies, collective trauma is also evident at the cellular level. Trauma can thus not be understood in purely individual terms. \nCollective trauma does not merely reflect a historical fact or the recollection of a traumatic event that happened to a group of people. Collective trauma suggests that the tragedy is represented in the collective memory of the group, and like all forms of memory it comprises not only a reproduction of the events, but also an ongoing reconstruction of the trauma in an attempt to make sense of it. Collective memory of a trauma is different from individual memory because collective memory persists beyond the lives of the direct survivors of the events, and is remembered by group members that may be far removed from the traumatic events in time and space. \nClarifying the term collective, Ursula K\u00f6nig (2018) focused on two different levels of collective trauma:\nAccording to these two distinctions, a collective trauma can only be defined as such if affects can be clearly defined at either level. For example, the traumatisation of many individuals may not be considered collective, unless their traumatic experiences are used as key identity markers in public discourses and/or as a way of self-expression/-definition. Once trauma of many individuals is framed and used as a collective identity marker we can speak of it as such.\nFurthermore, a distinction can be made between collective identity markers which in practice are all highly interwoven:\nTraumatic events witnessed by an entire society can stir up collective sentiment, often resulting in a shift in that society's culture and mass actions.\nWell known collective traumas include: The Holocaust, the Armenian Genocide, Slavery in the United States, the Atomic bombings of Hiroshima and Nagasaki, the Trail of Tears, the Great Irish Famine, Attack on Pearl Harbor, the MS Estonia in Sweden, the September 11, 2001 attacks in the United States, the Halabja chemical attack and various others.\nCollective traumas have been shown to play a key role in group identity formation (see: Law of Common Fate). During World War II, a US submarine, the USS \"Puffer\" (SS-268), came under several hours of depth charge attack by a Japanese surface vessel until the ship became convinced the submarine had somehow escaped. Psychological studies later showed that crewmen transferred to the submarine after the event were never accepted as part of the team. Later, US naval policy was changed so that after events of such psychological trauma, the crew would be dispersed to new assignments.\nRehabilitation of survivors becomes extremely difficult when an entire nation has experienced such severe traumas as war, genocide, torture, massacre, etc. Treatment is hardly effective when everybody is traumatized. Trauma remains chronic and would reproduce itself as long as social causes are not addressed and perpetrators continue to enjoy impunity. The whole society may suffer from an everlasting culture of pain. However, ways to heal collective trauma have recently been created (see section on Healing Collective Trauma below). \nDuring the Algerian War, Frantz Omar Fanon found his practice of treatment of native Algerians ineffective due to the continuation of the horror of a colonial war. He emphasized about the social origin of traumas, joined the liberation movement and urged oppressed people to purge themselves of their degrading traumas through their collective liberation struggle. He made the following remarks in his letter of resignation, as the Head of the Psychiatry Department at the Blida-Joinville Hospital in Algeria:\"If psychiatry is the medical technique that aims to enable man no longer to be a stranger to his environment, I owe it to myself to affirm that the Arab, permanently an alien in his own country, lives in a state of absolute depersonalization\" . Inculcation of horror and anxiety, through widespread torture, massacre, genocide and similar coercive measures has happened frequently in human history. There are plenty of examples in our modern history. Tyrants have always used their technique of \"psychological artillery\" in an attempt to cause havoc and confusion in the minds of people and hypnotize them with intimidation and cynicism. The result is a collective trauma that will pass through generations. There is no magic formula of rehabilitation. Collective trauma can be alleviated through cohesive and collective efforts such as recognition, remembrance, solidarity, communal therapy and massive cooperation.\nInfluence of Technology on Collective Trauma.\nTechnology provides many opportunities and potential for creative connection and collaboration, such as for example through commons based peer production, see for example commons-based peer production - Wikipedia itself is an example for this. \nHowever, there is also ways in which technology and its use nowadays creates a large amount of collective trauma enhancing content. The fast pace of information flow can overwhelm the human cognition- and nervous system. Trauma-researcher Thomas H\u00fcbl explains that humans can create, develop and evolve only if their nervous system integrates and digests well the data and information to be transformed. However, the current speed and complexity of data spread and consumed through technological infrastructure can create an enormous pressure onto the human nervous system. If a nervous system is overwhelmed, it is unable to integrate information, and it creates increased levels of anxiety, hyperactivity, stress, disembodiment and hence disconnection from the self. These are symptoms of trauma. \nFor example, the high speed level of violence-charged global news sent around through socio-technological infrastructures can cause a nervous system to be overwhelmed. Hence, the inability of receiving (historical or current) collective trauma content at a fast pace recreates collective trauma. \nHealing Collective Trauma.\nThe above-mentioned author and international group facilitator Thomas H\u00fcbl worked out and finalised a facilitated group process to address, integrate and heal collective trauma. He based this work on recent scientific insights, and called it the Collective Trauma Integration Process (CTIP). The aim of this process goes wider than merely addressing the well-being of individuals suffering from collective trauma: the process is based on the premise that healing collective trauma, collectives (communities, societies, nations) can more effectively transform and prevent further systemic disruptions creating trauma in the first place (wars, various forms of oppression, etc.). This requires an investigation of historic ancestral and transgenerational trauma, and how these create unconscious mechanisms of individual and cultural-collective behaviour. The application of CTIP has shown that when given the right space, hence when a coherent and safe space is created by the facilitator and the group, trauma shows up naturally and can then be grasped for healing. Thomas H\u00fcbl himself has led many of these processes and has this way helped healing collective traumata especially related to World War II and The Holocaust, and worked with groups from more than 40 countries. Furthermore, he highlights the importance of mediation practice and presence for the integration process: \n\u201eWhen we come together in the service of collective trauma integration, presence is the most essential and sacred substance we can bring. Presence opens us and allows the prima materia for integration to flow.\u201c \n\u201eJust as we can feel our physical bodies, presencing permits us to subtly perceive our interior dimensions.\u201c \nGlobal Social Witnessing.\n\u201cGlobal Social Witnessing\u201d is a term that was elaborated by Thomas H\u00fcbl and William Ury in 2017 as a practice of \u201ccontemplative social cognition\u201d. Following their understanding of collective trauma as being at the roots of most conflicts \u2013 although generally in an unrecognised and unconscious way \u2013, they defined Global Social Witnessing as a process of insight in which one has the ability to gain a precise and comprehensive picture of what is happening, which \u2013 they say \u2013 is required to make adequate peace-building and healing possible. \nThomas H\u00fcbl explains the related concept of conscious experience as the ability of a person or a system to have awareness of one\u2019s own current life process. Regarding this, Global Social Witnessing deals with the collective subject\u2019s awareness of its own process. \nOn the individual level, it is the ability of compassion that enables a human to depict the inner life of another being in him. This feeling of compassion is the pre-requisite for truly healing and the potential for promoting action. It also applies on a collective level when one acquires an understanding of the processes happening in society within his/her self. This is one\u2019s transformation into an adult and integrated citizen of a community (nation, culture, etc.). Relating appropriately to the events and processes of the culture is what enables one to come to an appropriate, creative action or response. \nIn this sence, witnessing a traumatic event is also about getting out of a disconnected-unrelated position that is simply pointing out the most obvious culprits, and instead actually acquiring a larger understanding of it as phenomenon of society. Global Social Witnessing can therefore be a point of breakage to the vicious circle of indifference and inadequate responses that follow collective traumatic events, and which, unhealed, continually cause more trauma. "}
{"id": "6325", "revid": "6938183", "url": "https://en.wikipedia.org/wiki?curid=6325", "title": "Church (building)", "text": "A church building, church house, or simply church, is a building used for Christian worship services and other Christian religious activities. The term is used to refer to the physical buildings where Christians worship and also to refer to the community of Christians. Sometimes it is used as an analogy for the buildings of other religions. In traditional Christian architecture the plan view of a church often forms a Christian cross; the center aisle and seating representing the vertical beam with the bema and altar forming the horizontal. Towers or domes may inspire contemplation of the heavens. Modern churches have a variety of architectural styles and layouts. Some buildings designed for other purposes have been converted to churches, while many original church buildings have been put to other uses.\nThe earliest identified Christian church building is a house church founded between 233 and 256. From the 11th through the 14th centuries there was a wave of church construction in western Europe. A cathedral is a church building housing a cathedra, the seat or throne of a presiding bishop.\nEtymology.\nIn Greek, the adjective \"kyriak-\u00f3s/-\u0113/-\u00f3n\" () means \"belonging, or pertaining, to a \"Kyrios\"\" (\"Lord\"), and the usage was adopted by early Christians of the Eastern Mediterranean with regard to anything pertaining to Jesus Christ: hence \"Kyriak\u00f3s o\u00edkos\" () (\"house of the Lord\", church), \"Kyriak\u0113\" () (\"[the day] of the Lord\", i.e. Sunday), or \"Kyriak\u0113 proseukh\u0113\" () (the \"Lord's Prayer\").\nIn standard Greek usage, the older word \"ecclesia\" (, \"ekkles\u00eda\", literally \"assembly\", \"congregation\", or the place where such a gathering occurs) was retained to signify both a specific edifice of Christian worship (a \"church\"), and the overall community of the faithful (the \"Church\"). This usage was also retained in Latin and the languages derived from Latin (e.g. French \"\u00e9glise\", Italian \"chiesa\", Spanish \"iglesia\", Portuguese \"igreja\", etc.), as well as in the Celtic languages (Welsh \"eglwys\", Irish \"eaglais\", Breton \"iliz\", etc.) and in Turkish (\"Kilise\").\nIn the Germanic and some Slavic languages, the word \"kyriak-\u00f3s/-\u0113/-\u00f3n\" was adopted instead and derivatives formed thereof. In Old English the sequence of derivation started as \"cirice\", then Middle English \"churche\", and eventually \"church\" in its current pronunciation. German \"Kirche\", Scots \"kirk\", Russian (\"tserkov\"), Serbo-Croatian \"crkva\", etc., are all similarly derived.\nHistory.\nAntiquity.\nAccording to the New Testament, the earliest Christians did not build church buildings. Instead, they gathered in homes (Acts 17:5, 20:20, 1 Corinthians 16:19) or in Jewish places of worship, like the Second Temple or synagogues (Acts 2:46, 19:8). The earliest archeologically identified Christian church is a house church (\"domus ecclesiae\"), the Dura-Europos church, founded between 233 and 256. In the second half of the 3rd century AD, the first purpose-built halls for Christian worship (\"aula ecclesiae\") began to be constructed. Although many of these were destroyed early in the next century during the Diocletianic Persecution, even larger and more elaborate church buildings began to appear during the reign of the Emperor Constantine the Great.\nMedieval times.\nFrom the 11th through the 14th centuries, a wave of cathedral-building and construction of smaller parish churches occurred across western Europe. Besides serving as a place of worship, the cathedral or parish church was frequently employed as a general gathering-place by the communities in which they were located, hosting such events as guild meetings, banquets, mystery plays, and fairs. Church grounds and buildings were also used for the threshing and storage of grain.\nRomanesque architecture.\nBetween 1000 and 1200 the romanesque style became popular across Europe. While the term \"Romanesque\" refers to the tradition of Roman architecture, the trend in fact appeared throughout western and central Europe. The romanesque style is defined by large and bulky edifices that are typically made up of simple, compact, sparsely decorated geometric structures. Frequent features of the Romanesque church include circular arches, round or octagonal towers and cushion capitals on pillars. In the early romanesque era, coffering on the ceiling was fashionable, while later in the same era, groined vault gained popularity. Interiors widened and the motifs of sculptures took on more epic traits and themes.\nGothic architecture.\nThe Gothic style emerged around 1140 in \u00cele-de-France and subsequently spread throughout Europe. Gothic churches lost the compact qualities of the romanesque era and decorations often contained symbolic and allegorical features. The first pointed arches, rib vaults and buttresses began to appear, all possessing geometric properties that reduced the need for large, rigid walls to ensure structural stability. This also permitted the size of windows to increase, producing brighter and lighter interiors. Nave ceilings became higher and pillars and steeples grew taller. Many architects used these developments to push the limits of structural possibility, an inclination which resulted in the collapse of several towers possessing designs that had unwittingly exceeded the boundaries of soundness. In Germany, the Netherlands, and Spain, it became popular to build hall churches, a style in which every vault would be built to the same height.\nGothic cathedrals were lavishly designed, as in the romanesque era, and many share romanesque traits. However, several also exhibit unprecedented degrees of detail and complexity in decoration. The Notre-Dame de Paris and Notre-Dame de Reims in France, as well as the San Francesco d\u2019Assisi in Palermo, and the Salisbury Cathedral and Wool Church in England demonstrate the elaborate stylings characteristic of Gothic cathedrals.\nSome of the most well-known gothic churches remained unfinished for centuries, after the gothic style fell out of popularity. The construction of the Cologne Cathedral, which was begun in 1248, halted in 1473, and not resumed until 1842 is one such example.\nRenaissance.\nIn the 15th and 16th century, the change in ethics and society due to the Renaissance and the Reformation also influenced the building of churches. The common style was much like the gothic style, but in a simplified way. The basilica was not the most popular type of church anymore, but instead hall churches were built. Typical features are columns and classical capitals.\nIn Protestant churches, where the proclamation of God's Word is of special importance, the visitor's line of view is directed towards the pulpit.\nBaroque architecture.\nThe baroque style was first used in Italy around 1575. From there it spread to the rest of Europe and to the European colonies. During the Baroque era, the building industry increased heavily. Buildings, even churches, were used as indicators for wealth, authority and influence. The use of forms known from the renaissance were extremely exaggerated. Domes and capitals were decorated with moulding and the former stucco sculptures were replaced by fresco paintings on the ceilings. For the first time, churches were seen as one connected work of art and consistent artistic concepts were developed. Instead of long buildings, more central-plan buildings were created. The sprawling decoration with floral ornamentation and mythological motives raised until about 1720 to the Rococo era.\nThe Protestant parishes preferred lateral churches, in which all the visitors could be as close as possible to the pulpit and the altar.\nArchitecture.\nA common architecture for churches is the shape of a cross (a long central rectangle, with side rectangles, and a rectangle in front for the altar space or sanctuary). These churches also often have a dome or other large vaulted space in the interior to represent or draw attention to the heavens. Other common shapes for churches include a circle, to represent eternity, or an octagon or similar star shape, to represent the church's bringing light to the world. Another common feature is the spire, a tall tower on the \"west\" end of the church or over the crossing.\nAnother common feature of many Christian churches is the eastwards orientation of the front altar. \nOften, the altar will not be oriented due east, but in the direction of sunrise. This tradition originated in Byzantium in the 4th century, and became prevalent in the West in the 8th to 9th century. \nThe old Roman custom of having the altar at the west end and the entrance at the east was sometimes followed as late as the 11th century even in areas of northern Europe under Frankish rule, as seen in Petershausen (Constance), Bamberg Cathedral, Augsburg Cathedral, Regensburg Cathedral, and Hildesheim Cathedral.\nTypes.\nBasilica.\nThe Latin word basilica (derived from Greek, \"Basilik\u00e9 Sto\u00e0\", Royal \"Stoa\") was originally used to describe a Roman public building (as in Greece, mainly a tribunal), usually located in the forum of a Roman town.\nAfter the Roman Empire became officially Christian, the term came by extension to refer to a large and important church that has been given special ceremonial rights by the Pope. Thus the word retains two senses today, one architectural and the other ecclesiastical.\nCathedral.\nA cathedral is a church, usually Catholic, Anglican, Oriental Orthodox or Eastern Orthodox, housing the seat of a bishop. The word cathedral takes its name from \"cathedra\", or Bishop's Throne (In ). The term is sometimes (improperly) used to refer to any church of great size.\nA church that has the function of cathedral is not necessarily a large building. It might be as small as Christ Church Cathedral in Oxford, England, Porvoo Cathedral in Porvoo, Finland, Sacred Heart Cathedral in Raleigh, United States, or Chur Cathedral in Switzerland. However, frequently, the cathedral along with some of the abbey churches, was the largest building in any region.\nPilgrimage church.\nA pilgrimage church is a church to which pilgrimages are regularly made, or a church along a pilgrimage route, often located at the tomb of a saints, or holding icons or relics to which miraculous properties are ascribed, the site of Marian apparitions, etc.\nConventual church.\nA conventual church (or monastery church, minster, \"katholikon\") is the main church building in a Christian monastery or abbey.\nCollegiate church.\nA collegiate church is a church where the daily office of worship is maintained by a college of canons, which may be presided over by a dean or provost.\nCollegiate churches were often supported by extensive lands held by the church, or by tithe income from appropriated benefices. They commonly provide distinct spaces for congregational worship and for the choir offices of their clerical community.\nEvangelical church structures.\nThe architecture of evangelical places of worship is mainly characterized by its sobriety. The Latin cross is one of the only spiritual symbols that can usually be seen on the building of an evangelical church and that identifies the place's belonging. Some services take place in theaters, schools or multipurpose rooms, rented for Sunday only. Because of their understanding of the second of the Ten Commandments, evangelicals do not have religious material representations such as statues, icons, or paintings in their places of worship. There is usually a baptistery on the stage of the auditorium (also called sanctuary) or in a separate room for baptisms by immersion.\nAlternative buildings.\nOld and disused church buildings can be seen as an interesting proposition for developers as the architecture and location often provide for attractive homes or city centre entertainment venues On the other hand, many newer churches have decided to host meetings in public buildings such as schools, universities, cinemas or theatres.\nThere is another trend to convert old buildings for worship rather than face the construction costs and planning difficulties of a new build. Unusual venues in the UK include a former tram power station, a former bus garage, a former cinema and bingo hall, a former Territorial Army drill hall, and a former synagogue. served as a floating church for mariners at Liverpool from 1827 until she sank in 1872. A windmill has also been converted into a church at Reigate Heath.\nThere has been an increase in partnerships between church management and private real estate companies to redevelop church properties into mixed uses. While it has garnered criticism from some, the partnership offers congregations the opportunity to increase revenue while preserving the property."}
{"id": "6326", "revid": "735741", "url": "https://en.wikipedia.org/wiki?curid=6326", "title": "Childe's Tomb", "text": "Childe's Tomb is a granite cross on Dartmoor, Devon, England. Although not in its original form, it is more elaborate than most of the crosses on Dartmoor, being raised upon a constructed base, and it is known that a kistvaen is underneath.\nA well-known legend attached to the site, first recorded in 1630 by Tristram Risdon, concerns a wealthy hunter, Childe, who became lost in a snow storm and supposedly died there despite disembowelling his horse and climbing into its body for protection. The legend relates that Childe left a note of some sort saying that whoever found and buried his body would inherit his lands at Plymstock. After a race between the monks of Tavistock Abbey and the men of Plymstock, the Abbey won.\nThe tomb was virtually destroyed in 1812 by a man who stole most of the stones to build a house nearby, but it was partly reconstructed in 1890.\nDescription.\nChilde's Tomb is a reconstructed granite cross on the south-east edge of Foxtor Mires, about 500 metres north of Fox Tor on Dartmoor, Devon, England at . According to William Burt, in his notes to \"Dartmoor, a Descriptive Poem\" by N. T. Carrington (1826), the original tomb consisted of a pedestal of three steps, the lowest of which was built of four stones each six feet long and twelve inches square. The two upper steps were made of eight shorter but similarly shaped stones, and on top was an octagonal block about three feet high with a cross fixed upon it.\nThe tomb lies on the line of several cairns that marked the east-west route of the ancient Monks' Path between Buckfast Abbey and Tavistock Abbey and it was no doubt erected here as part of that route: it would have been particularly useful in this part of the moor with few landmarks where a traveller straying from the path could easily end up in Foxtor Mires. Tristram Risdon, writing in about 1630, said that Childe's Tomb was one of three remarkable things in the Forest of Dartmoor (the others being Crockern Tor and Wistman's Wood). Risdon also stated that the original tomb bore an inscription: \"They fyrste that fyndes and bringes mee to my grave, The priorie of Plimstoke they shall have\", but no sign of this has ever been found.\nToday the cross, which is a replacement, is about tall and across at the crosspiece, and it has its base in a socket stone which rests on a pedestal of granite blocks that raises the total height of the cross to . The original, now broken, socket stone for the cross lies nearby. The whole is surrounded by a circle of granite stones set on their edge which once surrounded the cairn\u2014the rocks of which are now scattered around\u2014that was originally built over a large kistvaen that still exists beneath the pedestal.\nDestruction.\nIn the early 19th century there was much interest in enclosing and \"improving\" the open moorland on Dartmoor, encouraged by Sir Thomas Tyrwhitt's early successes at Tor Royal near Princetown. Enclosure was aided by the greatly enhanced access provided by the construction of the first turnpike roads over the moor: the road between Ashburton and Two Bridges opened in around 1800, for instance. In February 1809 one Thomas Windeatt, from Bridgetown, Totnes, took over the lease of a plot of land (a \"newtake\") of about 582 acres in the valley of the River Swincombe. In 1812 Windeatt started to build a farmhouse, Fox Tor Farm, on his land and his workmen robbed the nearby Childe's Tomb of most of its stones for the building and its doorsteps.\nIn 1902 William Crossing wrote that he had been told by an old moorman that some of the granite blocks from the tomb's pedestal had also been used to make a clapper bridge across a stream flowing into the River Swincombe near the farm. The moorman also said that they had lettering on their undersides. This encouraged Crossing to arrange to lift the clapper bridge, but no inscription was found. However, he did locate nine out of the twelve stones that had made up the pedestal, as well as the broken socket stone for the cross.\nReconstruction.\nCrossing rediscovered the original site of the tomb in 1882 and said that all that remained was a small mound and some half buried stones. He cleared out the kistvaen, reporting that it was long by wide and that unlike most kistvaens found on the moor, the stones lining it had apparently been shaped by man, which led him to suggest that it was less old than most. Having located most of the stones of the original tomb, Crossing thought that it could be rebuilt in its original form with little effort, but it was not to be.\nJ. Brooking Rowe, writing in 1895, states that the tomb was re-erected in 1890 under the direction of Mr. E. Fearnley Tanner, who said that he was dissatisfied with the result because several stones were missing and it was difficult to recreate the original character of the monument. Tanner was the honourable secretary of the Dartmoor Preservation Association, and this reconstruction was one of the first acts of that organisation. The replacement base and cross were made in Holne in 1885.\nChilde the Hunter.\nAccording to legend, the cross was erected over the kistvaen ('chest-stone' i.e. burial chamber) of Childe the Hunter, who was Ordulf, son of Ordgar, an Anglo-Saxon Earl of Devon in the 11th century. The name \"Childe\" is probably derived from the Old English word \"cild\" which was used as a title of honour.\nLegend has it that Childe was in a party hunting on the moor when they were caught in some changeable weather. Childe became separated from the main party and was lost. In order to save himself from dying of exposure, he killed his horse, disembowelled it and crept inside the warm carcass for shelter. He nevertheless froze to death, but before he died, he wrote a note to the effect that whoever should find him and bury him in their church should inherit his Plymstock estate.\nHis body was found by the monks of Tavistock Abbey, who started to carry it back. However, they heard of a plot to ambush them by the people of Plymstock, at a bridge over the River Tavy. They took a detour and built a new bridge over the river, just outside Tavistock. They were successful in burying the body in the grounds of the Abbey and inherited the Plymstock estate.\nThe first account of this story is to be found in Risdon's \"Survey of Devon\" which was completed in around 1632:\nFinberg pointed out, however, that a document of 1651 refers to Tavistock's guildhall as \"Guilehall\", so \"Guilebridge\" is more likely to be \"guild bridge\", probably because it was built or maintained by one of the town guilds.\nIn popular culture.\nDevon folk singer Seth Lakeman sang about Childe the Hunter on his 2006 album \"Freedom Fields\"."}
{"id": "6328", "revid": "6488475", "url": "https://en.wikipedia.org/wiki?curid=6328", "title": "Cognate", "text": "In linguistics, cognates, also called lexical cognates, are words that have a common etymological origin. Cognates are often inherited from a shared parent language, but they may also involve borrowings from some other language. For example, the English words \"dish\", \"disk\" and \"desk\" and the German word \"Tisch\" (\"table\") are cognates because they all come from Latin \"discus\", which relates to their flat surfaces. Cognates may have evolved similar, different or even opposite meanings, and although there are usually some similar sounds or letters in the words, they may appear to be dissimilar. Some words sound similar, but do not come from the same root; these are called false cognates, while some are truly cognate but differ in meaning; these are called false friends.\nThe word \"cognate\" derives from the Latin noun \"cognatus\", which means \"blood relative\".\nCharacteristics.\nCognates do not need to have the same meaning, which may have changed as the languages developed separately. For example English \"starve\" and Dutch \"sterven\" or German \"sterben\" (\"to die\") all derive from the same Proto-Germanic root, \"*sterban\u0105\" (\"die\"). \"Discus\" is from Greek (from the verb \"to throw\"). A later and separate English reflex of \"discus\", probably through medieval Latin , is \"desk\" (see OED s.v. \"desk\").\nAlso, cognates do not need to have similar forms: English \"father\", French \"p\u00e8re\", and Armenian \u0570\u0561\u0575\u0580 (\"hayr\") all descend directly from Proto-Indo-European \"*ph\u2082t\u1e17r\". An extreme case is Armenian \u0565\u0580\u056f\u0578\u0582 (\"erku\") and English \"two\", which descend from Proto-Indo-European \"*dw\u00f3h\u2081\" (note that the sound change \"*dw\" &gt; \"erk\" in Armenian is regular).\nAcross languages.\nExamples of cognates in Indo-European languages are the words \"night\" (English), \"nicht\" (Scots), \"Nacht\" (German), \"nacht\" (Dutch, Frisian), \"nag\" (Afrikaans), \"Naach\" (Colognian), \"natt\" (Swedish, Norwegian), \"nat\" (Danish), \"n\u00e1tt\" (Faroese), \"n\u00f3tt\" (Icelandic), \"noc\" (Czech, Slovak, Polish), \u043d\u043e\u0447\u044c, \"noch\" (Russian), \u043d\u043e\u045c, \"no\u0107\" (Macedonian), \u043d\u043e\u0449, \"nosht\" (Bulgarian), \"nishi\" (Bengali), \"\u043d\u0456\u0447\", \"nich\" (Ukrainian), \"\u043d\u043e\u0447\", \"noch\"/\"no\u010d\" (Belarusian), \"no\u010d\" (Slovene), \"no\u0107\" (Serbo-Croatian), \"nakts\" (Latvian), \"naktis\" (Lithuanian), \u03bd\u03cd\u03be, \"nyx\" (Ancient Greek), \"\u03bd\u03cd\u03c7\u03c4\u03b1\" / \"nychta\" (Modern Greek), \"nakt-\" (Sanskrit), \"nat\u00eb\" (Albanian), \"nos\" (Welsh, Cornish), \"noz\" (Breton), \"nox/nocte\" (Latin), \"nuit\" (French), \"noche\" (Spanish), \"nueche\" (Asturian), \"noite\" (Portuguese and Galician), \"notte\" (Italian), \"nit\" (Catalan), \"nuet/nit/nueit\" (Aragonese), \"nu\u00e8ch\" / \"nu\u00e8it\" (Occitan) and \"noapte\" (Romanian), all meaning \"night\" and being derived from the Proto-Indo-European \"night\".\nAnother Indo-European example is \"star\" (English), \"starn\" (Scots), \"Stern\" (German), \"ster\" (Dutch and Afrikaans), \"stjer\" (Frisian) \"Scht\u00e4hn\" (Colognian), \"stj\u00e4rna\" (Swedish), \"stjerne\" (Norwegian and Danish), \"stjarna\" (Icelandic), \"stj\u00f8rna\" (Faroese), \"stairno\" (Gothic), \"str-\" (Sanskrit), \"tara\" (Hindustani and Bengali), \"tera\" (Sylheti), \"tora\" (Assamese), \"set\u0101re\" (Persian), \"stoorei\" (Pashto), \"est\u00eare\" or \"st\u00eark\" (Kurdish), \"astgh\" (Armenian), \"\u1f00\u03c3\u03c4\u03ae\u03c1 (ast\u0113r)\" (Greek or \"\u1f00\u03c3\u03c4\u03ad\u03c1\u03b9\"/\"\u1f04\u03c3\u03c4\u03c1\u03bf\", \"asteri\"/\"astro\" in Modern Greek), \"astrum\" / \"stell\u0103\" (Latin), \"astre\" / \"\u00e9toile\" (French), \"astro\" / \"stella\" (Italian), \"stea\" (Romanian and Venetian), \"estel\" (Catalan), \"astru\" / \"isteddu\" (Sardinian), \"estela\" (Occitan), \"estrella\" and \"astro\" (Spanish), \"estrella\" (Asturian and Leonese), \"estrela\" and \"astro\" (Portuguese and Galician), \"seren\" (Welsh), \"steren\" (Cornish) and \"sterenn\" (Breton), from the Proto-Indo-European \"star\".\nThe Arabic \"sal\u0101m\", the Hebrew \"shalom\", the Assyrian Neo-Aramaic \"shlama\" and the Amharic \"selam\" (\"peace\") are also cognates, derived from the Proto-Semitic *\u0161al\u0101m- \"peace\".\nCognates may often be less easily recognised than the above examples, and authorities sometimes differ in their interpretations of the evidence. The English word \"milk\" is clearly a cognate of German \"Milch\", Dutch and Afrikaans \"melk\", Russian \u043c\u043e\u043b\u043e\u043a\u043e (moloko), Serbo-Croatian and Slovenian \"mleko\"/\"mlijeko\". On the other hand, French \"lait\", Catalan \"llet\", Italian \"latte\", Romanian \"lapte\", Spanish \"leche\" and \"leite\" (Portuguese and Galician) (all meaning \"milk\") are less-obvious cognates of Ancient Greek \"\" \"g\u00e1laktos\" (genitive singular of \"g\u00e1la\", \"milk\"), a relationship that is more evidently seen through the intermediate Latin \"lac\" \"milk\" as well as the English word \"lactic\" and other terms borrowed from Latin.\nSome cognates are semantic opposites. For instance, while the Hebrew word \"chutzpah\" means \"impudence\", its Classical Arabic cognate \"\u1e25a\u1e63\u0101fah\" means \"sound judgment.\" Another example is English \"empathy\" \"understanding of thoughts\" and Greek \"emp\u00e1theia\" \"malice\".\nWithin the same language.\nCognates within a single language, or \"doublets\", may have meanings that are slightly or even totally different. For example, English \"ward\" and \"guard\" (&lt;PIE \"*wer-\", \"to perceive, watch out for\") are cognates, as are \"shirt\" (garment on top) and \"skirt\" (garment on bottom) (&lt;PIE \"*sker-\", \"to cut\"). In some cases, including this one, one cognate (\"skirt\") has an ultimate source in another language related to English, but the other one (\"shirt\") is native. That happened with many loanwords, such as \"skirt\" in this example, which was borrowed from Old Norse during the Danelaw.\nSometimes both doublets come from other languages, often the same one but at different times. For example, the word \"chief\" (meaning the leader of any group) comes from the Middle French \"chef\" (\"head\"), and its modern pronunciation preserves the Middle French consonant sound; the word \"chef\" (the leader of the cooks) was borrowed from the same source centuries later, but by then, the consonant had changed to a \"sh\" sound in French. Such word sets can also be called etymological twins, and they may come in groups of higher numbers, as with, for example, the words \"wain\" (native), \"waggon/wagon\" (Dutch), and \"vehicle\" (Latin) in English.\nA word may also enter another language, develop a new form or meaning there, and be re-borrowed into the original language; that is called reborrowing. For example, the Greek word (\"k\u00ednima\", \"movement\") became French \"cin\u00e9ma\" (compare American English \"movie\") and then later returned to Greece as (\"sinem\u00e1\", \"the art of film\", \"movie theater\"). In Greek, (\"k\u00ednima\", \"movement\") and (\"sinem\u00e1\", \"filmmaking, cinema\") are now doublets.\nA less obvious English-language doublet pair is \"grammar\" and \"glamour\".\nFalse cognates.\nFalse cognates are words that people commonly believe are related (have a common origin), but that linguistic examination reveals are unrelated. For example, on the basis of superficial similarities, the Latin verb \"hab\u0113re\" and German \"haben\", both meaning 'to have', appear to be cognates. However, because the words evolved from different roots, in this case, different Proto-Indo-European (PIE) roots, they cannot be cognate (see for example Grimm's law). German \"haben\", like English \"have\", comes from PIE \"*kh\u2082py\u00e9-\" 'to grasp', and its real cognate in Latin is \"capere\", 'to seize, grasp, capture'. Latin \"hab\u0113re\", on the other hand, is from PIE \"*g\u02b0ab\u02b0\", 'to give, to receive', and hence cognate with English \"give\" and German \"geben\".\nLikewise, English \"much\" and Spanish \"mucho\" look similar and have a similar meaning but are not cognates, as they evolved from different roots: \"much\" from Proto-Germanic \"*mikilaz\" &lt; PIE \"*me\u01f5-\" and \"mucho\" from Latin \"multum\" &lt; PIE \"*mel-\". Instead, its real cognate is Spanish \"ma\u00f1o\"."}
{"id": "6329", "revid": "38062138", "url": "https://en.wikipedia.org/wiki?curid=6329", "title": "Chromatography", "text": "Chromatography is a laboratory technique for the separation of a mixture.\nThe mixture is dissolved in a fluid (gas, solvent, water, ...) called the \"mobile phase,\" which carries it through a system (a column, a capillary tube, a plate, or a sheet) on which is fixed a material called the \"stationary phase.\" The different constituents of the mixture have different affinities for the stationary phase. The different molecules stay longer or shorter on the stationary phase, depending on their interactions with its surface sites. So, they travel at different apparent velocities in the mobile fluid, causing them to separate. The separation is based on the differential partitioning between the mobile and the stationary phases. Subtle differences in a compound's partition coefficient result in differential retention on the stationary phase and thus affect the separation.\nChromatography may be preparative or analytical. The purpose of preparative chromatography is to separate the components of a mixture for later use, and is thus a form of purification. Analytical chromatography is done normally with smaller amounts of material and is for establishing the presence or measuring the relative proportions of analytes in a mixture. The two are not mutually exclusive.\nEtymology and pronunciation.\nChromatography, pronounced , is derived from Greek \u03c7\u03c1\u1ff6\u03bc\u03b1 \"chroma\", which means \"color\", and \u03b3\u03c1\u03ac\u03c6\u03b5\u03b9\u03bd \"graphein\", which means \"to write\". The combination of these two terms was directly inherited from the invention of the technique first used to separate pigments.\nHistory.\nChromatography was first devised in Russia by the Italian-born scientist Mikhail Tsvet in 1900. He developed the technique, he coined \"chromatography,\" in the first decade of the 20th century, primarily for the separation of plant pigments such as chlorophyll, carotenes, and xanthophylls. Since these components separate in bands of different colors (green, orange, and yellow, respectively) they directly inspired the name of the technique. New types of chromatography developed during the 1930s and 1940s made the technique useful for many separation processes.\nChromatography technique developed substantially as a result of the work of Archer John Porter Martin and Richard Laurence Millington Synge during the 1940s and 1950s, for which they won the 1952 Nobel Prize in Chemistry. They established the principles and basic techniques of partition chromatography, and their work encouraged the rapid development of several chromatographic methods: paper chromatography, gas chromatography, and what would become known as high-performance liquid chromatography. Since then, the technology has advanced rapidly. Researchers found that the main principles of Tsvet's chromatography could be applied in many different ways, resulting in the different varieties of chromatography described below. Advances are continually improving the technical performance of chromatography, allowing the separation of increasingly similar molecules.\nChromatography terms.\nChromatography is based on the concept of partition coefficient. Any solute partitions between two immiscible solvents. When we make one solvent immobile (by adsorption on a solid support matrix) and another mobile it results in most common applications of chromatography. If the matrix support, or stationary phase, is polar (e.g. paper, silica etc.) it is forward phase chromatography, and if it is non-polar (C-18) it is reverse phase.\nTechniques by chromatographic bed shape.\nColumn chromatography.\nColumn chromatography is a separation technique in which the stationary bed is within a tube. The particles of the solid stationary phase or the support coated with a liquid stationary phase may fill the whole inside volume of the tube (packed column) or be concentrated on or along the inside tube wall leaving an open, unrestricted path for the mobile phase in the middle part of the tube (open tubular column). Differences in rates of movement through the medium are calculated to different retention times of the sample.\nIn 1978, W. Clark Still introduced a modified version of column chromatography called \"flash column chromatography\" (flash). The technique is very similar to the traditional column chromatography, except that the solvent is driven through the column by applying positive pressure. This allowed most separations to be performed in less than 20 minutes, with improved separations compared to the old method. Modern flash chromatography systems are sold as pre-packed plastic cartridges, and the solvent is pumped through the cartridge. Systems may also be linked with detectors and fraction collectors providing automation. The introduction of gradient pumps resulted in quicker separations and less solvent usage.\nIn expanded bed adsorption, a fluidized bed is used, rather than a solid phase made by a packed bed. This allows omission of initial clearing steps such as centrifugation and filtration, for culture broths or slurries of broken cells.\nPhosphocellulose chromatography utilizes the binding affinity of many DNA-binding proteins for phosphocellulose. The stronger a protein's interaction with DNA, the higher the salt concentration needed to elute that protein.\nPlanar chromatography.\n\"Planar chromatography\" is a separation technique in which the stationary phase is present as or on a plane. The plane can be a paper, serving as such or impregnated by a substance as the stationary bed (paper chromatography) or a layer of solid particles spread on a support such as a glass plate (thin-layer chromatography). Different compounds in the sample mixture travel different distances according to how strongly they interact with the stationary phase as compared to the mobile phase. The specific Retention factor (Rf) of each chemical can be used to aid in the identification of an unknown substance.\nPaper chromatography.\nPaper chromatography is a technique that involves placing a small dot or line of sample solution onto a strip of \"chromatography paper\". The paper is placed in a container with a shallow layer of solvent and sealed. As the solvent rises through the paper, it meets the sample mixture, which starts to travel up the paper with the solvent. This paper is made of cellulose, a polar substance, and the compounds within the mixture travel further if they are less polar. More polar substances bond with the cellulose paper more quickly, and therefore do not travel as far.\nThin-layer chromatography (TLC).\nThin-layer chromatography (TLC) is a widely employed laboratory technique used to separate different biochemicals on the basis of their relative attractions to the stationary and mobile phases. It is similar to paper chromatography. However, instead of using a stationary phase of paper, it involves a stationary phase of a thin layer of adsorbent like silica gel, alumina, or cellulose on a flat, inert substrate. TLC is very versatile; multiple samples can be separated simultaneously on the same layer, making it very useful for screening applications such as testing drug levels and water purity. Possibility of cross-contamination is low since each separation is performed on a new layer. Compared to paper, it has the advantage of faster runs, better separations, better quantitative analysis, and the choice between different adsorbents. For even better resolution and faster separation that utilizes less solvent, high-performance TLC can be used. An older popular use had been to differentiate chromosomes by observing distance in gel (separation of was a separate step).\nDisplacement chromatography.\nThe basic principle of displacement chromatography is:\nA molecule with a high affinity for the chromatography matrix (the displacer) competes effectively for binding sites, and thus displaces all molecules with lesser affinities.\nThere are distinct differences between displacement and elution chromatography. In elution mode, substances typically emerge from a column in narrow, Gaussian peaks. Wide separation of peaks, preferably to baseline, is desired for maximum purification. The speed at which any component of a mixture travels down the column in elution mode depends on many factors. But for two substances to travel at different speeds, and thereby be resolved, there must be substantial differences in some interaction between the biomolecules and the chromatography matrix. Operating parameters are adjusted to maximize the effect of this difference. In many cases, baseline separation of the peaks can be achieved only with gradient elution and low column loadings. Thus, two drawbacks to elution mode chromatography, especially at the preparative scale, are operational complexity, due to gradient solvent pumping, and low throughput, due to low column loadings. Displacement chromatography has advantages over elution chromatography in that components are resolved into consecutive zones of pure substances rather than \"peaks\". Because the process takes advantage of the nonlinearity of the isotherms, a larger column feed can be separated on a given column with the purified components recovered at significantly higher concentrations.\nTechniques by physical state of mobile phase.\nGas chromatography.\nGas chromatography (GC), also sometimes known as gas-liquid chromatography, (GLC), is a separation technique in which the mobile phase is a gas. Gas chromatographic separation is always carried out in a column, which is typically \"packed\" or \"capillary\". Packed columns are the routine work horses of gas chromatography, being cheaper and easier to use and often giving adequate performance. Capillary columns generally give far superior resolution and although more expensive are becoming widely used, especially for complex mixtures. Further, capillary columns can be split into three classes: porous layer open tubular (PLOT), wall-coated open tubular (WCOT) and support-coated open tubular (SCOT) columns. PLOT columns are unique in a way that the stationary phase is adsorbed to the column walls, while WCOT columns have a stationary phase that is chemically bonded to the walls. SCOT columns are in a way the combination of the two types mentioned in a way that they have support particles adhered to column walls, but those particles have liquid phase chemically bonded onto them. Both types of column are made from non-adsorbent and chemically inert materials. Stainless steel and glass are the usual materials for packed columns and quartz or fused silica for capillary columns.\nGas chromatography is based on a partition equilibrium of analyte between a solid or viscous liquid stationary phase (often a liquid silicone-based material) and a mobile gas (most often helium). The stationary phase is adhered to the inside of a small-diameter (commonly 0.53 \u2013 0.18mm inside diameter) glass or fused-silica tube (a capillary column) or a solid matrix inside a larger metal tube (a packed column). It is widely used in analytical chemistry; though the high temperatures used in GC make it unsuitable for high molecular weight biopolymers or proteins (heat denatures them), frequently encountered in biochemistry, it is well suited for use in the petrochemical, environmental monitoring and remediation, and industrial chemical fields. It is also used extensively in chemistry research.\nLiquid chromatography.\nLiquid chromatography (LC) is a separation technique in which the mobile phase is a liquid. It can be carried out either in a column or a plane. Present day liquid chromatography that generally utilizes very small packing particles and a relatively high pressure is referred to as high-performance liquid chromatography (HPLC).\nIn HPLC the sample is forced by a liquid at high pressure (the mobile phase) through a column that is packed with a stationary phase composed of irregularly or spherically shaped particles, a porous monolithic layer, or a porous membrane. HPLC is historically divided into two different sub-classes based on the polarity of the mobile and stationary phases. Methods in which the stationary phase is more polar than the mobile phase (e.g., toluene as the mobile phase, silica as the stationary phase) are termed normal phase liquid chromatography (NPLC) and the opposite (e.g., water-methanol mixture as the mobile phase and C18 (octadecylsilyl) as the stationary phase) is termed reversed phase liquid chromatography (RPLC).\nSpecific techniques under this broad heading are listed below.\nAffinity chromatography.\nAffinity chromatography is based on selective non-covalent interaction between an analyte and specific molecules. It is very specific, but not very robust. It is often used in biochemistry in the purification of proteins bound to tags. These fusion proteins are labeled with compounds such as His-tags, biotin or antigens, which bind to the stationary phase specifically. After purification, some of these tags are usually removed and the pure protein is obtained.\nAffinity chromatography often utilizes a biomolecule's affinity for a metal (Zn, Cu, Fe, etc.). Columns are often manually prepared. Traditional affinity columns are used as a preparative step to flush out unwanted biomolecules.\nHowever, HPLC techniques exist that do utilize affinity chromatography properties. Immobilized Metal Affinity Chromatography (IMAC) is useful to separate aforementioned molecules based on the relative affinity for the metal (i.e. Dionex IMAC). Often these columns can be loaded with different metals to create a column with a targeted affinity. \nSupercritical fluid chromatography.\nSupercritical fluid chromatography is a separation technique in which the mobile phase is a fluid above and relatively close to its critical temperature and pressure.\nTechniques by separation mechanism.\nIon exchange chromatography.\nIon exchange chromatography (usually referred to as ion chromatography) uses an ion exchange mechanism to separate analytes based on their respective charges. It is usually performed in columns but can also be useful in planar mode. Ion exchange chromatography uses a charged stationary phase to separate charged compounds including anions, cations, amino acids, peptides, and proteins. In conventional methods the stationary phase is an ion-exchange resin that carries charged functional groups that interact with oppositely charged groups of the compound to retain. There are two types of ion exchange chromatography: Cation-Exchange and Anion-Exchange. In the Cation-Exchange Chromatography the stationary phase has negative charge and the exchangeable ion is a cation, whereas, in the Anion-Exchange Chromatography the stationary phase has positive charge and the exchangeable ion is an anion. Ion exchange chromatography is commonly used to purify proteins using FPLC.\nSize-exclusion chromatography.\nSize-exclusion chromatography (SEC) is also known as \"gel permeation chromatography\" (GPC) or \"gel filtration chromatography\" and separates molecules according to their size (or more accurately according to their hydrodynamic diameter or hydrodynamic volume).\nSmaller molecules are able to enter the pores of the media and, therefore, molecules are trapped and removed from the flow of the mobile phase. The average residence time in the pores depends upon the effective size of the analyte molecules. However, molecules that are larger than the average pore size of the packing are excluded and thus suffer essentially no retention; such species are the first to be eluted. It is generally a low-resolution chromatography technique and thus it is often reserved for the final, \"polishing\" step of a purification. It is also useful for determining the tertiary structure and quaternary structure of purified proteins, especially since it can be carried out under native solution conditions.\nExpanded bed adsorption chromatographic separation.\nAn expanded bed chromatographic adsorption (EBA) column for a biochemical separation process comprises a pressure equalization liquid distributor having a self-cleaning function below a porous blocking sieve plate at the bottom of the expanded bed, an upper part nozzle assembly having a backflush cleaning function at the top of the expanded bed, a better distribution of the feedstock liquor added into the expanded bed ensuring that the fluid passed through the expanded bed layer displays a state of piston flow. The expanded bed layer displays a state of piston flow. The expanded bed chromatographic separation column has advantages of increasing the separation efficiency of the expanded bed.\nExpanded-bed adsorption (EBA) chromatography is a convenient and effective technique for the capture of proteins directly from unclarified crude sample. In EBA chromatography, the settled bed is first expanded by upward flow of equilibration buffer. The crude feed, a mixture of soluble proteins, contaminants, cells, and cell debris, is then passed upward through the expanded bed. Target proteins are captured on the adsorbent, while particulates and contaminants pass through. A change to elution buffer while maintaining upward flow results in desorption of the target protein in expanded-bed mode. Alternatively, if the flow is reversed, the adsorbed particles will quickly settle and the proteins can be desorbed by an elution buffer. The mode used for elution (expanded-bed versus settled-bed) depends on the characteristics of the feed. After elution, the adsorbent is cleaned with a predefined cleaning-in-place (CIP) solution, with cleaning followed by either column regeneration (for further use) or storage.\nSpecial techniques.\nReversed-phase chromatography.\nReversed-phase chromatography (RPC) is any liquid chromatography procedure in which the mobile phase is significantly more polar than the stationary phase. It is so named because in normal-phase liquid chromatography, the mobile phase is significantly less polar than the stationary phase. Hydrophobic molecules in the mobile phase tend to adsorb to the relatively hydrophobic stationary phase. Hydrophilic molecules in the mobile phase will tend to elute first. Separating columns typically comprise a C8 or C18 carbon-chain bonded to a silica particle substrate.\nHydrophobic interaction chromatography.\nHydrophobic interactions between proteins and the chromatographic matrix can be exploited to purify proteins. In hydrophobic interaction chromatography the matrix material is lightly substituted with hydrophobic groups. These groups can range from methyl, ethyl, propyl, octyl, or phenyl groups. At high salt concentrations, non-polar sidechains on the surface on proteins \"interact\" with the hydrophobic groups; that is, both types of groups are excluded by the polar solvent (hydrophobic effects are augmented by increased ionic strength). Thus, the sample is applied to the column in a buffer which is highly polar. The eluant is typically an aqueous buffer with decreasing salt concentrations, increasing concentrations of detergent (which disrupts hydrophobic interactions), or changes in pH.\nIn general, Hydrophobic Interaction Chromatography (HIC) is advantageous if the sample is sensitive to pH change or harsh solvents typically used in other types of chromatography but not high salt concentrations. Commonly, it is the amount of salt in the buffer which is varied. In 2012, M\u00fcller and Franzreb described the effects of temperature on HIC using Bovine Serum Albumin (BSA) with four different types of hydrophobic resin. The study altered temperature as to effect the binding affinity of BSA onto the matrix. It was concluded that cycling temperature from 50 to 10 degrees would not be adequate to effectively wash all BSA from the matrix but could be very effective if the column would only be used a few times. Using temperature to effect change allows labs to cut costs on buying salt and saves money.\nIf high salt concentrations along with temperature fluctuations want to be avoided you can use a more hydrophobic to compete with your sample to elute it. [source] This so-called salt independent method of HIC showed a direct isolation of Human Immunoglobulin G (IgG) from serum with satisfactory yield and used Beta-cyclodextrin as a competitor to displace IgG from the matrix. This largely opens up the possibility of using HIC with samples which are salt sensitive as we know high salt concentrations precipitate proteins.\nHydrodynamic chromatography.\nHydrodynamic chromatography (HDC) is derived from the observed phenomenon that large droplets move faster than small ones. In a column, this happens because the center of mass of larger droplets is prevented from being as close to the sides of the column as smaller droplets because of their larger overall size. Larger droplets will elute first from the middle of the column while smaller droplets stick to the sides of the column and elute last. This form of chromatography is useful for separating analytes by molar mass, size, shape, and structure when used in conjunction with light scattering detectors, viscometers, and refractometers. The two main types of HDC are open tube and packed column. Open tube offers rapid separation times for small particles, whereas packed column HDC can increase resolution and is better suited for particles with an average molecular mass larger than formula_1 daltons. HDC differs from other types of chromatography because the separation only takes place in the interstitial volume, which is the volume surrounding and in between particles in a packed column.\nHDC shares the same order of elution as Size Exclusion Chromatography (SEC) but the two processes still vary in many ways. In a study comparing the two types of separation, Isenberg, Brewer, C\u00f4t\u00e9, and Striegel use both methods for polysaccharide characterization and conclude that HDC coupled with multiangle light scattering (MALS) achieves more accurate molar mass distribution when compared to off-line MALS than SEC in significantly less time. This is largely due to SEC being a more destructive technique because of the pores in the column degrading the analyte during separation, which tends to impact the mass distribution. However, the main disadvantage of HDC is low resolution of analyte peaks, which makes SEC a more viable option when used with chemicals that are not easily degradable and where rapid elution is not important.\nHDC plays an especially important role in the field of microfluidics. The first successful apparatus for HDC-on-a-chip system was proposed by Chmela, et al. in 2002. Their design was able to achieve separations using an 80\u00a0mm long channel on the timescale of 3 minutes for particles with diameters ranging from 26 to 110\u00a0nm, but the authors expressed a need to improve the retention and dispersion parameters. In a 2010 publication by Jellema, Markesteijn, Westerweel, and Verpoorte, implementing HDC with a recirculating bidirectional flow resulted in high resolution, size based separation with only a 3\u00a0mm long channel. Having such a short channel and high resolution was viewed as especially impressive considering that previous studies used channels that were 80\u00a0mm in length. For a biological application, in 2007, Huh, et al. proposed a microfluidic sorting device based on HDC and gravity, which was useful for preventing potentially dangerous particles with diameter larger than 6 microns from entering the bloodstream when injecting contrast agents in ultrasounds. This study also made advances for environmental sustainability in microfluidics due to the lack of outside electronics driving the flow, which came as an advantage of using a gravity based device.\nTwo-dimensional chromatography.\nIn some cases, the selectivity provided by the use of one column can be insufficient to provide resolution of analytes in complex samples. Two-dimensional chromatography aims to increase the resolution of these peaks by using a second column with different physico-chemical (chemical classification) properties. Since the mechanism of retention on this new solid support is different from the first dimensional separation, it can be possible to separate compounds by two-dimensional chromatography that are indistinguishable by one-dimensional chromatography. Furthermore, the separation on the second dimension occurs faster than the first dimension. An example of a two-dimensional TLC separation is where the sample is spotted at one corner of a square plate, developed, air-dried, then rotated by 90\u00b0 and usually redeveloped in a second solvent system. Two-dimensional chromatography can be applied to GC or LC separations. This separation method can also be used in a heart-cutting approach, where specific regions of interest on the first dimension are selected for separation by the second dimension, or in a comprehensive approach, where all the analytes from the first dimension undergo the second dimension separation.\nSimulated moving-bed chromatography.\nThe simulated moving bed (SMB) technique is a variant of high performance liquid chromatography; it is used to separate particles and/or chemical compounds that would be difficult or impossible to resolve otherwise. This increased separation is brought about by a valve-and-column arrangement that is used to lengthen the stationary phase indefinitely.\nIn the moving bed technique of preparative chromatography the feed entry and the analyte recovery are simultaneous and continuous, but because of practical difficulties with a continuously moving bed, simulated moving bed technique was proposed. In the simulated moving bed technique instead of moving the bed, the sample inlet and the analyte exit positions are moved continuously, giving the impression of a moving bed.\nTrue moving bed chromatography (TMBC) is only a theoretical concept. Its simulation, SMBC is achieved by the use of a multiplicity of columns in series and a complex valve arrangement, which provides for sample and solvent feed, and also analyte and waste takeoff at appropriate locations of any column, whereby it allows switching at regular intervals the sample entry in one direction, the solvent entry in the opposite direction, whilst changing the analyte and waste takeoff positions appropriately as well.\nPyrolysis gas chromatography.\nPyrolysis\u2013gas chromatography\u2013mass spectrometry is a method of chemical analysis in which the sample is heated to decomposition to produce smaller molecules that are separated by gas chromatography and detected using mass spectrometry.\nPyrolysis is the thermal decomposition of materials in an inert atmosphere or a vacuum. The sample is put into direct contact with a platinum wire, or placed in a quartz sample tube, and rapidly heated to 600\u20131000\u00a0\u00b0C. Depending on the application even higher temperatures are used. Three different heating techniques are used in actual pyrolyzers: Isothermal furnace, inductive heating (Curie Point filament), and resistive heating using platinum filaments. Large molecules cleave at their weakest points and produce smaller, more volatile fragments. These fragments can be separated by gas chromatography. Pyrolysis GC chromatograms are typically complex because a wide range of different decomposition products is formed. The data can either be used as fingerprint to prove material identity or the GC/MS data is used to identify individual fragments to obtain structural information. To increase the volatility of polar fragments, various methylating reagents can be added to a sample before pyrolysis.\nBesides the usage of dedicated pyrolyzers, pyrolysis GC of solid and liquid samples can be performed directly inside Programmable Temperature Vaporizer (PTV) injectors that provide quick heating (up to 30\u00a0\u00b0C/s) and high maximum temperatures of 600\u2013650\u00a0\u00b0C. This is sufficient for some pyrolysis applications. The main advantage is that no dedicated instrument has to be purchased and pyrolysis can be performed as part of routine GC analysis. In this case quartz GC inlet liners have to be used. Quantitative data can be acquired, and good results of derivatization inside the PTV injector are published as well.\nFast protein liquid chromatography.\nFast protein liquid chromatography (FPLC), is a form of liquid chromatography that is often used to analyze or purify mixtures of proteins. As in other forms of chromatography, separation is possible because the different components of a mixture have different affinities for two materials, a moving fluid (the \"mobile phase\") and a porous solid (the stationary phase). In FPLC the mobile phase is an aqueous solution, or \"buffer\". The buffer flow rate is controlled by a positive-displacement pump and is normally kept constant, while the composition of the buffer can be varied by drawing fluids in different proportions from two or more external reservoirs. The stationary phase is a resin composed of beads, usually of cross-linked agarose, packed into a cylindrical glass or plastic column. FPLC resins are available in a wide range of bead sizes and surface ligands depending on the application.\nCountercurrent chromatography.\nCountercurrent chromatography (CCC) is a type of liquid-liquid chromatography, where both the stationary and mobile phases are liquids and the liquid stationary phase is held stagnant by a strong centrifugal force.\nHydrodynamic countercurrent chromatography (CCC).\nThe operating principle of CCC instrument requires a column consisting of an open tube coiled around a bobbin. The bobbin is rotated in a double-axis gyratory motion (a cardioid), which causes a variable gravity (G) field to act on the column during each rotation. This motion causes the column to see one partitioning step per revolution and components of the sample separate in the column due to their partitioning coefficient between the two immiscible liquid phases used. There are many types of CCC available today. These include HSCCC (High Speed CCC) and HPCCC (High Performance CCC). HPCCC is the latest and best-performing version of the instrumentation available currently.\nHydrostatic countercurrent chromatography or centrifugal partition chromatography (CPC).\nIn the CPC instrument, the column consists of a series of cells interconnected by ducts attached to a rotor. This rotor rotates on its central axis creating the centrifugal field necessary to hold the stationary phase in place. The separation process in CPC is governed solely by the partitioning of solutes between the stationary and mobile phases, which mechanism can be easily described using the partition coefficients (\"KD\") of solutes. CPC instruments are commercially available for laboratory, pilot, and industrial-scale separations with different sizes of columns ranging from some 10 milliliters to 10 liters volume.\nPeriodic counter-current chromatography.\nIn contrast to Counter current chromatography (see above), periodic counter-current chromatography (PCC) uses a solid stationary phase and only a liquid mobile phase. It thus is much more similar to conventional affinity chromatography than to counter current chromatography. PCC uses multiple columns, which during the loading phase are connected in line. This mode allows for overloading the first column in this series without losing product, which already breaks through the column before the resin is fully saturated. The breakthrough product is captured on the subsequent column(s). In a next step the columns are disconnected from one another. The first column is washed and eluted, while the other column(s) are still being loaded. Once the (initially) first column is re-equilibrated, it is re-introduced to the loading stream, but as last column. The process then continues in a cyclic fashion.\nChiral chromatography.\nChiral chromatography involves the separation of stereoisomers. In the case of enantiomers, these have no chemical or physical differences apart from being three-dimensional mirror images. Conventional chromatography or other separation processes are incapable of separating them. To enable chiral separations to take place, either the mobile phase or the stationary phase must themselves be made chiral, giving differing affinities between the analytes. Chiral chromatography HPLC columns (with a chiral stationary phase) in both normal and reversed phase are commercially available.\nAqueous normal-phase chromatography.\nAqueous normal-phase (ANP) chromatography is characterized by the elution behavior of classical normal phase mode (i.e. where the mobile phase is significantly less polar than the stationary phase) in which water is one of the mobile phase solvent system components. It is distinguished from hydrophilic interaction liquid chromatography (HILIC) in that the retention mechanism is due to adsorption rather than partitioning."}
{"id": "6330", "revid": "753665", "url": "https://en.wikipedia.org/wiki?curid=6330", "title": "Clement Martyn Doke", "text": "Clement Martyn Doke (16 May 1893 in Bristol, United Kingdom \u2013 24 February 1980 in East London, South Africa) was a South African linguist working mainly on African languages. Realizing that the grammatical structures of Bantu languages are quite different from those of European languages, he was one of the first African linguists of his time to abandon the Euro-centric approach to language description for a more locally grounded one. A most prolific writer, he published a string of grammars, several dictionaries, comparative work, and a history of Bantu linguistics.\nMissionary in Lambaland.\nThe Doke family had been engaged in missionary activity for the Baptist Church for some generations. His father Reverend Joseph J. Doke left England and travelled to South Africa in 1882, where he met and married Agnes Biggs. They returned to England, where Clement was born as the third of four children. The family moved to New Zealand and eventually returned to South Africa in 1903, where they later on settled in Johannesburg.\nAt the age of 18, Clement received a bachelor's degree from Transvaal University College in Pretoria (now the University of Pretoria). He decided to devote his life to missionary activity. In 1913, he accompanied his father on a tour of north-western Rhodesia, to an area called Lambaland, now known as Ilamba. It is situated at the watershed of the Congo and Zambesi rivers, part of the district lay in Northern Rhodesia and part in the Belgian Congo State. The Cape-Cairo Railway threaded through its eastern portion; otherwise, travelling mostly had to be done on foot.\nThe Reverend William Arthur Phillips of the Nyasa Industrial Mission in Blantyre had established a Baptist mission there in 1905, serving an area of and 50,000 souls. The Dokes were supposed to investigate, whether the mission in Lambaland could be taken over by the Baptist Union of South Africa. It was on this trip that Doke's father contracted enteric fever and died soon afterwards (Gandhi attended the memorial service and addressed the congregation). Clement assumed his father's role.\nThe South African Baptists decided to take over Kafulafuta Mission, while its founder Reverend Phillips remained as superintendent. Clement Doke returned to Kafulafuta as missionary in 1914, followed by his sister Olive two years later.\nThe Lamba language.\nAt first, Clement Doke was frustrated by his inability to communicate with the Lamba. The only written material available at the time was a translation of Jonah and a collection of 47 hymns. Soon he mastered the language and published his first book \"Ifintu Fyakwe Lesa\" (The Things of God, a Primer of Scripture Knowledge) in 1917. He enrolled in Johannesburg as the extension of Transvaal University College for an MA degree. His thesis was published as \"The Grammar of the Lamba language\". The book is couched in traditional grammatical terms as Doke had not yet established his innovative method of analysis and description for the Bantu languages. His later \"Textbook of Lamba Grammar\" is far superior in this respect.\nClement Doke was also interested in ethnology. In 1931 he compiled \"The Lambas of Northern Rhodesia\", which remains one of the outstanding ethnographic descriptions of the peoples of Central Africa. For Doke, literacy was part of the evangelisation since people had to be able to read to appreciate the message of the Bible, but it was only after his retirement that he completed the translation of the Bible into Lamba. It was published under the title of \"Amasiwi AwaLesa\" (The Words of God) in 1959.\nUniversity of the Witwatersrand.\nIn 1919 Doke married Hilda Lehmann, who accompanied him back to Lambaland. They both contracted malaria during their work and she was forbidden to return to Lambaland. Clement Doke also realised that his field work couldn't continue much longer and left in 1921. He was recruited by the newly founded University of the Witwatersrand. In order to secure a qualification as a lecturer, the family moved to England, where he registered at the School of Oriental and African Studies. His major languages were Lamba and Luba, but as no suitable examiner was available, he eventually had to change his language to Zulu.\nDoke took up his appointment in the new Department of Bantu Studies at the University of Witwatersrand in 1923. In 1925 he received his D. Litt. for his doctoral thesis \"The Phonetics of the Zulu Language\" and was promoted to Senior Lecturer. In 1931 he was appointed to the Chair of Bantu Studies and thus headed the Department of Bantu Studies. The Department acted as a catalyst for the admission of Africans to the University: as early as 1925 a limited number were admitted to the vacation course in African Studies. Doke supported the appointment of Benedict Wallet Vilakazi as member of the staff, as he believed a native speaker was essential for acquiring a language. This provoked a storm of criticism and controversy from the public. They both collaborated on the \"Zulu-English Dictionary\", first published in 1948. It is still one of the best examples of lexicography for any of the Bantu languages.\nAt the request of the government of Southern Rhodesia, Doke investigated the range of dialect diversity among the languages of the country and made recommendations for \"Unified Shona\". This formed the basis for Standard Shona. He devised a unified orthography based on the Zezuru, Karanga and Manyika dialects. However, Doke's orthography was never fully accepted and the South African government introduced an alternative, leaving Shona with two competing orthographies between 1935 and 1955.\nDuring his tenure Doke developed and promoted a method of linguistic analysis and description of the Bantu languages that was based upon the structure of these languages. The \"Dokean model\" continues to be one of the dominant models of linguistic description in Southern and Central Africa. His classification of the Bantu languages was for many years the dominant view of the interrelations among the African languages. He was also an early describer of Khoisan and Bantu click consonants, devising phonetic symbols for a number of them.\nDoke served the University of the Witwatersrand until his retirement in 1953. He was awarded the honorary degree of Doctor of Letters by Rhodes University and the honorary degree of Doctor of Laws by the University of the Witwatersrand in 1972.\nThe former missionary always remained devoted to the Baptist Church. He was elected President of the South African Baptist Union in 1949 and spent a year visiting churches and mission stations. He used his presidential address in condemning the recently established apartheid policy: \"I solemnly warn the Government that the spirit behind their apartheid legislation, and the way in which they are introducing discriminatory measures of all types today, will bring disaster upon this fair land of ours.\""}
{"id": "6331", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=6331", "title": "Carl Meinhof", "text": "Carl Friedrich Michael Meinhof (July 23, 1857 \u2013 February 11, 1944) was a German linguist and one of the first linguists to study African languages.\nEarly years and career.\nMeinhof was born in Barzwitz near R\u00fcgenwalde in the Province of Pomerania. He studied at the University of T\u00fcbingen and at the University of Greifswald. In 1905 he became professor at the School of Oriental Studies in Berlin. On 5 May 1933 he became a member of the Nazi Party.\nWorks.\nHis most notable work was developing comparative grammar studies of the Bantu languages, building on the pioneering work of Wilhelm Bleek. In his work, Meinhof looked at the common Bantu languages such as Swahili and Zulu to determine similarities and differences.\nIn his work, Meinhof looked at noun classes with all Bantu languages having at least 10 classes and with 22 classes of nouns existing throughout the Bantu languages, though his definition of noun class differs slightly from the accepted one, considering the plural form of a word as belonging to a different class from the singular form (thus leading, for example, to consider a language like French as having four classes instead of two). While no language has all 22 (later: 23) classes active, Venda has 20, Lozi has 18, and Ganda has 16 or 17 (depending on whether the locative class 23 \"e-\" is included). All Bantu languages have a noun class specifically for humans (sometimes including other animate beings).\nMeinhof also examined other African languages, including groups classified at the time as Kordofanian, Bushman, Khoikhoi, and Hamitic.\nMeinhof developed a comprehensive classification scheme for African languages. His classification was the standard one for many years (Greenberg 1955:3). It was replaced by those of Joseph Greenberg in 1955 and in 1963.\nIn 1902, Meinhof made recordings of East African music. These are among the first recordings made of traditional African music.\nControversial views.\nIn 1912, Carl Meinhof published \"Die Sprachen der Hamiten\" (The Languages of the Hamites). He used the term Hamitic. Meinhof's system of classification of the Hamitic languages was based on a belief that \"speakers of Hamitic became largely coterminous with cattle herding peoples with essentially Caucasian origins, intrinsically different from and superior to the 'Negroes of Africa'.\" However, in the case of the so-called Nilo-Hamitic languages (a concept he introduced), it was based on the typological feature of gender and a \"fallacious theory of language mixture.\" Meinhof did this in spite of earlier work by scholars such as Lepsius and Johnston demonstrating that the languages which he would later dub \"Nilo-Hamitic\" were in fact Nilotic languages with numerous similarities in vocabulary with other Nilotic languages.\nFamily.\nCarl Meinhof was the great-uncle (the brother of the grandfather) of Ulrike Meinhof, a founding member of the German Red Army Faction (RAF), a left-wing militant group, which operated in West Germany in the 1970s and 1980s."}
{"id": "6335", "revid": "2153309", "url": "https://en.wikipedia.org/wiki?curid=6335", "title": "Cucurbitaceae", "text": "The Cucurbitaceae, also called cucurbits or the gourd family, are a plant family consisting of about 965 species in around 95 genera, of which the most important to humans are:\nThe plants in this family are grown around the tropics and in temperate areas, where those with edible fruits were among the earliest cultivated plants in both the Old and New Worlds. The family Cucurbitaceae ranks among the highest of plant families for number and percentage of species used as human food. The name \"Cucurbitaceae\" comes to international scientific vocabulary from New Latin, from \"Cucurbita\", the type genus, + \"-aceae\", a standardized suffix for plant family names in modern taxonomy. The genus name comes from the Classical Latin word \"cucurbita\", \"gourd\".\nDescription.\nMost of the plants in this family are annual vines, but some are woody lianas, thorny shrubs, or trees (\"Dendrosicyos\"). Many species have large, yellow or white flowers. The stems are hairy and pentangular. Tendrils are present at 90\u00b0 to the leaf petioles at nodes. Leaves are exstipulate alternate simple palmately lobed or palmately compound. The flowers are unisexual, with male and female flowers on different plants (dioecious) or on the same plant (monoecious). The female flowers have inferior ovaries. The fruit is often a kind of modified berry called a pepo.\nFossil history.\nOne of the oldest fossil cucurbits so far is \u2020\"Cucurbitaciphyllum lobatum\" from the Paleocene epoch, found at Shirley Canal, Montana. It was described for the first time in 1924 by the paleobotanist Frank Hall Knowlton. The fossil leaf is palmate, trilobed with rounded lobal sinuses and an entire or serrate margin. It has a leaf pattern similar to the members of the genera \"Kedrostis\", \"Melothria\" and \"Zehneria\".\nClassification.\nTribal classification.\nThe most recent classification of Cucurbitaceae delineates 15 tribes:\nSystematics.\nModern molecular phylogenetics suggest the following relationships:\n! style=\"background:#F0F2F5\" |Detailed Cladogram showing Cucurbitaceae phylogeny"}
{"id": "6336", "revid": "1024059600", "url": "https://en.wikipedia.org/wiki?curid=6336", "title": "Chorded keyboard", "text": "A keyset or chorded keyboard (also called a chorded keyset, \"chord keyboard\" or \"chording keyboard\") is a computer input device that allows the user to enter characters or commands formed by pressing several keys together, like playing a \"chord\" on a piano. The large number of combinations available from a small number of keys allows text or commands to be entered with one hand, leaving the other hand free. A secondary advantage is that it can be built into a device (such as a pocket-sized computer or a bicycle handlebar) that is too small to contain a normal-sized keyboard.\nA chorded keyboard minus the board, typically designed to be used while held in the hand, is called a keyer. Douglas Engelbart introduced the chorded keyset as a computer interface in 1968 at what is often called \"The Mother of All Demos\".\nPrinciples of operation.\nEach key is mapped to a number and then can be mapped to a corresponding letter or command. By pressing two or more keys together the user can generate many combinations. In Engelbart's original mapping, he used five keys: 1, 2, 4, 8, 16. The keys were mapped as follows: a = 1, b = 2, c = 3, d = 4, and so on. If the user pressed keys 1 + 2 = 3 simultaneously, and then released the keys, the letter \"c\" appeared. Unlike pressing a chord on a piano, the chord is recognized only after all the keys or mouse buttons are released. Since Engelbart introduced the keyset, several different designs have been developed based on similar concepts.\nAs a crude example, each finger might control one key which corresponds to one bit in a byte, so that using seven keys and seven fingers, one could enter any character in the ASCII set\u2014if the user could remember the binary codes. Due to the small number of keys required, chording is easily adapted from a desktop to mobile environment.\nPractical devices generally use simpler chords for common characters (\"e.g.,\" Baudot), or may have ways to make it easier to remember the chords (\"e.g.,\" Microwriter), but the same principles apply. These portable devices first became popular with the wearable computer movement in the 1980s.\nThad Starner from Georgia Institute of Technology and others published numerous studies showing that two-handed chorded text entry was faster and yielded fewer errors than on a QWERTY keyboard. Currently stenotype machines hold the record for fastest word entry. Many stenotype users can reach 300 words per minute. However, stenographers typically train for three years before reaching professional levels of speed and accuracy.\nHistory.\nThe earliest known chord keyboard was part of the \"five-needle\" telegraph operator station, designed by Wheatstone and Cooke in 1836, in which any two of the five needles could point left or right to indicate letters on a grid. It was designed to be used by untrained operators (who would determine which keys to press by looking at the grid), and was not used where trained telegraph operators were available.\nThe first widespread use of a chord keyboard was in the stenotype machine used by court reporters, which was invented in 1868 and is still in use. The output of the stenotype was originally a phonetic code that had to be transcribed later (usually by the same operator who produced the original output), rather than arbitrary text\u2014automatic conversion software is now commonplace.\nIn 1874, the five-bit Baudot telegraph code and a matching 5-key chord keyboard was designed to be used with the operator forming the codes manually. The code is optimized for speed and low wear: chords were chosen so that the most common characters used the simplest chords. But telegraph operators were already using typewriters with QWERTY keyboards to \"copy\" received messages, and at the time it made more sense to build a typewriter that could generate the codes automatically, rather than making them learn to use a new input device.\nSome early keypunch machines used a keyboard with 12 labeled keys to punch the correct holes in paper cards. The numbers 0 through 9 were represented by one punch; 26 letters were represented by combinations of two punches, and symbols were represented by combinations of two or three punches.\nBraille (a writing system for the blind) uses either 6 or 8 tactile 'points' from which all letters and numbers are formed. When Louis Braille invented it, it was produced with a needle holing successively all needed points in a cardboard sheet. In 1892, Frank Haven Hall, superintendent of the Illinois Institute for the Education of the Blind, created the Hall Braille Writer, which was like a typewriter with 6 keys, one for each dot in a braille cell. The Perkins Brailler, first manufactured in 1951, uses a 6-key chord keyboard (plus a spacebar) to produce braille output, and has been very successful as a mass market affordable product. Braille, like Baudot, uses a number symbol and a shift symbol, which may be repeated for shift lock, to fit numbers and upper case into the 63 codes that 6 bits offer.\nAfter World War II, with the arrival of electronics for reading chords and looking in tables of \"codes\", the postal sorting offices started to research chordic solutions to be able to employ people other than trained and expensive typists. In 1954, an important concept was discovered: chordic production is easier to master when the production is done at the release of the keys instead of when they are pressed.\nResearchers at IBM investigated chord keyboards for both typewriters and computer data entry as early as 1959, with the idea that it might be faster than touch-typing if some chords were used to enter whole words or parts of words. A 1975 design by IBM Fellow Nat Rochester had 14 keys that were dimpled on the edges as well as the top, so one finger could press two adjacent keys for additional combinations. Their results were inconclusive, but research continued until at least 1978.\nDoug Engelbart began experimenting with keysets to use with the mouse in the mid 1960s. In a famous 1968 demonstration, Engelbart introduced a computer human interface that included the QWERTY keyboard, a three button mouse, and a five key keyset. Engelbart used the keyset with his left hand and the mouse with his right to type text and enter commands. The mouse buttons marked selections and confirmed or aborted commands.\nUsers in Engelbart's Augmentation Research Center at SRI became proficient with the mouse and keyset. In the 1970s the funding Engelbart's group received from the Advanced Research Projects Agency (ARPA) was cut and many key members of Engelbart's team went to work for Xerox PARC where they continued to experiment with the mouse and keyset. Keychord sets were used at Xerox PARC in the early 1980s, along with mice, GUIs, on the Xerox Star and Alto workstations. A one button version of the mouse was incorporated into the Apple Macintosh but Steve Jobs decided against incorporating the chorded keyset.\nIn the early 1980s, Philips Research labs at Redhill, Surrey did a brief study into small, cheap keyboards for entering text on a telephone. One solution used a grid of hexagonal keys with symbols inscribed into dimples in the keys that were either in the center of a key, across the boundary of two keys, or at the joining of three keys. Pressing down on one of the dimples would cause either one, two or three of the hexagonal buttons to be depressed at the same time, forming a chord that would be unique to that symbol. With this arrangement, a nine button keyboard with three rows of three hexagonal buttons could be fitted onto a telephone and could produce up to 33 different symbols. By choosing widely separated keys, one could employ one dimple as a 'shift' key to allow both letters and numbers to be produced. With eleven keys in a 3/4/4 arrangement, 43 symbols could be arranged allowing for lowercase text, numbers and a modest number of punctuation symbols to be represented along with a 'shift' function for accessing uppercase letters. While this had the advantage of being usable by untrained users via 'hunt and peck' typing and requiring one less key switch than a conventional 12 button keypad, it had the disadvantage that some symbols required three times as much force to depress them as others which made it hard to achieve any speed with the device. That solution is still alive and proposed by Fastap and Unitap among others, and a commercial phone has been produced and promoted in Canada during 2006.\nStandards.\nHistorically, the baudot and braille keyboards were standardized to some extent, but they are unable to replicate the full character set of a modern keyboard. Braille comes closest, as it has been extended to eight bits.\nThe only proposed modern standard, GKOS (or Global Keyboard Open Standard) can support most characters and functions found on a computer keyboard but has had little commercial development. There is, however, a GKOS keyboard application available for iPhone since May 8, 2010, for Android since October 3, 2010 and for MeeGo Harmattan since October 27, 2011.\nOpen-source designs.\nFour open-source keyer/keyset designs are available: The pickey, a PS/2 device based on the PIC microcontroller; the spiffchorder, a USB device based on the Atmel AVR family of microcontrollers; the FeatherChorder, a BLE chorder based on the Adafruit Feather, an all-in-one board incorporating an Arduino-compatible microcontroller; and the GKOS keypad driver for Linux as well as the Gkos library for the Atmel/Arduino open-source board.\nPlover is a free, open-source, cross-platform program intended to bring real-time stenographic technology not just to stenographers, but also to hobbyists using anything from professional Stenotype machines to low-cost NKRO gaming keyboards. It is available for Linux, Microsoft Windows, and Apple Mac macOS.\nJoy2chord is a chorded keyboard driver for Linux. With a configuration file, any joystick or gamepad can be turned into a chorded keyboard. This design philosophy was decided on to lower the cost of building devices, and in turn lower the entry barrier to becoming familiar with chorded keyboards. Macro keys, and multiple modes are also easily implemented with a user space driver.\nCommercial devices.\nOne minimal chordic keyboard example is Edgar Matias' Half-Qwerty keyboard described in patent circa 1992 that produces the letters of the missing half when the user simultaneously presses the space bar along with the mirror key. INTERCHI '93 published a study by Matias, MacKenzie and Buxton showing that people who have already learned to touch-type can quickly recover 50 to 70% of their two-handed typing speed. The loss contributes to the speed discussion above. It is implemented on two popular mobile phones, each provided with software disambiguation, which allows users to avoid using the space-bar.\n\"Multiambic\" keyers for use with wearable computers were invented in Canada in the 1970s. Multiambic keyers are similar to chording keyboards but without the board, in that the keys are grouped in a cluster for being handheld, rather than for sitting on a flat surface.\nChording keyboards are also used as portable but two handed input devices for the visually impaired (either combined with a refreshable braille display or vocal synthesis). Such keyboards use a minimum of seven keys, where each key corresponds to an individual braille point, except one key which is used as a spacebar. In some applications, the spacebar is used to produce additional chords which enable the user to issue editing commands, such as moving the cursor, or deleting words. Note that the number of points used in braille computing is not 6, but 8, as this allows the user, among other things, to distinguish between small and capital letters, as well as identify the position of the cursor. As a result, most newer chorded keyboards for braille input include at least nine keys.\nTouch screen chordic keyboards are available to smartphone users as an optional way of entering text. As the number of keys is low the button areas can be made bigger and easier to hit on the small screen. The most common letters do not necessarily require chording as is the case with the GKOS keyboard optimised layouts (Android app) where the twelve most frequent characters only require single keys.\nHistorical.\nThe WriteHander, a 12-key chord keyboard from NewO Company, appeared in 1978 issues of ROM Magazine, an early microcomputer applications magazine.\nAnother early commercial model was the six-button Microwriter, designed by Cy Endfield and Chris Rainey, and first sold in 1980. Microwriting is the system of chord keying and is based on a set of mnemonics. It was designed only for right-handed use.\nIn 1982 the Octima 8 keys cord keyboard was presented by Ergoplic Kebords Ltd an Israeli Startup that was founded by Israeli researcher with intensive experience in Man Machine Interface design. The keyboard had 8 keys one for each finger and additional 3 keys that enabled the production of numbers, punctuations and control functions. The keyboard was fully compatible with the IBM PC &amp; AT keyboards and had an Apple IIe version as well. Its key combinations were based on a mnemonic system that enabled fast and easy touch type learning. Within a few hours the user could achieve a typing speed similar to hand writing speed. The unique design also gave a relief from hand stress (Carpal Tunnel Syndrome) and allowed longer typing sessions than traditional keyboards. It was multi-lingual supporting English, German, French and Hebrew.\nThe BAT is a 7-key hand-sized device from Infogrip, and has been sold since 1985. It provides one key for each finger and three for the thumb. It is proposed for the hand which does not hold the mouse, in an exact continuation of Engelbart's vision.\nModern.\nModern examples of chorded keyboards include the FrogPad, the In10did method, the EkaPad, TextFaster and HotTyper. Some of them are intended for tiny tablet computers and wireless mobile terminals, many of them are additionally available as apps on Apple's iOS devices. See also the on-screen virtual keyset at Teague Labs.\nCyKey.\nChris Rainey, the co-inventor of Microwriter, re-introduced Microwriting for PC and Palm PDAs with a standalone miniature chording keyboard called CyKey which caters to both left and right-handed users, being 9-keys. CyKey (pronounced sai-ki) is named after the Microwriter chord system's co-inventor Cy Endfield, who died in 1995 but the name also reflects its intuitive nature.\nSiWriter.\nThe SiWriter is an app for the iPad and iPhone which uses a close variant of the microwriter chording system developed by Cy Enfield. It is available via the Apple app store. More information can be found at The system is let down by the lack of haptic feedback - you can't tell if your fingers are in the right place without looking. The finger pad positions are adjustable to fit your hand size. It also works for left handed users and has a live speech output facility that could be helpful for people with speech impairments.\nFrogPad.\nThe FrogPad is a 20-key chorded keyboard about the size of a numeric keypad that can be used with one hand, and is optimized by character frequency. 85% of average keystrokes in English text can be typed without chording, and chords are limited to 2 fingers.\nDecatxt.\nThe Decatxt keyboard uses the IN10DID 10 key chording method[29] (pronounced \"intended\"), and is currently on Amazon. It is a wireless one-handed chord keyboard that places two keys under each finger in order to utilize one hand for typing. Typically only two fingers are needed for any operations. Each key is essentially a shift key so that with ten keys, there are ten single keystrokes and dozens of two and three key combinations. The alphabet is produced with a single press for ten letters or by shifting with either thumb for sixteen more. Changing modes, such as number lock, can make other input such as numbers, provided with a single keystroke. This avoids complex chords while providing enough keystrokes for efficient typing and allows for some unique implementations such as typing with gloves or on a steering wheel. A video game controller called the X-SKIN, using this system, was expected to be commercially available by 2010 to help make Morphs popular on console systems and ease entry of common data such as a username and password, but the USB device was never made commercially available. The IN10DID chording system can be applied in single hand configurations, two handed or with one key at a time if desired. Claimed advantages of the IN10DID method are the diversity of devices, limited motion and simple chords.\nTwiddler.\nThe Twiddler is a fully featured 16-key keyboard (plus mouse) designed to fit in the palm of one hand. It was originally introduced in the early 1990s by Handykey and is currently being produced by Tek Gear (Tek Gear acquired Handykey on April 30, 2008). It is popular among wearable computer researchers and hobbyists due to its ease of use, large community of users, and active support by the manufacturer. Every single and multi-key chord on the Twiddler can be customized by the end user. The Twiddler comes standard with an \"A, B, C, D\" chord set, with TabSpace and other chord sets available. Chords are not limited to single keystrokes - multiple keystrokes can be sent with a single chord press. An example of this is an email address or address block can be typed by pressing just one chord. The efficiency gained by using multi-character chords have novice Twiddler users typing at 47 WPM while experts can burst up to 130 WPM.\nASETNIOP.\nASETNIOP is a virtual keyboard based on chords that appeared in 2012. The alphabet uses the 8 keys of the home row as ASET and NIOP (the most commonly typed key for each finger when touch-typing on a QWERTY keyboard in English), plus 18 chorded combinations. The layout also makes a less-cluttered 10-button keypad for tablet computers, touchscreens, touchpads, and can be used in wired gloves."}
{"id": "6337", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=6337", "title": "Carolyn Beug", "text": "Carolyn Ann Mayer-Beug (December 11, 1952 \u2013 September 11, 2001) was a filmmaker and video producer from Santa Monica, California. She died in the September 11 attacks.\nCareer.\nIn addition to her work as video producer, Beug also directed three music videos for country singer Dwight Yoakam: \"Ain't That Lonely Yet\", \"A Thousand Miles from Nowhere\" and \"Fast as You.\" Beug co-directed the former two videos with Yoakam and was the sole director of the latter video. She won an MTV Video Music award for the Van Halen music video of the song \"Right Now\", which she produced. She also served as senior vice president of Walt Disney Records.\nPersonal life.\nBeug lived in a Tudor-style home in the North 25th Street neighborhood. She hosted an annual backyard barbecue for the Santa Monica High School cross country and track team, which her daughters captained. Beug was a Latter-day Saint.\nDeath and legacy.\nBeug was killed at the age of 48 in the crash of American Airlines Flight 11 in the September 11, 2001 attacks. At the time of her death, Carolyn Beug was working on a children's book about Noah's Ark which was to be told from Noah's wife's point of view. On the plane with her was her mother, Mary Alice Wahlstrom. Beug was survived by her twin eighteen-year-old daughters Lauren and Lindsey Mayer-Beug, her 13-year-old son, Nick, and her husband, John Beug, a senior vice president in charge of filmed production for Warner Brothers' record division. She was returning home from taking her daughters to college at the Rhode Island School of Design.\nAt the National 9/11 Memorial, Beug is memorialized at the North Pool, on Panel N-1."}
{"id": "6339", "revid": "612714", "url": "https://en.wikipedia.org/wiki?curid=6339", "title": "Cell biology", "text": "Cell biology (also cellular biology or cytology) is a branch of biology that studies the structure, function, and behavior of cells. Cell biology encompasses both prokaryotic and eukaryotic cells and can be divided into many sub-topics which may include the study of cell metabolism, cell communication, cell cycle, biochemistry, and cell composition. The study of cells is performed using several techniques such as cell culture, various types of microscopy, and cell fractionation. These have allowed for and are currently being used for discoveries and research pertaining to how cells function, ultimately giving insight into understanding larger organisms. Knowing the components of cells and how cells work is fundamental to all biological sciences while also being essential for research in biomedical fields such as cancer, and other diseases. Research in cell biology is interconnected to other fields such as genetics, molecular genetics, biochemistry, molecular biology, medical microbiology, immunology, and cytochemistry.\nHistory.\nCells were first seen in 17th century Europe with the invention of the compound microscope. In 1665, Robert Hooke termed the building block of all living organisms as \"cells\" after looking at a piece of cork and observing a cell-like structure, however, the cells were dead and gave no indication to the actual overall components of a cell. A few years later, in 1674, Anton Van Leeuwenhoek was the first to analyze live cells in his examination of algae. All of this preceded the cell theory which states that all living things are made up of cells and that cells are the functional and structural unit of organisms. This was ultimately concluded by plant scientist, Matthias Schleiden and animal scientist, Theodor Schwann in 1838, who viewed live cells in plant and animal tissue, respectively. 19 years later, Rudolf Virchow further contributed to the cell theory, adding that all cells come from the division of pre-existing cells. Although widely accepted, there have been many studies that question the validity of the cell theory. Viruses, for example, lack common characteristics of a living cell, such as membranes, cell organelles, and the ability to reproduce by themselves. Scientists have struggled to decide whether viruses are alive or not and whether they are in agreement with the cell theory.\nTechniques.\nModern-day cell biology research looks at different ways to culture and manipulate cells outside of a living body to further research in human anatomy and physiology, and to derive medications. The techniques by which cells are studied have evolved. Due to advancements in microscopy, techniques and technology have allowed for scientists to hold a better understanding of the structure and function of cells. Many techniques commonly used to study cell biology are listed below:\nCell classification and composition.\nThere are two fundamental classifications of cells: prokaryotic and eukaryotic. Prokaryotic cells are distinguished from eukaryotic cells by the absence of a cell nucleus or other membrane bound organelle. Prokaryotic cells are much smaller than eukaryotic cells, making them the smallest form of life. The study of eukaryotic cells is typically the main focus of cytologists, whereas prokaryotic cells are the focus of microbiologists.\nProkaryotic cells.\nProkaryotic cells include Bacteria and Archaea, and lack an enclosed cell nucleus. They both reproduce through binary fission. Bacteria, the most prominent type, have several different shapes which include mainly spherical, and rod-shaped. Bacteria can be classed as either gram positive or gram negative depending on the cell wall composition. Bacterial structural features include:\nThere are many process that occur in prokaryotic cells that allow them to survive. For instance, in a process termed conjugation, fertility factor allows the bacteria to possess a pilus which allows it to transmit DNA to another bacteria which lacks the F factor, permitting the transmittance of resistance allowing it to survive in certain environments.\nEukaryotic cells.\nEukaryotic cells can either be unicellular or multicellular and include animal, plant, fungi, and protozoa cells which all contain organelles with various shapes and sizes. These cells are composed of the following organelles:\nEukaryotic cells may also be composed of the following molecular components:\nProcesses.\nCell metabolism.\nCell metabolism is necessary for the production of energy for the cell and therefore its survival and includes many pathways. For cellular respiration, once glucose is available, glycolysis occurs within the cytosol of the cell to produce pyruvate. Pyruvate undergoes decarboxylation using the multi-enzyme complex to form acetyl coA which can readily be used in the TCA cycle to produce NADH and FADH2. These products are involved in the electron transport chain to ultimately form a proton gradient across the inner mitochondrial membrane. This gradient can then drive the production of ATP and H2O during oxidative phosphorylation. Metabolism in plant cells includes photosynthesis which is simply the exact opposite of respiration as it ultimately produces molecules of glucose.\nCell communication and signaling.\nCell communication is important for cell regulation and for cells to process information from the environment and respond accordingly. Communication can occur through direct cell contact or endocrine, paracrine, and autocrine signaling. Direct cell-cell contact is when a receptor on a cell binds a molecule that is attached to the membrane of another cell. Endocrine signaling occurs through molecules secreted into the bloodstream. Paracrine signaling uses molecules diffusing between two cells to communicate. Autocrine is a cell sending a signal to itself by secreting a molecule that binds to a receptor on its surface. Forms of communication can be through:\nCell cycle.\nThe growth process of the cell does not refer to the size of the cell, but the density of the number of cells present in the organism at a given time. Cell growth pertains to the increase in the number of cells present in an organism as it grows and develops; as the organism gets larger so does the number of cells present. Cells are the foundation of all organisms and are the fundamental unit of life. The growth and development of cells are essential for the maintenance of the host and survival of the organism. For this process, the cell goes through the steps of the cell cycle and development which involves cell growth, DNA replication, cell division, regeneration, and cell death. The cell cycle is divided into four distinct phases: G1, S, G2, and M. The G phase \u2013 which is the cell growth phase \u2013 makes up approximately 95% of the cycle. The proliferation of cells is instigated by progenitors. All cells start out in an identical form and can essentially become any type of cells. Cell signaling such as induction can influence nearby cells to differentiate determinate the type of cell it will become. Moreover, this allows cells of the same type to aggregate and form tissues, then organs, and ultimately systems. The G1, G2, and S phase (DNA replication, damage and repair) are considered to be the interphase portion of the cycle, while the M phase (mitosis) is the cell division portion of the cycle. Mitosis is composed of many stages which include, prophase, metaphase, anaphase, telophase, and cytokinesis, respectively. The ultimate result of mitosis is the formation of two identical daughter cells.\nThe cell cycle is regulated by a series of signaling factors and complexes such as cyclins, cyclin-dependent kinase, and p53. When the cell has completed its growth process and if it is found to be damaged or altered, it undergoes cell death, either by apoptosis or necrosis, to eliminate the threat it can cause to the organism's survival.\nPathology.\nThe scientific branch that studies and diagnoses diseases on the cellular level is called cytopathology. Cytopathology is generally used on samples of free cells or tissue fragments, in contrast to the pathology branch of histopathology, which studies whole tissues. Cytopathology is commonly used to investigate diseases involving a wide range of body sites, often to aid in the diagnosis of cancer but also in the diagnosis of some infectious diseases and other inflammatory conditions. For example, a common application of cytopathology is the Pap smear, a screening test used to detect cervical cancer, and precancerous cervical lesions that may lead to cervical cancer."}
{"id": "6340", "revid": "41405785", "url": "https://en.wikipedia.org/wiki?curid=6340", "title": "Canadian English", "text": "Canadian English (CanE, CE, en-CA) is the set of varieties of the English language native to Canada. According to the 2016 census, English was the first language of more than 19.4 million Canadians or 58.1% of the total population; the remainder of the population were native speakers of Canadian French (20.8%) or other languages (21.1%). A larger number, 28 million people, reported using English as their dominant language. Of Canadians outside the province of Quebec, 82% reported speaking English natively, but within Quebec the figure was just 7.5% as most of its residents are native speakers of Quebec French.\nCanadian English contains major elements of both British and American English, as well as some uniquely Canadian characteristics. While Canadian English tends to be closer to American English in most regards, the precise influence of American English, British English and other sources on Canadian English varieties has been the ongoing focus of systematic studies since the 1950s.\nPhonologically, Canadian and American English are classified together as North American English, emphasizing the fact that the vast majority of outsiders, even other native English speakers, cannot distinguish the typical accents of the two countries by sound alone. Canadians and Americans themselves sometimes have trouble differentiating their own two accents, particularly when the Canadian accent in question is Standard Canadian English: the mainstream mainland variety. There is even evidence that Standard Canadian English and some Western American English (Pacific Northwest and California English, for example) are undergoing a very similar vowel shift, since the 1980s. Most Canadian accents outside of Standard Canadian English fall under Atlantic Canadian English.\nHistory.\nThe term \"Canadian English\" is first attested in a speech by the Reverend A. Constable Geikie in an address to the Canadian Institute in 1857 (see DCHP-1 Online, s.v. \"Canadian English\", Avis \"et al.,\" 1967). Geikie, a Scottish-born Canadian, reflected the Anglocentric attitude that would be prevalent in Canada for the next hundred years when he referred to the language as \"a corrupt dialect\", in comparison with what he considered the proper English spoken by immigrants from Britain.\nCanadian English is the product of five waves of immigration and settlement over a period of more than two centuries. The first large wave of permanent English-speaking settlement in Canada, and linguistically the most important, was the influx of Loyalists fleeing the American Revolution, chiefly from the Mid-Atlantic States\u2014as such, Canadian English is believed by some scholars to have derived from northern American English. Canadian English has been developing features of its own since the early 19th century. The second wave from Britain and Ireland was encouraged to settle in Canada after the War of 1812 by the governors of Canada, who were worried about American dominance and influence among its citizens. Further waves of immigration from around the globe peaked in 1910, 1960 and at the present time had a lesser influence, but they did make Canada a multicultural country, ready to accept linguistic change from around the world during the current period of globalization.\nThe languages of Aboriginal peoples in Canada started to influence European languages used in Canada even before widespread settlement took place, and the French of Lower Canada provided vocabulary, with words such as \"toque\" and \"portage\", to the English of Upper Canada.\nWhile the process of the making of Canadian English\u2014its documentation and codification\u2014goes back to the 1930s, the 1960s were the key period. Like other social developments in Canada, the general acceptance of Canadian English has taken its time. According to a recent study, a noticeable shift in public discourse can only be seen in the middle of the first decade of the 2000s, when Canadian English was seen as a \"given\", generally accepted default variety, while before such statements were usually \"balanced\" by doubts.\nHistorical linguistics.\nStudies on earlier forms of English in Canada are rare, yet connections with other work to historical linguistics can be forged. An overview of diachronic work on Canadian English, or diachronically relevant work, is Dollinger (2012, updated to 2017). Until the 2000s, basically all commentators on the history of CanE have argued from the \"language-external\" history, i.e. social and political history. An exception has been in the area of lexis, where Avis \"et al.\"'s 1967 \"Dictionary of Canadianisms on Historical Principles\" offered real-time historical data through its quotations. Recently, historical linguists have started to study earlier Canadian English with historical linguistic data. DCHP-1 is now available in open access. Most notably, Dollinger (2008) pioneered the historical corpus linguistic approach for English in Canada with CONTE (Corpus of Early Ontario English, 1776\u20131849) and offers a developmental scenario for 18th- and 19th-century Ontario. Recently, Reuter (2015), with a 19th-century newspaper corpus from Ontario, has confirmed the scenario laid out in Dollinger (2008).\n Historically, Canadian English included a class-based sociolect known as \"Canadian dainty\". Treated as a marker of upper-class prestige in the 19th and early 20th centuries, Canadian dainty was marked by the use of some features of British English pronunciation, resulting in an accent similar, but not identical, to the Mid-Atlantic accent known in the United States. This accent faded in prominence following World War II, when it became stigmatized as pretentious, and is now almost never heard in modern Canadian life outside of archival recordings used in film, television or radio documentaries.\nOrthography.\nCanadian spelling of the English language combines British and American conventions, the two dominant varieties, and adds some domestic idiosyncrasies. Spelling in Canadian English co-varies with regional and social variables, somewhat more so, perhaps, than in the two dominant varieties of English, yet general trends have emerged since the 1970s.\nCanadian spelling conventions can be partly explained by Canada's trade history. For instance, the British spelling of the word \"cheque\" probably relates to Canada's once-important ties to British financial institutions. Canada's automobile industry, on the other hand, has been dominated by American firms from its inception, explaining why Canadians use the American spelling of \"tire\" (hence, \"Canadian Tire\") and American terminology for automobiles and their parts (for example, \"truck\" instead of \"lorry\", \"gasoline\" instead of \"petrol\", \"trunk\" instead of \"boot\").\nCanada's political history has also had an influence on Canadian spelling. Canada's first prime minister, John A. Macdonald, once advised the Governor General of Canada to issue an order-in-council directing that government papers be written in the British style.\nA contemporary reference for formal Canadian spelling is the spelling used for Hansard transcripts of the Parliament of Canada . Many Canadian editors, though, use the \"Canadian Oxford Dictionary\", often along with the chapter on spelling in \"Editing Canadian English\", and, where necessary (depending on context), one or more other references. \nThroughout part of the 20th century, some Canadian newspapers adopted American spellings, for example, \"color\" as opposed to the British-based \"colour\". Some of the most substantial historical spelling data can be found in Dollinger (2010) and Grue (2013). The use of such spellings was the long-standing practice of the Canadian Press perhaps since that news agency's inception, but visibly the norm prior to World War II. The practice of dropping the letter \"u\" in such words was also considered a labour-saving technique during the early days of printing in which movable type was set manually. Canadian newspapers also received much of their international content from American press agencies, therefore it was much easier for editorial staff to leave the spellings from the wire services as provided.\nIn the 1990s, Canadian newspapers began to adopt the British spelling variants such as \"-our\" endings, notably with \"The Globe and Mail\" changing its spelling policy in October 1990. Other Canadian newspapers adopted similar changes later that decade, such as the Southam newspaper chain's conversion in September 1998. The \"Toronto Star\" adopted this new spelling policy in September 1997 after that publication's ombudsman discounted the issue earlier in 1997. The \"Star\" had always avoided using recognized Canadian spelling, citing the \"Gage Canadian Dictionary\" in their defence. Controversy around this issue was frequent. When the \"Gage Dictionary\" finally adopted standard Canadian spelling, the \"Star\" followed suit. Some publishers, e.g. \"Maclean's\", continue to prefer American spellings.\nDictionaries.\nThe first Canadian dictionaries of Canadian English were edited by Walter Spencer Avis and published by Gage Ltd. The \"Beginner's Dictionary\" (1962), the \"Intermediate Dictionary\" (1964) and, finally, the \"Senior Dictionary\" (1967) were milestones in Canadian English lexicography. In November 1967 A Dictionary of Canadianisms on Historical Principles (DCHP) was published and completed the first edition of Gage's Dictionary of Canadian English Series. The DCHP documents the historical development of Canadian English words that can be classified as \"Canadianisms\". It therefore includes words such as mukluk, Canuck, and bluff, but does not list common core words such as desk, table or car. Many secondary schools in Canada use the graded dictionaries. The dictionaries have regularly been updated since: the \"Senior Dictionary,\" edited by Robert John Gregg, was renamed \"Gage Canadian Dictionary\". Its fifth edition was printed beginning in 1997. Gage was acquired by Thomson Nelson around 2003. The latest editions were published in 2009 by HarperCollins. On 17 March 2017 a second edition of DCHP, the online Dictionary of Canadianisms on Historical Principles 2 (DCHP-2), was published. DCHP-2 incorporates the c. 10\u200a000 lexemes from DCHP-1 and adds c. 1\u200a300 novel meanings or 1\u200a002 lexemes to the documented lexicon of Canadian English.\nIn 1997, the \"ITP Nelson Dictionary of the Canadian English Language\" was another product, but has not been updated since.\nIn 1998, Oxford University Press produced a Canadian English dictionary, after five years of lexicographical research, entitled \"The Oxford Canadian Dictionary\". A second edition, retitled \"The Canadian Oxford Dictionary\", was published in 2004. Just as the older dictionaries it includes uniquely Canadian words and words borrowed from other languages, and surveyed spellings, such as whether \"colour\" or \"color\" was the more popular choice in common use. Paperback and concise versions (2005, 2006), with minor updates, are available.\nPhonology and phonetics.\nIn terms of the major sound systems (phonologies) of English around the world, Canadian English aligns most closely to American English, both being grouped together under a common North American English sound system; the mainstream Canadian accent (\"Standard Canadian\") is often compared to the very similar and largely overlapping \"General American\" accent, an accent widely spoken throughout the United States and perceived there as being relatively lacking in any noticeable regional features.\nThe provinces east of Ontario show the largest dialect diversity. Northern Canada is, according to William Labov, a dialect region in formation, and a homogeneous English dialect has not yet formed. A very homogeneous dialect exists in Western and Central Canada, a situation that is similar to that of the Western United States. Labov identifies an \"Inland Canada\" region that concentrates all of the defining features of the dialect centred on the Prairies, with periphery areas with more variable patterns including the metropolitan areas of Vancouver and Toronto. This dialect forms a dialect continuum with the far Western US English; however, it is sharply differentiated from the Inland Northern US English of the central and eastern Great Lakes region.\nCanadian English raises the diphthong onsets /\u0259, \u028c/ before voiceless segments; diphthongs /ai/ and /au/.\nStandard Canadian English.\nStandard Canadian English is socially defined. It is the variety spoken, in Chamber's (1998: 252) definition, by Anglophone or multilingual residents, who are second generation or later (i.e. born in Canada) and who live in urban settings. Applying this definition, c. 36% of the Canadian population speak Standard Canadian English in the 2006 population, with 38% in the 2011 census.\nRegional variation.\nThe literature has for a long time conflated the notions of Standard Canadian English (StCE) and regional variation. While some regional dialects are close with the StCE, they are not identical with it. To the untrained ear, for instance, a B.C. middle class speaker from a rural setting may sound like a StCE speaker, while, given Chambers' definition, such person, because of the rural provenance, would not be included in the accepted definition (see the previous section). The \"Atlas of North American English,\" while being the best source for US regional variation, is not a good source for Canadian regional variation, as its analysis is based on only 33 Canadian speakers. Boberg's (2005, 2008) studies offer the best data for the delimitation of dialect zones. The results for vocabulary and phonetics overlap to a great extent, which has allowed the proposal of dialect zones. Dollinger and Clarke distinguish between:\nBritish Columbia.\nBritish Columbia English shares dialect features with both Standard Canadian English and the American Pacific Northwest English. In Vancouver, speakers exhibit more vowel retraction of before nasals than people from Toronto, and this retraction may become a regional marker of West Coast English. raising (found words such as bag, vague and bagel), a prominent feature in western American speakers, is also found in Vancouver speakers. Younger speakers in the Greater Vancouver area do not raise , causing \"about\" to sound somewhat like \"a boat\". The \"o\" in such words as \"holy, goal, load, know,\" etc. is pronounced as a back and rounded , but not as rounded as in the Prairies where there are strong Scandinavian, Slavic and German influences.\nOntario.\nCanadian raising is quite strong throughout the province of Ontario, except within the Ottawa Valley. The Canadian Shift is also a common vowel shift found in Ontario. The retraction of was found to be more advanced for women in Ontario than for people from the Prairies or Atlantic Canada and men.\nIn Southwestern Ontario (roughly in the line south from Sarnia to St. Catharines), despite the existence of the many characteristics of West/Central Canadian English, many speakers, especially those under 30, speak a dialect which is influenced by the Inland Northern American English dialect, though there are minor differences such as Canadian raising (e.g. \"ice\" vs \"my\").\nThe subregion of Midwestern Ontario consists of the Counties of Huron, Bruce, Grey, and Perth. The \"Queen's Bush,\" as the area was called, did not experience communication with Southwestern and Central dialects until the early 20th century. Thus, a strong accent similar to Central Ontarian is heard, yet many different phrasings exist. It is typical in the area to drop phonetic sounds to make shorter contractions, such as: \"prolly\" (probably), \"goin\"' (going), and \"Wuts goin' on tonight? D'ya wanna do sumthin'?\" It is particularly strong in the County of Bruce, so much that it is commonly referred to as being the Bruce Cownian (Bruce Countian) accent. Also 'er' sounds are often pronounced 'air', with \"were\" sounding more like \"wear\".\nResidents of the Golden Horseshoe (including the Greater Toronto Area) are known to merge the second with the in \"Toronto\", pronouncing the name variously as , or even or . This, however, is not unique to Toronto; for example, Atlanta is often pronounced \"Atlanna\" by residents. In the Greater Toronto Area, the \"th\" sound is sometimes pronounced . Sometimes is elided altogether, resulting in \"Do you want this one er'iss one?\" The word \"southern\" is often pronounced with . In the area north of the Regional Municipality of York and south of Parry Sound, notably among those who were born in the surrounding communities, the cutting down of syllables and consonants often heard, e.g. \"probably\" is reduced to \"prolly\" or \"probly\" when used as a response. In Greater Toronto, the diphthong tends to be fronted (as a result the word \"about\" is pronounced as or 'a-beh-oot'). The Greater Toronto Area is diverse linguistically, with 43 percent of its people having a mother tongue other than English. As a result Toronto English has distinctly more variability than Inland Canada.\nIn Eastern Ontario, Canadian raising is not as strong as it is in the rest of the province. In Prescott and Russell, parts of Stormont-Dundas-Glengarry and Eastern Ottawa, French accents are often mixed with English ones due to the high Franco-Ontarian population there. In Lanark County, Western Ottawa and Leeds-Grenville and the rest of Stormont-Dundas-Glengarry, the accent spoken is nearly identical to that spoken in Central Ontario and the Quinte area. Phrases such as \"got it\" is often pronounced as . \"Okay\" is often pronounced as , while \"hello\" is often pronounced as .\nA linguistic enclave has also formed in the Ottawa Valley, heavily influenced by original Scottish, Irish, and German settlers, and existing along the Ontario-Quebec boundary, which has its own distinct accent known as the Ottawa Valley twang (or brogue). Phonetically, the Ottawa Valley twang is characterized by the lack of Canadian raising as well as the cot\u2013caught merger, two common elements of mainstream Canadian English. However, this accent is quite rare in the region today.\nQuebec.\nEnglish is a minority language in Quebec (with French the majority), but has many speakers in Montreal, the Eastern Townships and in the Gatineau-Ottawa region. A person whose mother tongue is English and who still speaks English is called an \"Anglophone\", versus a \"Francophone\", or French speaker.\nMany people in Montreal distinguish between words like \"marry\" versus \"merry\" and \"parish\" versus \"perish\", which are homophones to most other speakers of Canadian English. Quebec Anglophones generally pronounce French street names in Montreal as French words. \"Pie IX\" Boulevard is pronounced as in French: not as \"pie nine\" but as (compare French /pi.n\u0153f/). On the other hand, Anglophones pronounce the final \"d\" as in \"Bernard\" and \"Bouchard\"; the word \"Montreal\" is pronounced as an English word and \"Rue Lambert-Closse\" is known as \"Clossy Street\" (vs French /kl\u0254s/). In the city of Montreal, especially in some of the western suburbs like C\u00f4te-St-Luc and Hampstead, there is a strong Jewish influence in the English spoken in those areas. A large wave of Jewish immigration from Eastern Europe and the former Soviet Union before and after World War II is also evident today. Their English has a strong Yiddish influence, and there are some similarities to English spoken in New York. Words used mainly in Quebec and especially in Montreal are: \"stage\" for \"apprenticeship\" or \"internship\", \"copybook\" for a notebook, \"d\u00e9panneur\" or \"dep\" for a convenience store, and \"guichet\" for an ABM/ATM. It is also common for Anglophones, particularly those of Greek or Italian descent, to use translated French words instead of common English equivalents such as \"open\" and \"close\" for \"on\" and \"off\" or \"Open the lights, please\" for \"Turn on the lights, please\".\nMaritimes.\nMany in the Maritime provinces\u00a0\u2013 Nova Scotia, New Brunswick and Prince Edward Island\u00a0\u2013 have an accent that sounds more like Scottish English and, in some places, Irish English than General American. Outside of major communities, dialects can vary markedly from community to community, as well as from province to province, reflecting ethnic origin as well as a past in which there were few roads and many communities, with some villages very isolated. Into the 1980s, residents of villages in northern Nova Scotia could identify themselves by dialects and accents distinctive to their village. The dialects of Prince Edward Island are often considered the most distinct grouping.\nThe phonology of Maritimer English has some unique features:\nNewfoundland.\nThe dialect spoken in the province of Newfoundland and Labrador, an autonomous dominion until 31 March 1949, is often considered the most distinctive Canadian English dialect. Some Newfoundland English differs in vowel pronunciation, morphology, syntax, and preservation of archaic adverbial-intensifiers. The dialect can vary markedly from community to community, as well as from region to region, reflecting ethnic origin as well as a past in which there were few roads and many communities, and fishing villages in particular remained very isolated. A few speakers have a transitional pin\u2013pen merger.\nAboriginal North.\nFirst Nations and Inuit people from Northern Canada speak a version of Canadian English influenced by the phonology of their first languages. European Canadians in these regions are relatively recent arrivals, and have not produced a dialect that is distinct from southern Canadian English.\nGrammar.\nThere are a handful of syntactical practices unique to Canadian English. When writing, Canadians may start a sentence with \"As well\", in the sense of \"in addition\"; this construction is a Canadianism.\nUnlike British English, North American English strongly prefers \"have\" to \"have got\" to denote possession or obligation (as in \"I have a car\" vs. \"I've got a car\"); Canadian English, however, differs from American English in that it tends to eschew plain \"got\" (\"I got a car\"), which is a common third option in very informal US English.\nThe grammatical construction \"\"be done\" something\" means roughly \"\"have/has finished\" something\". For example, \"I am done my homework\" and \"The dog is done dinner\" are genuine sentences in this dialect, respectively meaning \"I have finished my homework\" and \"The dog has finished dinner\". Another example, \"Let's start after you're done all the coffee\", means \"Let's start after you've finished all the coffee\". This is not exactly the same as the standard construction \"\"to be done with\" something\", since \"She is done the computer\" can only mean \"She is done with the computer\" in one sense: \"She has finished (building) the computer\". \nDate and time notation.\nDate and time notation in Canadian English is a mixture of British and American practices. The date can be written in the form of either \"\" or \"1 July 2017\": the latter is common in more formal writing and bilingual contexts. The Government of Canada only recommends writing all-numeric dates in the form of YYYY-MM-DD (e.g. 2017-07-01), following ISO 8601. Nonetheless, the traditional DD/MM/YY and MM/DD/YY systems remain in everyday use, which can be interpreted in multiple ways: 01/07/17 can mean either 1 July 2017 or 7 January 2017. Private members' bills have repeatedly attempted to clarify the situation. In business communication and filing systems the YYMMDD is used to assist in automatic ordering of electronic files. \nThe government also recommends use of the 24-hour clock, which is widely used in contexts such as transportation schedules, parking meters, and data transmission. Many speakers of English use the 12-hour clock in everyday speech, even when reading from a 24-hour display, similar to the use of the 24-hour clock in the United Kingdom.\nVocabulary.\nWhere Canadian English shares vocabulary with other English dialects, it tends to share most with American English, but also has many non-American terms distinctively shared instead with Britain. British and American terms also can coexist in Canadian English to various extents, sometimes with new nuances in meaning; a classic example is \"holiday\" (British) often used interchangeably with \"vacation\" (American), though, in Canadian speech, the latter can more narrowly mean a trip elsewhere and the former can mean general time off work. In addition, the vocabulary of Canadian English also features some words that are seldom (if ever) found elsewhere. A good resource for these and other words is the Dictionary of Canadianisms on Historical Principles, which is currently being revised at the University of British Columbia in Vancouver, British Columbia. The Canadian public appears to take interest in unique \"Canadianisms\": words that are distinctively characteristic of Canadian English\u2014though perhaps not exclusive to Canada; there is some disagreement about the extent to which \"Canadianism\" means a term actually unique to Canada, with such an understanding possibly overstated by the popular media. As a member of the Commonwealth of Nations, Canada shares many items of institutional terminology and professional designations with the countries of the former British Empire\u2014for example, \"constable\", for a police officer of the lowest rank, and \"chartered accountant\".\nEducation.\nThe term \"college\", which refers to post-secondary education in general in the US, refers in Canada to either a post-secondary technical or vocational institution, or to one of the colleges that exist as federated schools within some Canadian universities. Most often, a \"college\" is a community college, not a university. It may also refer to a CEGEP in Quebec. In Canada, \"college student\" might denote someone obtaining a diploma in business management (this would be an associate degree in the United States); while \"university student\" is the term for someone earning a bachelor's degree. For that reason, \"going to college\" in Canada does not have the same meaning as \"going to university\", unless the speaker or context clarifies the specific level of post-secondary education that is meant.\nWithin the public school system the chief administrator of a school is generally \"the principal\", as in the United States, but the term is not used preceding his or her name, i.e. \"Principal Smith\". The assistant to the principal is not titled as \"assistant principal\", but rather as \"vice-principal\", although the former is not unknown. This usage is identical to that in Northern Ireland.\nCanadian universities publish \"calendars\" or \"schedules\", not \"catalogs\" as in the US. Canadian students \"write\" or \"take\" exams (in the US, students generally \"take\" exams while teachers \"write\" them); they rarely \"sit\" them (standard British usage). Those who supervise students during an exam are sometimes called \"invigilators\" as in Britain, or sometimes \"proctors\" as in the US; usage may depend on the region or even the individual institution.\nSuccessive years of school are usually referred to as \"grade one\", \"grade two\", and so on. In Quebec, the speaker (if Francophone) will often say \"primary one\", \"primary two\" (a direct translation from the French), and so on; while Anglophones will say \"grade one\", \"grade two\". (Compare American \"first grade, second grade\" (sporadically found in Canada), and English/Welsh \"Year 1, Year 2\", Scottish/Northern Irish \"Primary 1, Primary 2\" or \"P1, P2\", and Southern Irish \"First Class, Second Class\" and so on.). The year of school before grade 1 is usually called \"Kindergarten\", with the exception of Nova Scotia, where it is called \"grade primary\".\nIn the US, the four years of high school are termed the freshman, sophomore, junior, and senior years (terms also used for college years); in Canada, the specific levels are used instead (i.e., \"grade nine\"). As for higher education, only the term \"freshman\" (often reduced to \"frosh\") has some currency in Canada. The American usages \"sophomore\", \"junior\" and \"senior\" are not used in Canadian university terminology, or in speech. The specific high-school grades and university years are therefore stated and individualized; for example, \"the grade 12s failed to graduate\"; \"John is in his second year at McMaster\". The \"first year\", \"third year\" designation also applies to Canadian law school students, as opposed to the common American usage of \"1L\", \"2L\" and \"3L\".\nCanadian students use the term \"marks\" (more common in England) or \"grades\" (more common in the US) to refer to their results; usage is very mixed.\nUnits of measurement.\nUnlike in the United States, use of metric units within a majority of (but not all) industries is standard in Canada, as a result of the partial national adoption of the metric system during the mid-to-late 1970s that was eventually stalled; this has spawned some colloquial usages such as \"klick\" for kilometre (as also heard in the US military).\nNonetheless, US units are still used in many situations. Imperial volumes are also used, albeit very rarely\u2014although many Canadians and Americans mistakenly conflate the measurement systems despite their slight differences from each other.\nFor example, English Canadians state their weight and height in pounds and feet/inches, respectively. This is also the case for many Quebec Francophones. Distances while playing golf are always marked and discussed in yards, though official scorecards may also show metres. Temperatures for cooking are often given in Fahrenheit, while the weather is given in Celsius. Directions in the Prairie provinces are sometimes given using miles, because the country roads generally follow the mile-based grid of the Dominion Land Survey. Motor vehicle speed limits are measured in kilometres per hour.\nCanadians measure property, both residential and commercial, floor areas are in square feet or square meters, property is in square feet, square meters, acres or hectares. Fuel efficiency is less frequently discussed in miles per US gallon, more often the metric L/100\u00a0km despite gasoline being sold by the litre. The Letter paper size of 8.5\u00a0inches \u00d7 11\u00a0inches is used instead of the international and metric equivalent A4 size of 210\u00a0mm \u00d7 297\u00a0mm. Beer cans are 355 mL (12 US oz), while beer bottles are typically 341 mL (12 Imperial oz), and draft beer is sold by the pint.\nBuilding materials are used in soft conversions of imperial sizes, but often purchased in relation to the imperial sizes. Example 8\" concrete masonry unit can be referred to as a 8\" CMU or 190 CMU. The actual material used in the US and Canada is the same.\nTransportation.\nHowever, \"expressway\" may also refer to a limited-access road that has control of access but has at-grade junctions, railway crossings (for example, the Harbour Expressway in Thunder Bay.) Sometimes the term \"Parkway\" is also used (for example, the Hanlon Parkway in Guelph). In Saskatchewan, the term 'grid road' is used to refer to minor highways or rural roads, usually gravel, referring to the 'grid' upon which they were originally designed. In Quebec, freeways and expressways are called autoroutes.\nIn Alberta, the generic \"Trail\" is often used to describe a freeway, expressway or major urban street (for example, Deerfoot Trail, Macleod Trail or Crowchild Trail in Calgary, Yellowhead Trail, Victoria Trail or Mark Messier/St.Albert Trail in Edmonton). The British term \"motorway\" is not used. The American terms \"turnpike\" and \"tollway\" for a toll road are not common. The term \"throughway\" or \"thruway\" was used for first tolled limited-access highways (for example, the Deas Island Throughway, now Highway 99, from Vancouver, BC, to Blaine, Washington, USA or the Saint John Throughway (Highway 1) in Saint John, NB), but this term is not common anymore. In everyday speech, when a particular roadway is not being specified, the term \"highway\" is generally or exclusively used.\nLaw.\nLawyers in all parts of Canada, except Quebec, which has its own civil law system, are called \"barristers and solicitors\" because any lawyer licensed in any of the common law provinces and territories must pass bar exams for, and is permitted to engage in, both types of legal practice in contrast to other common-law jurisdictions such as England, Wales and Ireland where the two are traditionally separated (i.e., Canada has a fused legal profession). The words \"lawyer\" and \"counsel\" (not \"counsellor\") predominate in everyday contexts; the word \"attorney\" refers to any personal representative. Canadian lawyers generally do not refer to themselves as \"attorneys\", a term that is common in the United States.\nThe equivalent of an American \"district attorney\", meaning the barrister representing the state in criminal proceedings, is called a \"crown attorney\" (in Ontario), \"crown counsel\" (in British Columbia), \"crown prosecutor\" or \"the crown\", on account of Canada's status as a constitutional monarchy in which the Crown is the locus of state power.\nThe words \"advocate\" and \"notary\"\u00a0\u2013 two distinct professions in Quebec civil law\u00a0\u2013 are used to refer to that province's approximate equivalents of barrister and solicitor, respectively. It is not uncommon, however, for English-speaking advocates in Quebec to refer to themselves in English as \"barrister(s) and solicitor(s)\", as most advocates chiefly perform what would traditionally be known as \"solicitor's work\", while only a minority of advocates actually appear in court. In Canada's common law provinces and territories, the word \"notary\" means strictly a notary public.\nWithin the Canadian legal community itself, the word \"solicitor\" is often used to refer to any Canadian lawyer in general (much like the way the word \"attorney\" is used in the United States to refer to any American lawyer in general). Despite the conceptual distinction between \"barrister\" and \"solicitor\", Canadian court documents would contain a phrase such as \"\"John Smith, \"solicitor\" for the Plaintiff\"\" even though \"John Smith\" may well himself be the barrister who argues the case in court. In a letter introducing him/herself to an opposing lawyer, a Canadian lawyer normally writes something like \"\"I am the \"solicitor\" for Mr. Tom Jones.\"\nThe word \"litigator\" is also used by lawyers to refer to a fellow lawyer who specializes in lawsuits even though the more traditional word \"barrister\" is still employed to denote the same specialization.\nJudges of Canada's superior courts, which exist at the provincial and territorial levels, are traditionally addressed as \"My Lord\" or \"My Lady\", however there are some variances across certain jurisdictions, with some superior court judges preferring the titles \"Mister Justice\" or \"Madam Justice\" to \"Lordship\".\nMasters are addressed as \"Mr. Master\" or simply \"Sir.\" In British Columbia, masters are addressed as \"Your Honour.\"\nJudges of provincial or inferior courts are traditionally referred to in person as \"Your Honour\". Judges of the Supreme Court of Canada and of the federal-level courts prefer the use of \"Mister/Madam (Chief) Justice\". Justices of The Peace are addressed as \"Your Worship\". \"Your Honour\" is also the correct form of address for a Lieutenant Governor.\nA serious crime is called an indictable offence, while a less-serious crime is called a summary offence. The older words felony and misdemeanour, which are still used in the United States, are not used in Canada's current \"Criminal Code\" (R.S.C. 1985, c. C-46) or by today's Canadian legal system. As noted throughout the \"Criminal Code\", a person accused of a crime is called \"the accused\" and not \"the defendant\", a term used instead in civil lawsuits.\nIn Canada, \"visible minority\" refers to a non-aboriginal person or group visibly not one of the majority race in a given population. The term comes from the Canadian Employment Equity Act, which defines such people as \"persons, other than Aboriginal people, who are non-Caucasian in race or non-white in colour.\" The term is used as a demographic category by Statistics Canada. The qualifier \"visible\" is used to distinguish such minorities from the \"invisible\" minorities determined by language (English vs. French) and certain distinctions in religion (Catholics vs. Protestants).\nA county in British Columbia means only a regional jurisdiction of the courts and justice system and is not otherwise connected to governance as with counties in other provinces and in the United States. The rough equivalent to \"county\" as used elsewhere is a \"Regional District\".\nPlaces.\nDistinctive Canadianisms are:\nDaily life.\nTerms common in Canada, Britain and Ireland but less frequent or nonexistent in the United States are:\nThe following are more or less distinctively Canadian:\nApparel.\nThe following are common in Canada, but not in the United States or the United Kingdom.\nPrairies (Manitoba, Saskatchewan and Alberta).\nA strong Canadian raising exists in the prairie regions together with certain older usages such as \"chesterfield\" and \"front room\" also associated with the Maritimes. Aboriginal Canadians are a larger and more conspicuous population in prairie cities than elsewhere in the country and certain elements of aboriginal speech in English are sometimes to be heard. Similarly, the linguistic legacy, mostly intonation but also speech patterns and syntax, of the Scandinavian, Slavic and German settlers\u00a0\u2013 who are far more numerous and historically important in the Prairies than in Ontario or the Maritimes\u00a0\u2013 can be heard in the general milieu. Again, the large M\u00e9tis population in Saskatchewan and Manitoba also carries with it certain linguistic traits inherited from French, Aboriginal and Celtic forebears.\nSome terms are derived from immigrant groups or are just local inventions:\nIn farming communities with substantial Ukrainian, German or Mennonite populations, accents, sentence structure and vocabulary influenced by these languages is common. These communities are most common in the Saskatchewan Valley region of Saskatchewan and Red River Valley region of Manitoba.\nDescendants of marriages between Hudson's Bay Company workers of mainly Scottish descent and Cree women spoke Bungi, a creole that blends Cree and English. A few Bungi speakers can still be found in Manitoba. It is marked by no masculine, feminine or third-person pronouns.\nBritish Columbia.\nBritish Columbian English has several words still in current use borrowed from the Chinook Jargon although the use of such vocabulary is observably decreasing. The most famous and widely used of these terms are \"skookum\" and \"saltchuck\". However, among young British Columbians, almost no one uses this vocabulary, and only a small percentage is even familiar with the meaning of such words. In the Yukon, \"cheechako\" is used for newcomers or greenhorns.\nOntario.\nNorthern Ontario English has several distinct qualities stemming from its large Franco-Ontarian population. As a result several French and English words are used interchangeably. A number of phrases and expressions may also be found in Northern Ontario that are not present in the rest of the province, such as the use of \"camp\" for a summer home where Southern Ontario speakers would idiomatically use cottage.\nIn the early 2010s, certain words from London slang and Arabic were popularized among Toronto youth, especially in immigrant communities. These included words such as \"mandems\", \"styll\", \"wallahi\", \"wasteman\", and \"yute\".\nInformal speech.\nOne of the most distinctive Canadian phrases is the spoken interrogation or tag \"eh\". The only usage of \"eh\" exclusive to Canada, according to the \"Canadian Oxford Dictionary\", is for \"ascertaining the comprehension, continued interest, agreement, etc., of the person or persons addressed\" as in, \"It's four kilometres away, eh, so I have to go by bike.\" In that case, \"eh?\" is used to confirm the attention of the listener and to invite a supportive noise such as \"mm\" or \"oh\" or \"okay\". This usage is also common in Queensland, Australia and New Zealand. Other uses of \"eh\"\u00a0\u2013 for instance, in place of \"huh?\" or \"what?\" meaning \"please repeat or say again\"\u00a0\u2013 are also found in parts of the British Isles and Australia. It is common in Northern/Central Ontario, the Maritimes and the Prairie provinces. The word \"eh\" is used quite frequently in the North Central dialect, so a Canadian accent is often perceived in people from North Dakota, Michigan, Minnesota, and Wisconsin.\nA \"rubber\" in the US and Canada is slang for a condom; however, in Canada it is sometimes (rarely except for Newfoundland and South Western Ontario) another term for an eraser (as it is in the United Kingdom and Ireland).\nThe word \"bum\" can refer either to the buttocks (as in Britain), or, derogatorily, to a homeless person (as in the US). However, the \"buttocks\" sense does not have the indecent character it retains in British use, as it and \"butt\" are commonly used as a polite or childish euphemism for ruder words such as \"arse\" (commonly used in Atlantic Canada and among older people in Ontario and to the west) or \"ass\", or \"mitiss\" (used in the Prairie Provinces, especially in northern and central Saskatchewan; probably originally a Cree loanword). Older Canadians may see \"bum\" as more polite than \"butt\", which before the 1980s was often considered rude.\nSimilarly the word \"pissed\" can refer either to being drunk (as in Britain), or being angry (as in the US), though anger is more often said as \"pissed off\", while \"piss drunk\" or \"pissed up\" is said to describe inebriation (though \"piss drunk\" is sometimes also used in the US, especially in the northern states).\nThe term \"Canuck\" simply means \"Canadian\" in its demonymic form, and, as a term used even by Canadians themselves, it is not considered derogatory. (In the 19th century and early 20th century it tended to refer to French-Canadians.) The only Canadian-built version of the popular World War I-era American Curtiss JN-4 \"Jenny\" training biplane aircraft, the JN-4C, 1,260 of which were built, got the \"Canuck\" nickname; so did another aircraft, the Fleet Model 80, built from the mid-1940s until the late 1950s. The nickname Janey Canuck was used by Anglophone women's rights writer Emily Murphy in the 1920s and the \"Johnny Canuck\" comic book character of the 1940s. Throughout the 1970s, Canada's winning World Cup men's downhill ski team was called the \"Crazy Canucks\" for their fearlessness on the slopes. It is also the name of the Vancouver Canucks, the National Hockey League team of Vancouver, British Columbia.\nThe term \"hoser\", popularized by Bob &amp; Doug McKenzie, typically refers to an uncouth, beer-swilling male and is a euphemism for \"loser\" coming from the earlier days of hockey played on an outdoor rink and the losing team would have to hose down the ice after the game so it froze smooth. Bob &amp; Doug also popularized the use of \"Beauty, eh\", another western slang term which may be used to describe something as being of interest or note or deserving approval. \nA \"Newf\" or \"Newfie\" is someone from Newfoundland and Labrador; sometimes considered derogatory. In Newfoundland, the term \"Mainlander\" refers to any Canadian (sometimes American, occasionally Labradorian) not from the island of Newfoundland. \"Mainlander\" is also occasionally used derogatorily.\nIn the Maritimes, a \"Caper\" or \"Cape Bretoner\" is someone from Cape Breton Island, a \"Bluenoser\" is someone with a thick, usually southern Nova Scotia accent or as a general term for a Nova Scotian (including Cape Bretoners), while an \"Islander\" is someone from Prince Edward Island (the same term is used in British Columbia for people from Vancouver Island, or the numerous islands along it). A \"Haligonian\" refers to someone from the city of Halifax.\nCape Bretoners and Newfies (from Newfoundland and Labrador) often have similar slang. \"Barmp\" is often used as the sound a car horn makes, example: \"He cut me off so I barmped the horn at him\". When saying \"B'y\", while sounds like the traditional farewell, it is a syncopated shortening of the word \"boy\", referring to a person, example: \"How's it goin, b'y?\". Another slang that is commonly used is \"doohickey\" which means an object, example: \"Pass me that doohickey over there\". When an individual uses the word \"biffed\", they mean that they threw something. Example: \"I got frustrated so I biffed it across the room\".\nAttitudes.\nIn 2011, just under 21.5\u00a0million Canadians, representing 65% of the population, spoke English most of the time at home, while 58% declared it their mother language. English is the major language everywhere in Canada except Quebec, and most Canadians (85%) can speak English. While English is not the preferred language in Quebec, 36.1% of the Qu\u00e9b\u00e9cois can speak English. Nationally, Francophones are five times more likely to speak English than Anglophones are to speak French \u2013 44% and 9% respectively. Only 3.2% of Canada's English-speaking population resides in Quebec\u2014mostly in Montreal.\nAttitude studies on Canadian English are somewhat rare. A perceptual study on Albertan and Ontarians exists in combination with older literature from the 1970s\u201380s. Sporadic reports can be found in the literature, e.g. on Vancouver English, in which more than 80% believe in a \"Canadian way of speaking\", with those with a university education reporting higher than those without.\nJaan Lilles argues in an essay for \"English Today\" that there is no variety of \"Canadian English\". He acknowledges that no variety of English is more \"real\" or \"natural\" than any other, but that, in the words of American linguist John Algeo, \"All linguistic varieties are fictions.\" According to Lilles, Canadian English is simply not a \"useful fiction\". He goes on to argue that too often national identity is conflated with linguistic identity, and that in the case of \"Canadian English\", supposedly unique features of Canadian speakers, such as certain lexical terms such as \"muskeg\" are artificially exaggerated to distinguish Canadian speech primarily from that found in the United States.\nFurther reading.\nDollinger, Stefan (2015). The Written Questionnaire in Social Dialectology: History, Theory, Practice. Amsterdam/Philadelphia: Benjamins. The book's examples are exclusive taken from Canadian English and represent one of the more extensive collections of variables for Canadian English."}
{"id": "6343", "revid": "1023487251", "url": "https://en.wikipedia.org/wiki?curid=6343", "title": "Czech language", "text": "Czech (; Czech ), historically also Bohemian (; \"lingua Bohemica\" in Latin), is a West Slavic language of the Czech\u2013Slovak group. Spoken by over 13 million people, it serves as the official language of the Czech Republic. Czech is closely related to Slovak, to the point of high mutual intelligibility, as well as to Polish to a lesser degree. Like other Slavic languages, Czech is a fusional language with a rich system of morphology and relatively flexible word order. Its vocabulary has been extensively influenced by Latin and German.\nThe Czech\u2013Slovak group developed within West Slavic in the high medieval period, and the standardization of Czech and Slovak within the Czech\u2013Slovak dialect continuum emerged in the early modern period. In the later 18th to mid-19th century, the modern written standard became codified in the context of the Czech National Revival. The main non-standard variety, known as Common Czech, is based on the vernacular of Prague, but is now spoken as an interdialect throughout most of the Czech Republic. The Moravian dialects spoken in the eastern part of the country are also classified as Czech, although some of their eastern variants are closer to Slovak.\nCzech has a moderately-sized phoneme inventory, comprising ten monophthongs, three diphthongs and 25 consonants (divided into \"hard\", \"neutral\" and \"soft\" categories). Words may contain complicated consonant clusters or lack vowels altogether. Czech has a raised alveolar trill, which is known to occur as a phoneme in only a few other languages, represented by the grapheme \"\u0159\". Czech uses a simple orthography which phonologists have used as a model.\nClassification.\nCzech is a member of the West Slavic sub-branch of the Slavic branch of the Indo-European language family. This branch includes Polish, Kashubian, Upper and Lower Sorbian and Slovak. Slovak is the most closely related language to Czech, followed by Polish and Silesian.\nThe West Slavic languages are spoken in Central Europe. Czech is distinguished from other West Slavic languages by a more-restricted distinction between \"hard\" and \"soft\" consonants (see Phonology below).\nHistory.\nMedieval/Old Czech.\nThe term \"Old Czech\" is applied to the period predating the 16th century, with the earliest records of the high medieval period also classified as \"early Old Czech\", but the term \"Medieval Czech\" is also used. The function of the written language was initially performed by Old Slavonic written in Glagolitic, later by Latin written in Latin script. \nAround the 7th century, the Slavic expansion reached Central Europe, settling on the eastern fringes of the Frankish Empire. The West Slavic polity of Great Moravia formed by the 9th century. The Christianization of Bohemia took place during the 9th and 10th centuries. The diversification of the Czech-Slovak group within West Slavic began around that time, marked among other things by its use of the voiced velar fricative consonant (/\u0263/) and consistent stress on the first syllable.\nThe Bohemian (Czech) language is first recorded in writing in glosses and short notes during the 12th to 13th centuries. Literary works written in Czech appear in the late 13th and early 14th century and administrative documents first appear towards the late 14th century. The first complete Bible translation also dates to this period. Old Czech texts, including poetry and cookbooks, were produced outside the university as well.\nLiterary activity becomes widespread in the early 15th century in the context of the Bohemian Reformation. Jan Hus contributed significantly to the standardization of Czech orthography, advocated for widespread literacy among Czech commoners (particularly in religion) and made early efforts to model written Czech after the spoken language.\nEarly Modern Czech.\nThere was no standardization distinguishing between Czech and Slovak prior to the 15th century. In the 16th century, the division between Czech and Slovak becomes apparent, marking the confessional division between Lutheran Protestants in Slovakia using Czech orthography and Catholics, especially Slovak Jesuits, beginning to use a separate Slovak orthography based on the language of the Trnava region.\nThe publication of the Kralice Bible between 1579 and 1593 (the first complete Czech translation of the Bible from the original languages) became very important for standardization of the Czech language in the following centuries.\nIn 1615, the Bohemian diet tried to declare Czech to be the only official language of the kingdom. After the Bohemian Revolt (of predominantly Protestant aristocracy) which was defeated by the Habsburgs in 1620, the Protestant intellectuals had to leave the country. This emigration together with other consequences of the Thirty Years' War had a negative impact on the further use of the Czech language. In 1627, Czech and German became official languages of the Kingdom of Bohemia and in the 18th century German became dominant in Bohemia and Moravia, especially among the upper classes.\nModern Czech.\nThe modern standard Czech language originates in standardization efforts of the 18th century. By then the language had developed a literary tradition, and since then it has changed little; journals from that period have no substantial differences from modern standard Czech, and contemporary Czechs can understand them with little difficulty. Sometime before the 18th century, the Czech language abandoned a distinction between phonemic /l/ and /\u028e/ which survives in Slovak.\nWith the beginning of the national revival of the mid-18th century, Czech historians began to emphasize their people's accomplishments from the 15th through the 17th centuries, rebelling against the Counter-Reformation (the Habsburg re-catholization efforts which had denigrated Czech and other non-Latin languages). Czech philologists studied sixteenth-century texts, advocating the return of the language to high culture. This period is known as the Czech National Revival (or Renaissance).\nDuring the national revival, in 1809 linguist and historian Josef Dobrovsk\u00fd released a German-language grammar of Old Czech entitled \"Ausf\u00fchrliches Lehrgeb\u00e4ude der b\u00f6hmischen Sprache\" (\"Comprehensive Doctrine of the Bohemian Language\"). Dobrovsk\u00fd had intended his book to be descriptive, and did not think Czech had a realistic chance of returning as a major language. However, Josef Jungmann and other revivalists used Dobrovsk\u00fd's book to advocate for a Czech linguistic revival. Changes during this time included spelling reform (notably, \"\u00ed\" in place of the former \"j\" and \"j\" in place of \"g\"), the use of \"t\" (rather than \"ti\") to end infinitive verbs and the non-capitalization of nouns (which had been a late borrowing from German). These changes differentiated Czech from Slovak. Modern scholars disagree about whether the conservative revivalists were motivated by nationalism or considered contemporary spoken Czech unsuitable for formal, widespread use.\nAdherence to historical patterns was later relaxed and standard Czech adopted a number of features from Common Czech (a widespread, informally used interdialectal variety), such as leaving some proper nouns undeclined. This has resulted in a relatively high level of homogeneity among all varieties of the language.\nGeographic distribution.\nCzech is spoken by about 10 million residents of the Czech Republic. A Eurobarometer survey conducted from January to March 2012 found that the first language of 98 percent of Czech citizens was Czech, the third-highest proportion of a population in the European Union (behind Greece and Hungary).\nAs the official language of the Czech Republic (a member of the European Union since 2004), Czech is one of the EU's official languages and the 2012 Eurobarometer survey found that Czech was the foreign language most often used in Slovakia. Economist Jonathan van Parys collected data on language knowledge in Europe for the 2012 European Day of Languages. The five countries with the greatest use of Czech were the Czech Republic (98.77 percent), Slovakia (24.86 percent), Portugal (1.93 percent), Poland (0.98 percent) and Germany (0.47 percent).\nCzech speakers in Slovakia primarily live in cities. Since it is a recognised minority language in Slovakia, Slovak citizens who speak only Czech may communicate with the government in their language to the extent that Slovak speakers in the Czech Republic may do so.\nUnited States.\nImmigration of Czechs from Europe to the United States occurred primarily from 1848 to 1914. Czech is a Less Commonly Taught Language in U.S. schools, and is taught at Czech heritage centers. Large communities of Czech Americans live in the states of Texas, Nebraska and Wisconsin. In the 2000 United States Census, Czech was reported as the commonest language spoken at home (besides English) in Valley, Butler and Saunders Counties, Nebraska and Republic County, Kansas. With the exception of Spanish (the non-English language most commonly spoken at home nationwide), Czech was the commonest home language in more than a dozen additional counties in Nebraska, Kansas, Texas, North Dakota and Minnesota. As of 2009, 70,500 Americans spoke Czech as their first language (49th place nationwide, after Turkish and before Swedish).\nPhonology.\nStandard Czech contains ten basic vowel phonemes, and three diphthongs. The vowels are , and their long counterparts . The diphthongs are ; the last two are found only in loanwords such as \"car\" and \"euro\".\nIn Czech orthography, the vowels are spelled as follows:\nThe letter indicates that the previous consonant is palatalised (e.g. ). After a labial it represents (e.g. ); but is pronounced /m\u0272\u025b/, cf. ().\nEach word usually has primary stress on its first syllable, except for enclitics (minor, monosyllabic, unstressed syllables). In all words of more than two syllables, every odd-numbered syllable receives secondary stress. Stress is unrelated to vowel length; both long and short vowels can be stressed or unstressed. Vowels are never reduced in tone (e.g. to schwa sounds) when unstressed. When a noun is preceded by a monosyllabic preposition, the stress moves to the preposition, e.g. \"to Prague\".\nVoiced consonants with unvoiced counterparts are unvoiced at the end of a word before a pause, and in consonant clusters voicing assimilation occurs, which matches voicing to the following consonant. The unvoiced counterpart of /\u0266/ is /x/.\nCzech consonants are categorized as \"hard\", \"neutral\", or \"soft\":\nIn Czech orthography, the consonants are spelled as follows:\nHard consonants may not be followed by \"i\" or \"\u00ed\" in writing, or soft ones by \"y\" or \"\u00fd\" (except in loanwords such as \"kilogram\"). Neutral consonants may take either character. Hard consonants are sometimes known as \"strong\", and soft ones as \"weak\". This distinction is also found in the declension patterns of nouns, which vary according to whether the final consonant of the noun is hard or soft.\nThe phoneme represented by the letter \"\u0159\" (capital \"\u0158\") is often considered unique to Czech. It represents the raised alveolar non-sonorant trill (IPA: ), a sound somewhere between Czech's \"r\" and \"\u017e\" (example: ), and is present in \"Dvo\u0159\u00e1k\". In unvoiced environments, /r\u031d/ is realized as its voiceless allophone [r\u031d\u030a].\nThe consonants can be syllabic, acting as syllable nuclei in place of a vowel. \"Str\u010d prst skrz krk\" (\"Stick [your] finger through [your] throat\") is a well-known Czech tongue twister using only syllabic consonants.\nConsonants&lt;br&gt;\nVowels&lt;br&gt;\nGrammar.\nCzech grammar, like that of other Slavic languages, is fusional; its nouns, verbs, and adjectives are inflected by phonological processes to modify their meanings and grammatical functions, and the easily separable affixes characteristic of agglutinative languages are limited. \nCzech inflects for case, gender and number in nouns and tense, aspect, mood, person and subject number and gender in verbs.\nParts of speech include adjectives, adverbs, numbers, interrogative words, prepositions, conjunctions and interjections. Adverbs are primarily formed from adjectives by taking the final \"\u00fd\", \"\u00ed\" or \"\u00e1\" of the base form and replacing it with \"e\", \"\u011b\", or \"o\". Negative statements are formed by adding the affix \"ne-\" to the main verb of a clause, with one exception: \"je\" (he, she or it is) becomes \"nen\u00ed\".\nSentence and clause structure.\nBecause Czech uses grammatical case to convey word function in a sentence (instead of relying on word order, as English does), its word order is flexible. As a pro-drop language, in Czech an intransitive sentence can consist of only a verb; information about its subject is encoded in the verb. Enclitics (primarily auxiliary verbs and pronouns) appear in the second syntactic slot of a sentence, after the first stressed unit. The first slot must contain a subject or object, a main form of a verb, an adverb, or a conjunction (except for the light conjunctions \"a\", \"and\", \"i\", \"and even\" or \"ale\", \"but\").\nCzech syntax has a subject\u2013verb\u2013object sentence structure. In practice, however, word order is flexible and used for topicalization and focus. Although Czech has a periphrastic passive construction (like English), in colloquial style, word-order changes frequently replace the passive voice. For example, to change \"Peter killed Paul\" to \"Paul was killed by Peter\" the order of subject and object is inverted: \"Petr zabil Pavla\" (\"Peter killed Paul\") becomes \"Paul, Peter killed\" (\"Pavla zabil Petr\"). \"Pavla\" is in the accusative case, the grammatical object of the verb.\nA word at the end of a clause is typically emphasized, unless an upward intonation indicates that the sentence is a question:\nIn parts of Bohemia (including Prague), questions such as \"J\u00ed pes bagetu?\" without an interrogative word (such as \"co\", \"what\" or \"kdo\", \"who\") are intoned in a slow rise from low to high, quickly dropping to low on the last word or phrase.\nIn modern Czech syntax, adjectives precede nouns, with few exceptions. Relative clauses are introduced by relativizers such as the adjective \"kter\u00fd\", analogous to the English relative pronouns \"which\", \"that\" and \"who\"/\"whom\". As with other adjectives, it agrees with its associated noun in gender, number and case. Relative clauses follow the noun they modify. The following is a glossed example:\nEnglish: I want to visit the university that John attends.\nDeclension.\nIn Czech, nouns and adjectives are declined into one of seven grammatical cases which indicate their function in a sentence, two numbers (singular and plural) and three genders (masculine, feminine and neuter). The masculine gender is further divided into animate and inanimate classes.\nCase.\nA nominative\u2013accusative language, Czech marks subject nouns of transitive and intransitive verbs in the nominative case, which is the form found in dictionaries, and direct objects of transitive verbs are declined in the accusative case. The vocative case is used to address people. The remaining cases (genitive, dative, locative and instrumental) indicate semantic relationships, such as noun adjuncts (genitive), indirect objects (dative), or agents in passive constructions (instrumental). Additionally prepositions and some verbs require their complements to be declined in a certain case. The locative case is only used after prepositions. An adjective's case agrees with that of the noun it modifies. When Czech children learn their language's declension patterns, the cases are referred to by number: \nSome Czech grammatical texts order the cases differently, grouping the nominative and accusative (and the dative and locative) together because those declension patterns are often identical; this order accommodates learners with experience in other inflected languages, such as Latin or Russian. This order is nominative, accusative, genitive, dative, locative, instrumental and vocative.\nSome prepositions require the nouns they modify to take a particular case. The cases assigned by each preposition are based on the physical (or metaphorical) direction, or location, conveyed by it. For example, \"od\" (from, away from) and \"z\" (out of, off) assign the genitive case. Other prepositions take one of several cases, with their meaning dependent on the case; \"na\" means \"onto\" or \"for\" with the accusative case, but \"on\" with the locative.\nThis is a glossed example of a sentence using several cases:\n\"English:\" I carried the box into the house with my friend.\nGender.\nCzech distinguishes three genders\u2014masculine, feminine, and neuter\u2014and the masculine gender is subdivided into animate and inanimate. With few exceptions, feminine nouns in the nominative case end in \"-a\", \"-e\", or a consonant; neuter nouns in \"-o\", \"-e\", or \"-\u00ed\", and masculine nouns in a consonant. Adjectives agree in gender and animacy with the nouns they modify. The main effect of gender in Czech morphology is the difference in noun and adjective declension, as well as in endings of verbal participles and past-tense verbs, which are also marked for gender, e.g. \"d\u011blal\" (he did, or made); \"d\u011blala\" (she did, or made) and \"d\u011blalo\" (it did, or made). Gender also plays a semantic role; most nouns that describe people and animals, including personal names, have separate masculine and feminine forms which are normally formed by adding a suffix to the stem, for example \"\u010cech\" (Czech man) has the feminine form \"\u010ce\u0161ka\" (Czech woman).\nExamples of declension patterns for noun phrases of various genders follow:\nNumber.\nNouns are also inflected for number, distinguishing between singular and plural. Typical of a Slavic language, Czech cardinal numbers one through four allow the nouns and adjectives they modify to take any case, but numbers over five require subject and direct object noun phrases to be declined in the genitive plural instead of the nominative or accusative, and when used as subjects these phrases take singular verbs. For example:\nNumbers decline for case, and the numbers one and two are also inflected for gender. Numbers one through five are shown below as examples. The number one has declension patterns identical to those of the demonstrative pronoun \"ten\".\nAlthough Czech's grammatical numbers are singular and plural, several residuals of dual forms remain, such as the words \"dva\" (\"two\") and \"oba\" (\"both\"), which decline the same way. Some nouns for paired body parts use a historical dual form to express plural in some cases: \"ruka\" (hand)\u2014\"ruce\" (nominative); \"noha\" (leg)\u2014\"nohama\" (instrumental), \"nohou\" (genitive/locative); \"oko\" (eye)\u2014\"o\u010di\", and \"ucho\" (ear)\u2014\"u\u0161i\". While two of these nouns are neuter in their singular forms, all plural forms are considered feminine; their gender is relevant to their associated adjectives and verbs. These forms are plural semantically, used for any non-singular count, as in \"mezi \u010dty\u0159ma o\u010dima\" (face to face, lit. \"among four eyes\"). The plural number paradigms of these nouns are a mixture of historical dual and plural forms. For example, \"nohy\" (legs; nominative/accusative) is a standard plural form of this type of noun.\nVerb conjugation.\nCzech verbs agree with their subjects in person (first, second or third), number (singular or plural), and in constructions involving participles also in gender. They are conjugated for tense (past, present or future) and mood (indicative, imperative or conditional). For example, the conjugated verb \"mluv\u00edme\" (we speak) is in the present tense and first-person plural; it is distinguished from other conjugations of the infinitive \"mluvit\" by its ending, \"-\u00edme\". The infinitive form of Czech verbs ends in \"-t\" (archaically, \"-ti\"). It is the form found in dictionaries and the form that follows auxiliary verbs (for example, \"m\u016f\u017eu t\u011b sly\u0161et\"\u2014\"I can \"hear\" you\").\nAspect.\nTypical of Slavic languages, Czech marks its verbs for one of two grammatical aspects: perfective and imperfective. Most verbs are part of inflected aspect pairs\u2014for example, \"koupit\" (perfective) and \"kupovat\" (imperfective). Although the verbs' meaning is similar, in perfective verbs the action is completed and in imperfective verbs it is ongoing or repeated. This is distinct from past and present tense. Any verb of either aspect can be conjugated into either the past or present tense, but the future tense is only used with imperfective verbs. Aspect describes the state of the action at the time specified by the tense.\nThe verbs of most aspect pairs differ in one of two ways: by prefix or by suffix. In prefix pairs, the perfective verb has an added prefix\u2014for example, the imperfective \"ps\u00e1t\" (to write, to be writing) compared with the perfective \"napsat\" (to write down). The most common prefixes are \"na-\", \"o-\", \"po-\", \"s-\", \"u-\", \"vy-\", \"z-\" and \"za-\". In suffix pairs, a different infinitive ending is added to the perfective stem; for example, the perfective verbs \"koupit\" (to buy) and \"prodat\" (to sell) have the imperfective forms \"kupovat\" and \"prod\u00e1vat\". Imperfective verbs may undergo further morphology to make other imperfective verbs (iterative and frequentative forms), denoting repeated or regular action. The verb \"j\u00edt\" (to go) has the iterative form \"chodit\" (to go repeatedly) and the frequentative form \"chod\u00edvat\" (to go occasionally).\nMany verbs have only one aspect, and verbs describing continual states of being\u2014\"b\u00fdt\" (to be), \"cht\u00edt\" (to want), \"moct\" (to be able to), \"le\u017eet\" (to lie down, to be lying down)\u2014have no perfective form. Conversely, verbs describing immediate states of change\u2014for example, \"ot\u011bhotn\u011bt\" (to become pregnant) and \"nadchnout se\" (to become enthusiastic)\u2014have no imperfective aspect.\nTense.\nThe present tense in Czech is formed by adding an ending which agrees with the person and number of the subject at the end of the verb stem. As Czech is a null-subject language, the subject pronoun can be omitted unless it is needed for clarity. The past tense is formed using a participle which ends in \"-l\" and a further ending which agrees with the gender and number of the subject. For the first and second persons, the auxiliary verb \"b\u00fdt\" conjugated in the present tense is added.\nIn some contexts, the present tense of perfective verbs (which differs from the English present perfect) implies future action; in others, it connotes habitual action. The perfective present is used to refer to completion of actions in the future and is distinguished from the imperfective future tense, which refers to actions that will be ongoing in the future. The future tense is regularly formed using the future conjugation of \"b\u00fdt\" (as shown in the table on the left) and the infinitive of an imperfective verb, for example, \"budu j\u00edst\"\u2014\"I will eat\" or \"I will be eating\". Where \"budu\" has a noun or adjective complement it means \"I will be\", for example, \"budu \u0161\u0165astn\u00fd\" (I will be happy). Some verbs of movement form their future tense by adding the prefix \"po-\" to the present tense forms instead, e.g. \"jedu\" (\"I go\") &gt; \"pojedu\" (\"I will go\").\nMood.\nCzech verbs have three grammatical moods: indicative, imperative and conditional. The imperative mood is formed by adding specific endings for each of three person\u2013number categories: \"-\u00d8/-i/-ej\" for second-person singular, \"-te/-ete/-ejte\" for second-person plural and \"-me/-eme/-ejme\" for first-person plural. Imperatives are usually expressed using perfective verbs if positive and imperfective verbs if negative. The conditional mood is formed with a particle after the participle ending in -l which is used to form the past tense. This mood indicates hypothetical events and can also be used to express wishes.\nVerb classes.\nMost Czech verbs fall into one of five classes, which determine their conjugation patterns. The future tense of \"b\u00fdt\" would be classified as a Class I verb because of its endings. Examples of the present tense of each class and some common irregular verbs follow in the tables below:\nOrthography.\nCzech has one of the most phonemic orthographies of all European languages. Its thirty-one graphemes represent thirty sounds (in most dialects, \"i\" and \"y\" have the same sound), and it contains only one digraph: \"ch\", which follows \"h\" in the alphabet. As a result, some of its characters have been used by phonologists to denote corresponding sounds in other languages. The characters \"q\", \"w\" and \"x\" appear only in foreign words. The h\u00e1\u010dek (\u02c7) is used with certain letters to form new characters: \"\u0161\", \"\u017e\", and \"\u010d\", as well as \"\u0148\", \"\u011b\", \"\u0159\", \"\u0165\", and \"\u010f\" (the latter five uncommon outside Czech). The last two letters are sometimes written with a comma above (\u02bc, an abbreviated h\u00e1\u010dek) because of their height.\nUnlike most European languages, Czech distinguishes vowel length; long vowels are indicated by an acute accent or, occasionally with \"\u016f\", a ring. Long \"u\" is usually written \"\u00fa\" at the beginning of a word or morpheme (\"\u00faroda\", \"ne\u00farodn\u00fd\") and \"\u016f\" elsewhere, except for loanwords (\"sk\u00fatr\") or onomatopoeia (\"b\u00fa\"). Long vowels and \"\u011b\" are not considered separate letters in the alphabetical order. The character \"\u00f3\" exists only in loanwords and onomatopoeia.\nCzech typographical features not associated with phonetics generally resemble those of most European languages that use the Latin script, including English. Proper nouns, honorifics, and the first letters of quotations are capitalized, and punctuation is typical of other Latin European languages. Writing of ordinal numerals is similar to most European languages. The Czech language uses a decimal comma instead of a decimal point. When writing a long number, spaces between every three digits, including those in decimal places, may be used for better orientation in handwritten texts. The number 1,234,567.89101 may be written as 1234567,89101 or 1 234 567,891 01. Ordinal numbers (1st) use a point as in German (1.). In proper noun phrases (except personal and settlement names), only the first word is capitalized (\"Pra\u017esk\u00fd hrad\", Prague Castle) (included proper nouns are also capitalized).\nVarieties.\nThe modern literary standard and prestige variety, known as \"Standard Czech\" () is based on the standardization during the Czech National Revival in the 1830s, significantly influenced by Josef Jungmann's Czech\u2013German dictionary published during 1834\u20131839. Jungmann used vocabulary of the Bible of Kralice (1579\u20131613) period and of the language used by his contemporaries. He borrowed words not present in Czech from other Slavic languages or created neologisms. Standard Czech is the formal register of the language which is used in official documents, formal literature, newspaper articles, education and occasionally public speeches. It is codified by the Czech Language Institute, who publish occasional reforms to the codification. The most recent reform took place in 1993. The term (lit. \"Colloquial Czech\") is sometimes used to refer to the spoken variety of standard Czech.\nThe most widely spoken vernacular form of the language is called \"Common Czech\" (), an interdialect influenced by spoken Standard Czech and the Central Bohemian dialects of the Prague region. Other Bohemian regional dialects have become marginalized, while Moravian dialects remain more widespread and diverse, with a political movement for Moravian linguistic revival active since the 1990s.\nThese varieties of the language (Standard Czech, spoken/colloquial Standard Czech, Common Czech, and regional dialects) form a stylistic continuum, in which contact between varieties of a similar prestige influences change within them.\nCommon Czech.\nThe main Czech vernacular, spoken primarily in and around Prague but also throughout the country, is known as Common Czech (\"obecn\u00e1 \u010de\u0161tina\"). This is an academic distinction; most Czechs are unaware of the term or associate it with deformed or \"incorrect\" Czech. Compared to Standard Czech, Common Czech is characterized by simpler inflection patterns and differences in sound distribution.\nCommon Czech is distinguished from spoken/colloquial Standard Czech (), which is a stylistic variety within standard Czech. Tomasz Kamusella defines the spoken variety of Standard Czech as a compromise between Common Czech and the written standard, while Miroslav Kom\u00e1rek calls Common Czech an intersection of spoken Standard Czech and regional dialects.\nCommon Czech has become ubiquitous in most parts of the Czech Republic since the later 20th century. It is usually defined as an interdialect used in common speech in Bohemia and western parts of Moravia (by about two thirds of all inhabitants of the Czech Republic). Common Czech is not codified, but some of its elements have become adopted in the written standard. Since the second half of the 20th century, Common Czech elements have also been spreading to regions previously unaffected, as a consequence of media influence. Standard Czech is still the norm for politicians, businesspeople and other Czechs in formal situations, but Common Czech is gaining ground in journalism and the mass media. The colloquial form of Standard Czech finds limited use in daily communication due to the expansion of the Common Czech interdialect. It is sometimes defined as a theoretical construct rather than an actual tool of colloquial communication, since in casual contexts, the non-standard interdialect is preferred.\nCommon Czech phonology is based on that of the Central Bohemian dialect group, which has a slightly different set of vowel phonemes to Standard Czech. The phoneme /\u025b\u02d0/ is peripheral and usually merges with /i\u02d0/, e.g. in \"mal\u00fd m\u011bsto\" (small town), \"plam\u00ednek\" (little flame) and \"l\u00edtat\" (to fly), and a second native diphthong /\u025b\u026a\u032f/ occurs, usually in places where Standard Czech has /i\u02d0/, e.g. \"malej d\u016fm\" (small house), \"mlejn\" (mill), \"plejtvat\" (to waste), \"bejt\" (to be). In addition, a prothetic \"v-\" is added to most words beginning \"o-\", such as \"votev\u0159\u00edt vokno\" (to open the window).\nNon-standard morphological features that are more or less common among all Common Czech speakers include:\nExamples of declension (Standard Czech is added in italics for comparison):\n\"mlad\u00fd \u010dlov\u011bk \u2013 young man/person, mlad\u00ed lid\u00e9 \u2013 young people, mlad\u00fd st\u00e1t \u2013 young state, mlad\u00e1 \u017eena \u2013 young woman, mlad\u00e9 zv\u00ed\u0159e \u2013 young animal\"\nBohemian dialects.\nApart from the Common Czech vernacular, there remain a variety of other Bohemian dialects, mostly in marginal rural areas. Dialect use began to weaken in the second half of the 20th century, and by the early 1990s regional dialect use was stigmatized, associated with the shrinking lower class and used in literature or other media for comedic effect. Increased travel and media availability to dialect-speaking populations has encouraged them to shift to (or add to their own dialect) Standard Czech.\nThe Czech Statistical Office in 2003 recognized the following Bohemian dialects:\nMoravian dialects.\nThe Czech dialects spoken in Moravia and Silesia are known as Moravian (\"morav\u0161tina\"). In the Austro-Hungarian Empire, \"Bohemian-Moravian-Slovak\" was a language citizens could register as speaking (with German, Polish and several others). Of the Czech dialects, only Moravian is distinguished in nationwide surveys by the Czech Statistical Office. As of 2011, 62,908 Czech citizens spoke Moravian as their first language and 45,561 were diglossic (speaking Moravian and standard Czech as first languages).\nBeginning in the sixteenth century, some varieties of Czech resembled Slovak; the southeastern Moravian dialects, in particular, are sometimes considered dialects of Slovak rather than Czech. These dialects form a continuum between the Czech and Slovak languages, using the same declension patterns for nouns and pronouns and the same verb conjugations as Slovak.\nThe Czech Statistical Office in 2003 recognized the following Moravian dialects:\nSample.\nIn a 1964 textbook on Czech dialectology, B\u0159etislav Koudela used the following sentence to highlight phonetic differences between dialects:\nMutual intelligibility with Slovak.\nCzech and Slovak have been considered mutually intelligible; speakers of either language can communicate with greater ease than those of any other pair of West Slavic languages. Since the 1993 dissolution of Czechoslovakia, mutual intelligibility has declined for younger speakers, probably because Czech speakers now experience less exposure to Slovak and vice versa.\nIn phonetic differences, Czech is characterized by a glottal stop before initial vowels and Slovak by its less-frequent use of long vowels than Czech; however, Slovak has long forms of the consonants \"r\" and \"l\" when they function as vowels. Slovak phonotactics employs a \"rhythmic law\", which forbids two syllables with long vowels from following one another in a word, unlike in Czech. Grammatically, although Czech (unlike Slovak) has a fully productive vocative case, both languages share a common syntax.\nOne study showed that Czech and Slovak lexicons differed by 80 percent, but this high percentage was found to stem primarily from differing orthographies and slight inconsistencies in morphological formation; Slovak morphology is more regular (when changing from the nominative to the locative case, \"Praha\" becomes \"Praze\" in Czech and \"Prahe\" in Slovak). The two lexicons are generally considered similar, with most differences found in colloquial vocabulary and some scientific terminology. Slovak has slightly more borrowed words than Czech.\nThe similarities between Czech and Slovak led to the languages being considered a single language by a group of 19th-century scholars who called themselves \"Czechoslavs\" (\"\u010cechoslovan\u00e9\"), believing that the peoples were connected in a way which excluded German Bohemians and (to a lesser extent) Hungarians and other Slavs. During the First Czechoslovak Republic (1918\u20131938), although \"Czechoslovak\" was designated as the republic's official language, both Czech and Slovak written standards were used. Standard written Slovak was partially modeled on literary Czech, and Czech was preferred for some official functions in the Slovak half of the republic. Czech influence on Slovak was protested by Slovak scholars, and when Slovakia broke off from Czechoslovakia in 1938 as the Slovak State (which then aligned with Nazi Germany in World War II), literary Slovak was deliberately distanced from Czech. When the Axis powers lost the war and Czechoslovakia reformed, Slovak developed somewhat on its own (with Czech influence); during the Prague Spring of 1968, Slovak gained independence from (and equality with) Czech, due to the transformation of Czechoslovakia from a unitary state to a federation. Since the dissolution of Czechoslovakia in 1993, \"Czechoslovak\" has referred to improvised pidgins of the languages which have arisen from the decrease in mutual intelligibility.\nVocabulary.\nCzech vocabulary derives primarily from Slavic, Baltic and other Indo-European roots. Although most verbs have Balto-Slavic origins, pronouns, prepositions and some verbs have wider, Indo-European roots. Some loanwords have been restructured by folk etymology to resemble native Czech words (e.g. \"h\u0159bitov\", \"graveyard\" and \"listina\", \"list\").\nMost Czech loanwords originated in one of two time periods. Earlier loanwords, primarily from German, Greek and Latin, arrived before the Czech National Revival. More recent loanwords derive primarily from English and French, and also from Hebrew, Arabic and Persian. Many Russian loanwords, principally animal names and naval terms, also exist in Czech.\nAlthough older German loanwords were colloquial, recent borrowings from other languages are associated with high culture. During the nineteenth century, words with Greek and Latin roots were rejected in favor of those based on older Czech words and common Slavic roots; \"music\" is \"muzyka\" in Polish and \"\u043c\u0443\u0437\u044b\u043a\u0430\" (\"muzyka\") in Russian, but in Czech it is \"hudba\". Some Czech words have been borrowed as loanwords into English and other languages\u2014for example, \"robot\" (from \"robota\", \"labor\") and \"polka\" (from \"polka\", \"Polish woman\" or from \"p\u016flka\" \"half\").\nSample text.\nAccording to Article 1 of the United Nations Universal Declaration of Human Rights:\nCzech: \"V\u0161ichni lid\u00e9 se rod\u00ed svobodn\u00ed a sob\u011b rovn\u00ed co do d\u016fstojnosti a pr\u00e1v. Jsou nad\u00e1ni rozumem a sv\u011bdom\u00edm a maj\u00ed spolu jednat v duchu bratrstv\u00ed.\"\nEnglish: \"All human beings are born free and equal in dignity and rights. They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood.\""}
{"id": "6344", "revid": "1021880414", "url": "https://en.wikipedia.org/wiki?curid=6344", "title": "Capsid", "text": "A capsid is the protein shell of a virus, enclosing its genetic material. It consists of several oligomeric (repeating) structural subunits made of protein called protomers. The observable 3-dimensional morphological subunits, which may or may not correspond to individual proteins, are called capsomeres. The proteins making up the capsid are called capsid proteins or viral coat proteins (VCP). The capsid and inner genome is called the nucleocapsid.\nCapsids are broadly classified according to their structure. The majority of the viruses have capsids with either helical or icosahedral structure. Some viruses, such as bacteriophages, have developed more complicated structures due to constraints of elasticity and electrostatics. The icosahedral shape, which has 20 equilateral triangular faces, approximates a sphere, while the helical shape resembles the shape of a spring, taking the space of a cylinder but not being a cylinder itself. The capsid faces may consist of one or more proteins. For example, the foot-and-mouth disease virus capsid has faces consisting of three proteins named VP1\u20133.\nSome viruses are \"enveloped\", meaning that the capsid is coated with a lipid membrane known as the \"viral envelope\". The envelope is acquired by the capsid from an intracellular membrane in the virus' host; examples include the inner nuclear membrane, the Golgi membrane, and the cell's outer membrane.\nOnce the virus has infected a cell and begins replicating itself, new capsid subunits are synthesized using the protein biosynthesis mechanism of the cell. In some viruses, including those with helical capsids and especially those with RNA genomes, the capsid proteins co-assemble with their genomes. In other viruses, especially more complex viruses with double-stranded DNA genomes, the capsid proteins assemble into empty precursor procapsids that includes a specialized portal structure at one vertex. Through this portal, viral DNA is translocated into the capsid.\nStructural analyses of major capsid protein (MCP) architectures have been used to categorise viruses into lineages. For example, the bacteriophage PRD1, the algal virus Paramecium bursaria Chlorella virus (PBCV-1), mimivirus and the mammalian adenovirus have been placed in the same lineage, whereas tailed, double-stranded DNA bacteriophages (Caudovirales) and herpesvirus belong to a second lineage.\nSpecific shapes.\nIcosahedral.\nThe icosahedral structure is extremely common among viruses. The icosahedron consists of 20 triangular faces delimited by 12 fivefold vertexes and consists of 60 asymmetric units. Thus, an icosahedral virus is made of 60N protein subunits. The number and arrangement of capsomeres in an icosahedral capsid can be classified using the \"quasi-equivalence principle\" proposed by Donald Caspar and Aaron Klug. Like the Goldberg polyhedra, an icosahedral structure can be regarded as being constructed from pentamers and hexamers. The structures can be indexed by two integers \"h\" and \"k\", with formula_1 and formula_2; the structure can be thought of as taking \"h\" steps from the edge of a pentamer, turning 60 degrees counterclockwise, then taking \"k\" steps to get to the next pentamer. The triangulation number \"T\" for the capsid is defined as:\nIn this scheme, icosahedral capsids contain 12 pentamers plus 10(\"T\"\u00a0\u2212\u00a01) hexamers. The \"T\"-number is representative of the size and complexity of the capsids. Geometric examples for many values of \"h\", \"k\", and \"T\" can be found at List of geodesic polyhedra and Goldberg polyhedra.\nMany exceptions to this rule exist: For example, the polyomaviruses and papillomaviruses have pentamers instead of hexamers in hexavalent positions on a quasi-T=7 lattice. Members of the double-stranded RNA virus lineage, including reovirus, rotavirus and bacteriophage \u03c66 have capsids built of 120 copies of capsid protein, corresponding to a \"T=2\" capsid, or arguably a T=1 capsid with a dimer in the asymmetric unit. Similarly, many small viruses have a pseudo-T=3 (or P=3) capsid, which is organized according to a T=3 lattice, but with distinct polypeptides occupying the three quasi-equivalent positions \nT-numbers can be represented in different ways, for example \"T\"\u00a0=\u00a01 can only be represented as an icosahedron or a dodecahedron and, depending on the type of quasi-symmetry, \"T\"\u00a0=\u00a03 can be presented as a truncated dodecahedron, an icosidodecahedron, or a truncated icosahedron and their respective duals a triakis icosahedron, a rhombic triacontahedron, or a pentakis dodecahedron.\nProlate.\nAn elongated icosahedron is a common shape for the heads of bacteriophages. Such a structure is composed of a cylinder with a cap at either end. The cylinder is composed of 10 elongated triangular faces. The Q number (or Tmid), which can be any positive integer, specifies the number of triangles, composed of asymmetric subunits, that make up the 10 triangles of the cylinder. The caps are classified by the T (or Tend) number.\nThe bacterium \"E. coli\" is the host for bacteriophage T4 that has a prolate head structure. The bacteriophage encoded gp31 protein appears to be functionally homologous to \"E. coli\" chaparone protein GroES and able to substitute for it in the assembly of bacteriophage T4 virions during infection. Like GroES, gp31 forms a stable complex with GroEL chaperonin that is absolutely necessary for the folding and assembly \"in vivo\" of the bacteriophage T4 major capsid protein gp23.\nHelical.\nMany rod-shaped and filamentous plant viruses have capsids with helical symmetry. The helical structure can be described as a set of \"n\" 1-D molecular helices related by an \"n\"-fold axial symmetry. The helical transformation are classified into two categories: one-dimensional and two-dimensional helical systems. Creating an entire helical structure relies on a set of translational and rotational matrices which are coded in the protein data bank. Helical symmetry is given by the formula \"P\"\u00a0=\u00a0\"\u03bc\"\u00a0x\u00a0\"\u03c1\", where \"\u03bc\" is the number of structural units per turn of the helix, \"\u03c1\" is the axial rise per unit and \"P\" is the pitch of the helix. The structure is said to be open due to the characteristic that any volume can be enclosed by varying the length of the helix. The most understood helical virus is the tobacco mosaic virus. The virus is a single molecule of (+) strand RNA. Each coat protein on the interior of the helix bind three nucleotides of the RNA genome. Influenza A viruses differ by comprising multiple ribonucleoproteins, the viral NP protein organizes the RNA into a helical structure. The size is also different; the tobacco mosaic virus has a 16.33 protein subunits per helical turn, while the influenza A virus has a 28 amino acid tail loop.\nFunctions.\nThe functions of the capsid are to:\nThe virus must assemble a stable, protective protein shell to protect the genome from lethal chemical and physical agents. These include forms of natural radiation, extremes of pH or temperature and proteolytic and nucleolytic enzymes. For non-enveloped viruses, the capsid itself may be involved in interaction with receptors on the host cell, leading to penetration of the host cell membrane and internalization of the capsid. Delivery of the genome occurs by subsequent uncoating or disassembly of the capsid and release of the genome into the cytoplasm, or by ejection of the genome through a specialized portal structure directly into the host cell nucleus.\nOrigin and evolution.\nIt has been suggested that many viral capsid proteins have evolved on multiple occasions from functionally diverse cellular proteins. The recruitment of cellular proteins appears to have occurred at different stages of evolution so that some cellular proteins were captured and refunctionalized prior to the divergence of cellular organisms into the three contemporary domains of life, whereas others were hijacked relatively recently. As a result, some capsid proteins are widespread in viruses infecting distantly related organisms (e.g., capsid proteins with the jelly-roll fold), whereas others are restricted to a particular group of viruses (e.g., capsid proteins of alphaviruses).\nA computational model (2015) has shown that capsids may have originated before viruses and that they served as a means of horizontal transfer between replicator communities since these communities could not survive if the number of gene parasites increased, with certain genes being responsible for the formation of these structures and those that favored the survival of self-replicating communities. The displacement of these ancestral genes between cellular organisms could favor the appearance of new viruses during evolution."}
{"id": "6345", "revid": "5364", "url": "https://en.wikipedia.org/wiki?curid=6345", "title": "Central Dogma Of Genetics", "text": ""}
{"id": "6346", "revid": "122969", "url": "https://en.wikipedia.org/wiki?curid=6346", "title": "Chloramphenicol", "text": "Chloramphenicol is an antibiotic useful for the treatment of a number of bacterial infections. This includes use as an eye ointment to treat conjunctivitis. By mouth or by injection into a vein, it is used to treat meningitis, plague, cholera, and typhoid fever. Its use by mouth or by injection is only recommended when safer antibiotics cannot be used. Monitoring both blood levels of the medication and blood cell levels every two days is recommended during treatment.\nCommon side effects include bone marrow suppression, nausea, and diarrhea. The bone marrow suppression may result in death. To reduce the risk of side effects treatment duration should be as short as possible. People with liver or kidney problems may need lower doses. In young children a condition known as gray baby syndrome may occur which results in a swollen stomach and low blood pressure. Its use near the end of pregnancy and during breastfeeding is typically not recommended. Chloramphenicol is a broad-spectrum antibiotic that typically stops bacterial growth by stopping the production of proteins.\nChloramphenicol was discovered after being isolated from \"Streptomyces venezuelae\" in 1947. Its chemical structure was identified and it was first synthesized in 1949. It is on the World Health Organization's List of Essential Medicines. It is available as a generic medication.\nMedical uses.\nThe original indication of chloramphenicol was in the treatment of typhoid, but the now almost universal presence of multiple drug-resistant \"Salmonella typhi\" has meant it is seldom used for this indication except when the organism is known to be sensitive.\nIn low-income countries, the WHO no longer recommends only chloramphenicol as first-line to treat meningitis, but recognises it may be used with caution if there are no available alternatives.\nIn the context of preventing endophthalmitis, a complication of cataract surgery, a 2017 systematic review found moderate evidence that using chloramphenicol eye drops in addition to an antibiotic injection (cefuroxime or penicillin) will likely lower the risk of endophthalmitis, compared to eye drops or antibiotic injections alone.\nSpectrum.\nChloramphenicol has a broad spectrum of activity and has been effective in treating ocular infections such as conjunctivitis, blepharitis etc. caused by a number of bacteria including \"Staphylococcus aureus, Streptococcus pneumoniae\", and \"Escherichia coli\". It is not effective against \"Pseudomonas aeruginosa\". The following susceptibility data represent the minimum inhibitory concentration for a few medically significant organisms.\nEach of these concentrations is dependent upon the bacterial strain being targeted. Some strains of \"E. coli\", for example, show spontaneous emergence of chloramphenicol resistance.\nResistance.\nThree mechanisms of resistance to chloramphenicol are known: reduced membrane permeability, mutation of the 50S ribosomal subunit, and elaboration of chloramphenicol acetyltransferase. It is easy to select for reduced membrane permeability to chloramphenicol \"in vitro\" by serial passage of bacteria, and this is the most common mechanism of low-level chloramphenicol resistance. High-level resistance is conferred by the \"cat\"-gene; this gene codes for an enzyme called chloramphenicol acetyltransferase, which inactivates chloramphenicol by covalently linking one or two acetyl groups, derived from acetyl-\"S\"-coenzyme A, to the hydroxyl groups on the chloramphenicol molecule. The acetylation prevents chloramphenicol from binding to the ribosome. Resistance-conferring mutations of the 50S ribosomal subunit are rare.\nChloramphenicol resistance may be carried on a plasmid that also codes for resistance to other drugs. One example is the ACCoT plasmid (A=ampicillin, C=chloramphenicol, Co=co-trimoxazole, T=tetracycline), which mediates multiple drug resistance in typhoid (also called R factors).\nAs of 2014 some \"Enterococcus faecium\" and\" Pseudomonas aeruginosa\" strains are resistant to chloramphenicol. Some \"Veillonella\" spp. and \"Staphylococcus capitis\" strains have also developed resistance to chloramphenicol to varying degrees.\nAdverse effects.\nAplastic anemia.\nThe most serious side effect of chloramphenicol treatment is aplastic anaemia. This effect is rare and sometimes fatal. The risk of AA is high enough that alternatives should be strongly considered. Treatments are available but expensive. No way exists to predict who may or may not get this side effect. The effect usually occurs weeks or months after treatment has been stopped, and a genetic predisposition may be involved. It is not known whether monitoring the blood counts of patients can prevent the development of aplastic anaemia, but patients are recommended to have a baseline blood count with a repeat blood count every few days while on treatment. Chloramphenicol should be discontinued if the complete blood count drops. The highest risk is with oral chloramphenicol (affecting 1 in 24,000\u201340,000) and the lowest risk occurs with eye drops (affecting less than one in 224,716 prescriptions).\nThiamphenicol, a related compound with a similar spectrum of activity, is available in Italy and China for human use, and has never been associated with aplastic anaemia. Thiamphenicol is available in the U.S. and Europe as a veterinary antibiotic, but is not approved for use in humans.\nBone marrow suppression.\nChloramphenicol may cause bone marrow suppression during treatment; this is a direct toxic effect of the drug on human mitochondria. This effect manifests first as a fall in hemoglobin levels, which occurs quite predictably once a cumulative dose of 20\u00a0g has been given. The anaemia is fully reversible once the drug is stopped and does not predict future development of aplastic anaemia. Studies in mice have suggested existing marrow damage may compound any marrow damage resulting from the toxic effects of chloramphenicol.\nLeukemia.\nLeukemia, a cancer of the blood or bone marrow, is characterized by an abnormal increase of immature white blood cells. The risk of childhood leukemia is increased, as demonstrated in a Chinese case\u2013control study, and the risk increases with length of treatment.\nGray baby syndrome.\nIntravenous chloramphenicol use has been associated with the so-called gray baby syndrome.\nThis phenomenon occurs in newborn infants because they do not yet have fully functional liver enzymes (i.e. UDP-glucuronyl transferase), so chloramphenicol remains unmetabolized in the body.\nThis causes several adverse effects, including hypotension and cyanosis. The condition can be prevented by using the drug at the recommended doses, and monitoring blood levels.\nHypersensitivity reactions.\nFever, macular and vesicular rashes, angioedema, urticaria, and anaphylaxis may occur. Herxheimer's reactions have occurred during therapy for typhoid fever.\nNeurotoxic reactions.\nHeadache, mild depression, mental confusion, and delirium have been described in patients receiving chloramphenicol. Optic and peripheral neuritis have been reported, usually following long-term therapy. If this occurs, the drug should be promptly withdrawn.\nPharmacokinetics.\nChloramphenicol is extremely lipid-soluble; it remains relatively unbound to protein and is a small molecule. It has a large apparent volume of distribution and penetrates effectively into all tissues of the body, including the brain. Distribution is not uniform, with highest concentrations found in the liver and kidney, with lowest in the brain and cerebrospinal fluid. The concentration achieved in brain and cerebrospinal fluid is around 30 to 50% of the overall average body concentration, even when the meninges are not inflamed; this increases to as high as 89% when the meninges are inflamed.\nChloramphenicol increases the absorption of iron.\nUse in special populations.\nChloramphenicol is metabolized by the liver to chloramphenicol glucuronate (which is inactive). In liver impairment, the dose of chloramphenicol must therefore be reduced. No standard dose reduction exists for chloramphenicol in liver impairment, and the dose should be adjusted according to measured plasma concentrations.\nThe majority of the chloramphenicol dose is excreted by the kidneys as the inactive metabolite, chloramphenicol glucuronate. Only a tiny fraction of the chloramphenicol is excreted by the kidneys unchanged. Plasma levels should be monitored in patients with renal impairment, but this is not mandatory. Chloramphenicol succinate ester (an intravenous prodrug form) is readily excreted unchanged by the kidneys, more so than chloramphenicol base, and this is the major reason why levels of chloramphenicol in the blood are much lower when given intravenously than orally.\nChloramphenicol passes into breast milk, so should therefore be avoided during breast feeding, if possible.\nDose monitoring.\nPlasma levels of chloramphenicol must be monitored in neonates and patients with abnormal liver function. Plasma levels should be monitored in all children under the age of four, the elderly, and patients with kidney failure.\nBecause efficacy and toxicity of chloramphenicol are associated with a maximum serum concentration, peak levels (one hour after the intravenous dose is given) should be 10\u201320\u00a0\u00b5g/ml with toxicity ; trough levels (taken immediately before a dose) should be 5\u201310\u00a0\u00b5g/ml.\nDrug interactions.\nAdministration of chloramphenicol concomitantly with bone marrow depressant drugs is contraindicated, although concerns over aplastic anaemia associated with ocular chloramphenicol have largely been discounted.\nChloramphenicol is a potent inhibitor of the cytochrome P450 isoforms CYP2C19 and CYP3A4 in the liver. Inhibition of CYP2C19 causes decreased metabolism and therefore increased levels of, for example, antidepressants, antiepileptics, proton-pump inhibitors, and anticoagulants if they are given concomitantly. Inhibition of CYP3A4 causes increased levels of, for example, calcium channel blockers, immunosuppressants, chemotherapeutic drugs, benzodiazepines, azole antifungals, tricyclic antidepressants, macrolide antibiotics, SSRIs, statins, cardiac antiarrhythmics, antivirals, anticoagulants, and PDE5 inhibitors.\nDrug antagonistic.\nChloramphenicol is antagonistic with most cephalosporins and using both together should be avoided in the treatment of infections.\nMechanism of action.\nChloramphenicol is a bacteriostatic by inhibiting protein synthesis. It prevents protein chain elongation by inhibiting the peptidyl transferase activity of the bacterial ribosome. It specifically binds to A2451 and A2452 residues in the 23S rRNA of the 50S ribosomal subunit, preventing peptide bond formation. Chloramphenicol directly interferes with substrate binding in the ribosome, as compared to macrolides, which sterically block the progression of the growing peptide.\nHistory.\nChloramphenicol was first isolated from \"Streptomyces venezuelae\" in 1947 and in 1949 a team of scientists at Parke-Davis including Mildred Rebstock published their identification of the chemical structure and their synthesis, making it the first antibiotic to be made instead of extracted from a microorganism.\nIn 2007, the accumulation of reports associating aplastic anemia and blood dyscrasia with chloramphenicol eye drops lead to the classification of \u201cprobable human carcinogen\u201d according to World Health Organization criteria, based on the known published case reports and the spontaneous reports submitted to the National Registry of Drug-Induced Ocular Side Effects.\nSociety and culture.\nNames.\nChloramphenicol is available as a generic worldwide under many brandnames and also under various generic names in eastern Europe and Russia, including chlornitromycin, levomycetin, and chloromycetin; the racemate is known as synthomycetin.\nFormulations.\nChloramphenicol is available as a capsule or as a liquid. In some countries, it is sold as chloramphenicol palmitate ester (CPE). CPE is inactive, and is hydrolysed to active chloramphenicol in the small intestine. No difference in bioavailability is noted between chloramphenicol and CPE.\nManufacture of oral chloramphenicol in the U.S. stopped in 1991, because the vast majority of chloramphenicol-associated cases of aplastic anaemia are associated with the oral preparation. No oral formulation of chloramphenicol is now available in the U.S.\nIn molecular biology, chloramphenicol is prepared in ethanol.\nIntravenous.\nThe intravenous (IV) preparation of chloramphenicol is the succinate ester. This creates a problem: Chloramphenicol succinate ester is an inactive prodrug and must first be hydrolysed to chloramphenicol; however, the hydrolysis process is often incomplete, and 30% of the dose is lost and removed in the urine. Serum concentrations of IV chloramphenicol are only 70% of those achieved when chloramphenicol is given orally. For this reason, the dose needs to be increased to 75\u00a0mg/kg/day when administered IV to achieve levels equivalent to the oral dose.\nOily.\nOily chloramphenicol (or chloramphenicol oil suspension) is a long-acting preparation of chloramphenicol first introduced by Roussel in 1954; marketed as Tifomycine, it was originally used as a treatment for typhoid. Roussel stopped production of oily chloramphenicol in 1995; the International Dispensary Association has manufactured it since 1998, first in Malta and then in India from December 2004.\nOily chloramphenicol was first used to treat meningitis in 1975 and numerous studies since have demonstrated its efficacy. It is the cheapest treatment available for meningitis (US$5 per treatment course, compared to US$30 for ampicillin and US$15 for five days of ceftriaxone). It has the great advantage of requiring only a single injection, whereas ceftriaxone is traditionally given daily for five days. This recommendation may yet change, now that a single dose of ceftriaxone (cost US$3) has been shown to be equivalent to one dose of oily chloramphenicol.\nEye drops.\nChloramphenicol is still used occasionally in topical preparations (ointments and eye drops) for the treatment of bacterial conjunctivitis. Isolated case reports of aplastic anaemia following use of chloramphenicol eyedrops exist, but the risk is estimated to be of the order of less than one in 224,716 prescriptions. In Mexico, this is the treatment used prophylactically in newborns.\nVeterinary uses.\nAlthough its use in veterinary medicine is highly restricted, chloramphenicol still has some important veterinary uses. It is currently considered the most useful treatment of chlamydial disease in koalas. The pharmacokinetics of chloramphenicol have been investigated in koalas.\nAlthough unpublished, recent research suggests chloramphenicol could also be applied to frogs to prevent their widespread destruction from fungal infections. It has recently been discovered to be a life-saving cure for chytridiomycosis in amphibians. Chytridiomycosis is a fungal disease, blamed for the extinction of one-third of the 120 frog species lost since 1980."}
{"id": "6347", "revid": "8412748", "url": "https://en.wikipedia.org/wiki?curid=6347", "title": "Cut-up technique", "text": "The cut-up technique (or \"d\u00e9coup\u00e9\" in French) is an aleatory literary technique in which a written text is cut up and rearranged to create a new text. The concept can be traced to at least the Dadaists of the 1920s, but was popularized in the late 1950s and early 1960s by writer William S. Burroughs. It has since been used in a wide variety of contexts.\nTechnique.\nThe cut-up and the closely associated fold-in are the two main techniques:\nWilliam Burroughs cited T. S. Eliot's 1922 poem, \"The Waste Land\", and John Dos Passos' \"U.S.A.\" trilogy, which incorporated newspaper clippings, as early examples of the cut ups he popularized.\nGil J. Wolman developed cut-up techniques as part of his lettrist practice in the early 1950s.\nAlso in the 1950s, painter and writer Brion Gysin more fully developed the cut-up method after accidentally re-discovering it. He had placed layers of newspapers as a mat to protect a tabletop from being scratched while he cut papers with a razor blade. Upon cutting through the newspapers, Gysin noticed that the sliced layers offered interesting juxtapositions of text and image. He began deliberately cutting newspaper articles into sections, which he randomly rearranged. The book \"Minutes to Go\" resulted from his initial cut-up experiment: unedited and unchanged cut-ups which emerged as coherent and meaningful prose. South African poet Sinclair Beiles also used this technique and co-authored \"Minutes To Go\".\nGysin introduced Burroughs to the technique at the Beat Hotel. The pair later applied the technique to printed media and audio recordings in an effort to decode the material's implicit content, hypothesizing that such a technique could be used to discover the true meaning of a given text. Burroughs also suggested cut-ups may be effective as a form of divination saying, \"When you cut into the present the future leaks out.\" Burroughs also further developed the \"fold-in\" technique. In 1977, Burroughs and Gysin published \"The Third Mind\", a collection of cut-up writings and essays on the form. Jeff Nuttall's publication \"My Own Mag\" was another important outlet for the then-radical technique.\nIn an interview, Alan Burns noted that for \"Europe After The Rain\" (1965) and subsequent novels he used a version of cut-ups: \"I did not actually use scissors, but I folded pages, read across columns, and so on, discovering for myself many of the techniques Burroughs and Gysin describe\".\nArgentine writer Julio Cort\u00e1zar often used cut ups in his 1963 novel \"Hopscotch\".\nHistory in literature.\nIn 1969, poets Howard W. Bergerson and J. A. Lindon developed a cut-up technique known as vocabularyclept poetry, in which a poem is formed by taking all the words of an existing poem and rearranging them, often preserving the metre and stanza lengths.\nA precedent of the technique occurred during a Dadaist rally in the 1920s in which Tristan Tzara offered to create a poem on the spot by pulling words at random from a hat. Collage, which was popularized roughly contemporaneously with the Surrealist movement, sometimes incorporated texts such as newspapers or brochures. Prior to this event, the technique had been published in an issue of 391 in the poem by Tzara, \"dada manifesto on feeble love and bitter love\" under the sub-title, \"TO MAKE A DADAIST POEM\".\nA drama scripted for five voices by performance poet Hedwig Gorski in 1977 originated the idea of creating poetry only for performance instead of for print publication. The \"neo-verse drama\" titled \"Booby, Mama!\" written for \"guerilla theater\" performances in public places used a combination of newspaper cut-ups that were edited and choreographed for a troupe of non-professional street actors.\nKathy Acker, a literary and intermedia artist, sampled external sources and reconfigured them into the creation of shifting versions of her own constructed identity. In her late 1970s novel \"Blood and Guts in High School\", Acker explored literary cut-up and appropriation as an integral part of her method.\nHistory in film.\nAntony Balch and Burroughs created a collaboration film, \"The Cut-Ups\" that opened in London in 1967. This was part of an abandoned project called \"Guerrilla Conditions\" meant as a documentary on Burroughs and filmed throughout 1961\u20131965. Inspired by Burroughs' and Gysin's technique of cutting up text and rearranging it in random order, Balch had an editor cut his footage for the documentary into little pieces and impose no control over its reassembly. The film opened at Oxford Street's Cinephone cinema and had a disturbing reaction. Many audience members claimed the film made them ill, others demanded their money back, while some just stumbled out of the cinema ranting \"it's disgusting\". Other cut-up films include \"Ghost at n\u00b09 (Paris)\" (1963\u201372), a posthumously released short film compiled from reels found at Balch's office after his death, and \"William Buys a Parrott\" (1982), \"Bill and Tony\" (1972), \"Towers Open Fire\" (1963) and \"The Junky's Christmas\" (1966).\nInfluence in music.\nFrom the early 1970s, David Bowie used cut-ups to create some of his lyrics. Thom Yorke applied a similar method in Radiohead's \"Kid A\" (2000) album, writing single lines, putting them into a hat, and drawing them out at random while the band rehearsed the songs. Perhaps indicative of Thom Yorke's influences, instructions for \"How to make a Dada poem\" appeared on Radiohead's website at this time.\nStephen Mallinder of Cabaret Voltaire reported to \"Inpress\" magazine's Andrez Bergen that \"I do think the manipulation of sound in our early days \u2013 the physical act of cutting up tapes, creating tape loops and all that \u2013 has a strong reference to Burroughs and Gysin.\" Another industrial music pioneer, Al Jourgensen of Ministry, named Burroughs and his cut-up technique as the most important influence on how he approached the use of samples."}
{"id": "6348", "revid": "835", "url": "https://en.wikipedia.org/wiki?curid=6348", "title": "Congressional Medal of Honour", "text": ""}
{"id": "6352", "revid": "148026", "url": "https://en.wikipedia.org/wiki?curid=6352", "title": "Congenital iodine deficiency syndrome", "text": "Congenital iodine deficiency syndrome (also known as Cretinism) is a medical condition present at birth marked by impaired physical and mental development, due to insufficient thyroid hormone (hypothyroidism) often caused by insufficient dietary iodine during pregnancy. It is one cause of underactive thyroid function at birth, called congenital hypothyroidism, and also referred to as \"cretinism\". If untreated, it results in impairment of both physical and mental development. Symptoms may include goiter, poor length growth in infants, reduced adult stature, thickened skin, hair loss, enlarged tongue, a protruding abdomen; delayed bone maturation and puberty in children; and mental deterioration, neurological impairment, impeded ovulation, and infertility in adults.\nIn developed countries, thyroid function testing of newborns has assured that in those affected, treatment with the thyroid hormone thyroxine is begun promptly. This screening and treatment has virtually eliminated the consequences of the disease.\nSigns and symptoms.\nIodine deficiency causes gradual enlargement of the thyroid gland, referred to as a goiter. Poor length growth is apparent as early as the first year of life. Adult stature without treatment ranges from , depending on severity, sex, and other genetic factors. Other signs include thickened skin, hair loss, enlarged tongue, and a protruding abdomen. In children, bone maturation and puberty are severely delayed. In adults, ovulation is impeded and infertility is common.&lt;ref&gt;"}
{"id": "6353", "revid": "30127365", "url": "https://en.wikipedia.org/wiki?curid=6353", "title": "Cretin", "text": "Cretin may refer to:\nEducation.\nSome education institutions are named after bishop Joseph Cr\u00e9tin:"}
{"id": "6354", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=6354", "title": "Council of Trent", "text": "The Council of Trent (), held between 1545 and 1563 in Trent (or Trento, in northern Italy), was the 19th ecumenical council of the Catholic Church. Prompted by the Protestant Reformation, it has been described as the embodiment of the Counter-Reformation.\nThe Council issued condemnations of what it defined to be heresies committed by proponents of Protestantism, and also issued key statements and clarifications of the Church's doctrine and teachings, including scripture, the Biblical canon, sacred tradition, original sin, justification, salvation, the sacraments, the Mass, and the veneration of saints. The Council met for twenty-five sessions between 13 December 1545 and 4 December 1563. Pope Paul III, who convoked the Council, oversaw the first eight sessions (1545\u201347), while the twelfth to sixteenth sessions (1551\u201352) were overseen by Pope Julius III and the seventeenth to twenty-fifth sessions (1562\u201363) by Pope Pius IV.\nThe consequences of the Council were also significant with regard to the Church's liturgy and practices. During its deliberations, the Council made the Vulgate the official example of the Biblical canon and commissioned the creation of a standard version, although this was not achieved until the 1590s. In 1565, a year after the Council finished its work, Pius IV issued the Tridentine Creed (after \"Tridentum\", Trent's Latin name) and his successor Pius V then issued the Roman Catechism and revisions of the Breviary and Missal in, respectively, 1566, 1568 and 1570. These, in turn, led to the codification of the Tridentine Mass, which remained the Church's primary form of the Mass for the next four hundred years.\nMore than three hundred years passed until the next ecumenical council, the First Vatican Council, was convened in 1869.\nBackground information.\nObstacles and events before the Council's problem area.\nOn 15 March 1517, the Fifth Council of the Lateran closed its activities with a number of reform proposals (on the selection of bishops, taxation, censorship and preaching) but not on the major problems that confronted the Church in Germany and other parts of Europe. A few months later, on 31 October 1517, Martin Luther issued his \"95 Theses\" in Wittenberg.\nA general, free council in Germany.\nLuther's position on ecumenical councils shifted over time, but in 1520 he appealed to the German princes to oppose the papal Church, if necessary with a council in Germany, open and free of the Papacy. After the Pope condemned in \"Exsurge Domine\" fifty-two of Luther's theses as heresy, German opinion considered a council the best method to reconcile existing differences. German Catholics, diminished in number, hoped for a council to clarify matters.\nIt took a generation for the council to materialise, partly due to papal fears over potentially renewing a schism over conciliarism; partly because Lutherans demanded the exclusion of the papacy from the Council; partly because of ongoing political rivalries between France and the Holy Roman Empire; and partly due to the Turkish dangers in the Mediterranean. Under Pope Clement VII (1523\u201334), troops of the Catholic Holy Roman Emperor Charles V sacked Papal Rome in 1527, \"raping, killing, burning, stealing, the like had not been seen since the Vandals\". Saint Peter's Basilica and the Sistine Chapel were used for horses. Pope Clement, fearful of the potential for more violence, delayed calling the Council.\nCharles V strongly favoured a council but needed the support of King Francis I of France, who attacked him militarily. Francis I generally opposed a general council due to partial support of the Protestant cause within France. In 1532 he agreed to the Nuremberg Religious Peace granting religious liberty to the Protestants, and in 1533 he further complicated matters when suggesting a general council to include both Catholic and Protestant rulers of Europe that would devise a compromise between the two theological systems. This proposal met the opposition of the Pope for it gave recognition to Protestants and also elevated the secular Princes of Europe above the clergy on church matters. Faced with a Turkish attack, Charles held the support of the Protestant German rulers, all of whom delayed the opening of the Council of Trent.\nOccasion, sessions, and attendance.\nIn reply to the Papal bull \"Exsurge Domine\" of Pope Leo X (1520), Martin Luther burned the document and appealed for a general council. In 1522 German diets joined in the appeal, with Charles V seconding and pressing for a council as a means of reunifying the Church and settling the Reformation controversies. Pope Clement VII (1523\u20131534) was vehemently against the idea of a council, agreeing with Francis I of France, after Pope Pius II, in his bull \"Execrabilis\" (1460) and his reply to the University of Cologne (1463), set aside the theory of the supremacy of general councils laid down by the Council of Constance.\nPope Paul III (1534\u20131549), seeing that the Protestant Reformation was no longer confined to a few preachers, but had won over various princes, particularly in Germany, to its ideas, desired a council. Yet when he proposed the idea to his cardinals, it was almost unanimously opposed. Nonetheless, he sent nuncios throughout Europe to propose the idea. Paul III issued a decree for a general council to be held in Mantua, Italy, to begin on 23 May 1537. Martin Luther wrote the Smalcald Articles in preparation for the general council. The Smalcald Articles were designed to sharply define where the Lutherans could and could not compromise. The council was ordered by the Emperor and Pope Paul III to convene in Mantua on 23 May 1537. It failed to convene after another war broke out between France and Charles V, resulting in a non-attendance of French prelates. Protestants refused to attend as well. Financial difficulties in Mantua led the Pope in the autumn of 1537 to move the council to Vicenza, where participation was poor. The Council was postponed indefinitely on 21 May 1539. Pope Paul III then initiated several internal Church reforms while Emperor Charles V convened with Protestants and Cardinal Gasparo Contarini at the Diet of Regensburg, to reconcile differences. Mediating and conciliatory formulations were developed on certain topics. In particular, a two-part doctrine of justification was formulated that would later be rejected at Trent. Unity failed between Catholic and Protestant representatives \"because of different concepts of \"Church\" and \"justification\"\".\nHowever, the council was delayed until 1545 and, as it happened, convened right before Luther's death. Unable, however, to resist the urging of Charles V, the pope, after proposing Mantua as the place of meeting, convened the council at Trent (at that time ruled by a prince-bishop under the Holy Roman Empire), on 13 December 1545; the Pope's decision to transfer it to Bologna in March 1547 on the pretext of avoiding a plague failed to take effect and the Council was indefinitely prorogued on 17 September 1549. None of the three popes reigning over the duration of the council ever attended, which had been a condition of Charles V. Papal legates were appointed to represent the Papacy.\nReopened at Trent on 1 May 1551 by the convocation of Pope Julius III (1550\u20131555), it was broken up by the sudden victory of Maurice, Elector of Saxony over Emperor Charles V and his march into surrounding state of Tirol on 28 April 1552. There was no hope of reassembling the council while the very anti-Protestant Paul IV was Pope. The council was reconvened by Pope Pius IV (1559\u20131565) for the last time, meeting from 18 January 1562 at Santa Maria Maggiore, and continued until its final adjournment on 4 December 1563. It closed with a series of ritual acclamations honouring the reigning Pope, the Popes who had convoked the Council, the emperor and the kings who had supported it, the papal legates, the cardinals, the ambassadors present, and the bishops, followed by acclamations of acceptance of the faith of the Council and its decrees, and of anathema for all heretics.\nThe history of the council is thus divided into three distinct periods: 1545\u20131549, 1551\u20131552 and 1562\u20131563. During the second period, the Protestants present asked for a renewed discussion on points already defined and for bishops to be released from their oaths of allegiance to the Pope. When the last period began, all intentions of conciliating the Protestants was gone and the Jesuits had become a strong force. This last period was begun especially as an attempt to prevent the formation of a general council including Protestants, as had been demanded by some in France.\nThe number of attending members in the three periods varied considerably. The council was small to begin with, opening with only about 30 bishops. It increased toward the close, but never reached the number of the First Council of Nicaea (which had 318 members) nor of the First Vatican Council (which numbered 744). The decrees were signed in 1563 by 255 members, the highest attendance of the whole council, including four papal legates, two cardinals, three patriarchs, twenty-five archbishops, and 168 bishops, two-thirds of whom were Italians. The Italian and Spanish prelates were vastly preponderant in power and numbers. At the passage of the most important decrees, not more than sixty prelates were present. Although most Protestants did not attend, ambassadors and theologians of Brandenburg, W\u00fcrttemberg, and Strasbourg attended having been granted an improved safe conduct\nThe French monarchy boycotted the entire council until the last minute when a delegation led by Charles de Guise, Cardinal of Lorraine finally arrived in November 1562. The first outbreak of the French Wars of Religion had occurred earlier in the year and the French Church, facing a significant and powerful Protestant minority in France, experienced iconoclasm violence regarding the use of sacred images. Such concerns were not primary in the Italian and Spanish Churches. The last-minute inclusion of a decree on sacred images was a French initiative, and the text, never discussed on the floor of the council or referred to council theologians, was based on a French draft.\nObjectives and overall results.\nThe main objectives of the council were twofold, although there were other issues that were also discussed:\nThe doctrinal decisions of the council are set forth in decrees (\"decreta\"), which are divided into chapters (\"capita\"), which contain the positive statement of the conciliar dogmas, and into short canons (\"canones\"), which condemn the dissenting Protestant views with the concluding \"anathema sit\" (\"let him be anathema\").\nDecrees.\nThe doctrinal acts are as follows: after reaffirming the Niceno-Constantinopolitan Creed (third session), the decree was passed (fourth session) confirming that the deuterocanonical books were on a par with the other books of the canon (against Luther's placement of these books in the Apocrypha of his edition) and coordinating church tradition with the Scriptures as a rule of faith. The Vulgate translation was affirmed to be authoritative for the text of Scripture.\nJustification (sixth session) was declared to be offered upon the basis of human cooperation with divine grace as opposed to the Protestant doctrine of passive reception of grace. Understanding the Protestant \"faith alone\" doctrine to be one of simple human confidence in divine mercy, the Council rejected the \"vain confidence\" of the Protestants, stating that no one can know who has received the grace of God. Furthermore, the Council affirmed\u2014against some Protestants\u2014that the grace of God can be forfeited through mortal sin.\nThe greatest weight in the Council's decrees is given to the sacraments. The seven sacraments were reaffirmed and the Eucharist pronounced to be a true propitiatory sacrifice as well as a sacrament, in which the bread and wine were consecrated into the Eucharist (thirteenth and twenty-second sessions). The term transubstantiation was used by the Council, but the specific Aristotelian explanation given by Scholasticism was not cited as dogmatic. Instead, the decree states that Christ is \"really, truly, substantially present\" in the consecrated forms. The sacrifice of the Mass was to be offered for dead and living alike and in giving to the apostles the command \"do this in remembrance of me,\" Christ conferred upon them a sacerdotal power. The practise of withholding the cup from the laity was confirmed (twenty-first session) as one which the Church Fathers had commanded for good and sufficient reasons; yet in certain cases the Pope was made the supreme arbiter as to whether the rule should be strictly maintained. On the language of the Mass, \"contrary to what is often said\", the council condemned the belief that only vernacular languages should be used, while insisting on the use of Latin.\nOrdination (twenty-third session) was defined to imprint an indelible character on the soul. The priesthood of the New Testament takes the place of the Levitical priesthood. To the performance of its functions, the consent of the people is not necessary.\nIn the decrees on marriage (twenty-fourth session) the excellence of the celibate state was reaffirmed, concubinage condemned and the validity of marriage made dependent upon the wedding taking place before a priest and two witnesses, although the lack of a requirement for parental consent ended a debate that had proceeded from the 12th century. In the case of a divorce, the right of the innocent party to marry again was denied so long as the other party was alive, even if the other party had committed adultery. However the council \"refused \u2026 to assert the necessity or usefulness of clerical celibacy\".\nIn the twenty-fifth and last session, the doctrines of purgatory, the invocation of saints and the veneration of relics were reaffirmed, as was also the efficacy of indulgences as dispensed by the Church according to the power given her, but with some cautionary recommendations, and a ban on the sale of indulgences. Short and rather inexplicit passages concerning religious images, were to have great impact on the development of Catholic Church art. Much more than the Second Council of Nicaea (787) the Council fathers of Trent stressed the pedagogical purpose of Christian images.\nThe council appointed, in 1562 (eighteenth session), a commission to prepare a list of forbidden books (\"Index Librorum Prohibitorum\"), but it later left the matter to the Pope. The preparation of a catechism and the revision of the Breviary and Missal were also left to the pope. The catechism embodied the council's far-reaching results, including reforms and definitions of the sacraments, the Scriptures, church dogma, and duties of the clergy.\nRatification and promulgation.\nOn adjourning, the Council asked the supreme pontiff to ratify all its decrees and definitions. This petition was complied with by Pope Pius IV, on 26 January 1564, in the papal bull, \"Benedictus Deus\", which enjoins strict obedience upon all Catholics and forbids, under pain of ex-communication, all unauthorised interpretation, reserving this to the Pope alone and threatens the disobedient with \"the indignation of Almighty God and of his blessed apostles, Peter and Paul.\" Pope Pius appointed a commission of cardinals to assist him in interpreting and enforcing the decrees.\nThe \"Index librorum prohibitorum\" was announced in 1564 and the following books were issued with the papal imprimatur: the Profession of the Tridentine Faith and the Tridentine Catechism (1566), the Breviary (1568), the Missal (1570) and the Vulgate (1590 and then 1592).\nThe decrees of the council were acknowledged in Italy, Portugal, Poland and by the Catholic princes of Germany at the Diet of Augsburg in 1566. Philip II of Spain accepted them for Spain, the Netherlands and Sicily inasmuch as they did not infringe the royal prerogative. In France, they were officially recognised by the king only in their doctrinal parts. Although the disciplinary or moral reformatory decrees were never published by the throne, they received official recognition at provincial synods and were enforced by the bishops. Holy Roman Emperors Ferdinand I and Maximilian II never recognized the existence of any of the decrees. No attempt was made to introduce it into England. Pius IV sent the decrees to Mary, Queen of Scots, with a letter dated 13 June 1564, requesting her to publish them in Scotland, but she dared not do it in the face of John Knox and the Reformation.\nThese decrees were later supplemented by the First Vatican Council of 1870.\nPublication of documents.\nA comprehensive history is found in Hubert Jedin's \"The History of the Council of Trent (Geschichte des Konzils von Trient)\" with about 2500 pages in four volumes: \"The History of the Council of Trent: The fight for a Council\" (Vol I, 1951); \"The History of the Council of Trent: The first Sessions in Trent (1545\u20131547)\" (Vol II, 1957); \"The History of the Council of Trent: Sessions in Bologna 1547\u20131548 and Trento 1551\u20131552\" (Vol III, 1970, 1998); \"The History of the Council of Trent: Third Period and Conclusion\" (Vol IV, 1976).\nThe canons and decrees of the council have been published very often and in many languages. The first issue was by Paulus Manutius (Rome, 1564). Commonly-used Latin editions are by Judocus Le Plat (Antwerp, 1779) and by Johann Friedrich von Schulte and Aemilius Ludwig Richter (Leipzig, 1853). Other editions are in vol. vii. of the \"Acta et decreta conciliorum recentiorum. Collectio Lacensis\" (7 vols., Freiburg, 1870\u201390), reissued as independent volume (1892); \"Concilium Tridentinum: Diariorum, actorum, epistularum, \u2026 collectio\", ed. Sebastianus Merkle (4 vols., Freiburg, 1901 sqq.); as well as Mansi, \"Concilia\", xxxv. 345 sqq. Note also Carl Mirbt, \"Quellen\", 2d ed, pp.\u00a0202\u2013255. An English edition is by James Waterworth (London, 1848; \"With Essays on the External and Internal History of the Council\").\nThe original acts and debates of the council, as prepared by its general secretary, Bishop Angelo Massarelli, in six large folio volumes, are deposited in the Vatican Library and remained there unpublished for more than 300 years and were brought to light, though only in part, by Augustin Theiner, priest of the oratory (d. 1874), in \"Acta genuina sancti et oecumenici Concilii Tridentini nunc primum integre edita\" (2 vols., Leipzig, 1874).\nMost of the official documents and private reports, however, which bear upon the council, were made known in the 16th century and since. The most complete collection of them is that of J. Le Plat, \"Monumentorum ad historicam Concilii Tridentini collectio\" (7 vols., Leuven, 1781\u201387). New materials(Vienna, 1872); by JJI von D\u00f6llinger \"(Ungedruckte Berichte und Tageb\u00fccher zur Geschichte des Concilii von Trient)\" (2 parts, N\u00f6rdlingen, 1876); and August von Druffel, \"Monumenta Tridentina\" (Munich, 1884\u201397).\nProtestant response.\nOut of 87 books written between 1546 and 1564 attacking the Council of Trent, 41 were written by Pier Paolo Vergerio, a former papal nuncio turned Protestant Reformer. The 1565\u201373 \"Examen decretorum Concilii Tridentini\" (\"Examination of the Council of Trent\") by Martin Chemnitz was the main Lutheran response to the Council of Trent. Making extensive use of scripture and patristic sources, it was presented in response to a polemical writing which Diogo de Payva de Andrada had directed against Chemnitz. The \"Examen\" had four parts: Volume I examined sacred scripture, free will, original sin, justification, and good works. Volume II examined the sacraments, including baptism, confirmation, the sacrament of the eucharist, communion under both kinds, the mass, penance, extreme unction, holy orders, and matrimony. Volume III examined virginity, celibacy, purgatory, and the invocation of saints. Volume IV examined the relics of the saints, images, indulgences, fasting, the distinction of foods, and festivals.\nIn response, Andrada wrote the five-part \"Defensio Tridentin\u00e6 fidei\", which was published posthumously in 1578. However, the \"Defensio\" did not circulate as extensively as the \"Examen\", nor were any full translations ever published. A French translation of the \"Examen\" by Eduard Preuss was published in 1861. German translations were published in 1861, 1884, and 1972. In English, a complete translation by Fred Kramer drawing from the original Latin and the 1861 German was published beginning in 1971."}
{"id": "6355", "revid": "14423536", "url": "https://en.wikipedia.org/wiki?curid=6355", "title": "Chloroplast", "text": "Chloroplasts are organelles that conduct photosynthesis, where the photosynthetic pigment chlorophyll captures the energy from sunlight, converts it, and stores it in the energy-storage molecules ATP and NADPH while freeing oxygen from water in plant and algal cells. They then use the ATP and NADPH to make organic molecules from carbon dioxide in a process known as the Calvin cycle. Chloroplasts carry out a number of other functions, including fatty acid synthesis, much amino acid synthesis, and the immune response in plants. The number of chloroplasts per cell varies from one, in unicellular algae, up to 100 in plants like \"Arabidopsis\" and wheat.\nA chloroplast is a type of organelle known as a plastid, characterized by its two membranes and a high concentration of chlorophyll. Other plastid types, such as the leucoplast and the chromoplast, contain little chlorophyll and do not carry out photosynthesis.\nChloroplasts are highly dynamic\u2014they circulate and are moved around within plant cells, and occasionally pinch in two to reproduce. Their behavior is strongly influenced by environmental factors like light color and intensity. Chloroplasts, like mitochondria, contain their own DNA, which is thought to be inherited from their ancestor\u2014a photosynthetic cyanobacterium that was engulfed by an early eukaryotic cell. Chloroplasts cannot be made by the plant cell and must be inherited by each daughter cell during cell division.\nWith one exception (the amoeboid \"Paulinella chromatophora\"), all chloroplasts can probably be traced back to a single endosymbiotic event, when a cyanobacterium was engulfed by the eukaryote. Despite this, chloroplasts can be found in an extremely wide set of organisms, some not even directly related to each other\u2014a consequence of many secondary and even tertiary endosymbiotic events.\nThe word \"chloroplast\" is derived from the Greek words \"chloros\" (\u03c7\u03bb\u03c9\u03c1\u03cc\u03c2), which means green, and \"plastes\" (\u03c0\u03bb\u03ac\u03c3\u03c4\u03b7\u03c2), which means \"the one who forms\".\nDiscovery.\nThe first definitive description of a chloroplast (\"Chlorophyllk\u00f6rnen\", \"grain of chlorophyll\") was given by Hugo von Mohl in 1837 as discrete bodies within the green plant cell. In 1883, Andreas Franz Wilhelm Schimper would name these bodies as \"chloroplastids\" (\"Chloroplastiden\"). In 1884, Eduard Strasburger adopted the term \"chloroplasts\" (\"Chloroplasten\").\nLineages and evolution.\nChloroplasts are one of many types of organelles in the plant cell. They are considered to have evolved from endosymbiotic cyanobacteria. Mitochondria are thought to have come from a similar endosymbiosis event, where an aerobic prokaryote was engulfed. This origin of chloroplasts was first suggested by the Russian biologist Konstantin Mereschkowski in 1905 after Andreas Franz Wilhelm Schimper observed in 1883 that chloroplasts closely resemble cyanobacteria. Chloroplasts are only found in plants, algae, and the amoeboid \"Paulinella chromatophora\".\nParent group: Cyanobacteria.\nChloroplasts are considered endosymbiotic Cyanobacteria. Cyanobacteria are sometimes called blue-green algae even though they are prokaryotes. They are a diverse phylum of bacteria capable of carrying out photosynthesis, and are gram-negative, meaning that they have two cell membranes. Cyanobacteria also contain a peptidoglycan cell wall, which is thicker than in other gram-negative bacteria, and which is located between their two cell membranes. Like chloroplasts, they have thylakoids within. On the thylakoid membranes are photosynthetic pigments, including chlorophyll \"a\". Phycobilins are also common cyanobacterial pigments, usually organized into hemispherical phycobilisomes attached to the outside of the thylakoid membranes (phycobilins are not shared with all chloroplasts though).\nPrimary endosymbiosis.\nSomewhere between 1 to 2 billion years ago,\na free-living cyanobacterium entered an early eukaryotic cell, either as food or as an internal parasite, but managed to escape the phagocytic vacuole it was contained in. The two innermost lipid-bilayer membranes that surround all chloroplasts correspond to the outer and inner membranes of the ancestral cyanobacterium's gram negative cell wall, and not the phagosomal membrane from the host, which was probably lost.\nThe new cellular resident quickly became an advantage, providing food for the eukaryotic host, which allowed it to live within it. Over time, the cyanobacterium was assimilated, and many of its genes were lost or transferred to the nucleus of the host. From genomes that probably originally contained over 3000 genes only about 130 genes remain in the chloroplasts of contemporary plants. Some of its proteins were then synthesized in the cytoplasm of the host cell, and imported back into the chloroplast (formerly the cyanobacterium). Separately, somewhere about 90\u2013140 million years ago, it happened again and led to the amoeboid \"Paulinella chromatophora\".\nThis event is called \"endosymbiosis\", or \"cell living inside another cell with a mutual benefit for both\". The external cell is commonly referred to as the \"host\" while the internal cell is called the \"endosymbiont\".\nChloroplasts are believed to have arisen after mitochondria, since all eukaryotes contain mitochondria, but not all have chloroplasts. This is called \"serial endosymbiosis\"\u2014an early eukaryote engulfing the mitochondrion ancestor, and some descendants of it then engulfing the chloroplast ancestor, creating a cell with both chloroplasts and mitochondria.\nWhether or not primary chloroplasts came from a single endosymbiotic event, or many independent engulfments across various eukaryotic lineages, has long been debated. It is now generally held that organisms with primary chloroplasts share a single ancestor that took in a cyanobacterium 600\u20132000 million years ago. It has been proposed this the closest living relative of this bacterium is \"Gloeomargarita lithophora.\" The exception is the amoeboid \"Paulinella chromatophora\", which descends from an ancestor that took in a \"Prochlorococcus\" cyanobacterium 90\u2013500 million years ago.\nThese chloroplasts, which can be traced back directly to a cyanobacterial ancestor, are known as \"primary plastids\" (\"plastid\" in this context means almost the same thing as chloroplast). All primary chloroplasts belong to one of four chloroplast lineages\u2014the glaucophyte chloroplast lineage, the amoeboid \"Paulinella chromatophora\" lineage, the rhodophyte (red algal) chloroplast lineage, or the chloroplastidan (green) chloroplast lineage. The rhodophyte and chloroplastidan lineages are the largest, with chloroplastidan (green) being the one that contains the land plants.\nGlaucophyta.\nUsually the endosymbiosis event is considered to have occurred in the Archaeplastida, within which the glaucophyta being the possible earliest diverging lineage. The glaucophyte chloroplast group is the smallest of the three primary chloroplast lineages, being found in only 13 species, and is thought to be the one that branched off the earliest. Glaucophytes have chloroplasts that retain a peptidoglycan wall between their double membranes, like their cyanobacterial parent. For this reason, glaucophyte chloroplasts are also known as 'muroplasts' (besides 'cyanoplasts' or 'cyanelles'). Glaucophyte chloroplasts also contain concentric unstacked thylakoids, which surround a carboxysome \u2013 an icosahedral structure that glaucophyte chloroplasts and cyanobacteria keep their carbon fixation enzyme RuBisCO in. The starch that they synthesize collects outside the chloroplast. Like cyanobacteria, glaucophyte and rhodophyte chloroplast thylakoids are studded with light collecting structures called phycobilisomes. For these reasons, glaucophyte chloroplasts are considered a primitive intermediate between cyanobacteria and the more evolved chloroplasts in red algae and plants.\nRhodophyceae (red algae).\nThe rhodophyte, or red algae chloroplast group is another large and diverse chloroplast lineage. Rhodophyte chloroplasts are also called \"rhodoplasts\", literally \"red chloroplasts\".\nRhodoplasts have a double membrane with an intermembrane space and phycobilin pigments organized into phycobilisomes on the thylakoid membranes, preventing their thylakoids from stacking. Some contain pyrenoids. Rhodoplasts have chlorophyll \"a\" and phycobilins for photosynthetic pigments; the phycobilin phycoerythrin is responsible for giving many red algae their distinctive red color. However, since they also contain the blue-green chlorophyll \"a\" and other pigments, many are reddish to purple from the combination. The red phycoerytherin pigment is an adaptation to help red algae catch more sunlight in deep water\u2014as such, some red algae that live in shallow water have less phycoerythrin in their rhodoplasts, and can appear more greenish. Rhodoplasts synthesize a form of starch called floridean starch, which collects into granules outside the rhodoplast, in the cytoplasm of the red alga.\nChloroplastida (green algae and plants).\nThe chloroplastida chloroplasts, or green chloroplasts, are another large, highly diverse primary chloroplast lineage. Their host organisms are commonly known as the green algae and land plants. They differ from glaucophyte and red algal chloroplasts in that they have lost their phycobilisomes, and contain chlorophyll \"b\" instead. Most green chloroplasts are (obviously) green, though some aren't, like some forms of \"H\u00e6matococcus pluvialis\", due to accessory pigments that override the chlorophylls' green colors. Chloroplastida chloroplasts have lost the peptidoglycan wall between their double membrane, leaving an intermembrane space. Some plants seem to have kept the genes for the synthesis of the peptidoglycan layer, though they've been repurposed for use in chloroplast division instead.\nMost of the chloroplasts depicted in this article are green chloroplasts.\nGreen algae and plants keep their starch \"inside\" their chloroplasts, and in plants and some algae, the chloroplast thylakoids are arranged in grana stacks. Some green algal chloroplasts contain a structure called a pyrenoid, which is functionally similar to the glaucophyte carboxysome in that it is where RuBisCO and CO are concentrated in the chloroplast.\n\"Helicosporidium\" is a genus of nonphotosynthetic parasitic green algae that is thought to contain a vestigial chloroplast. Genes from a chloroplast and nuclear genes indicating the presence of a chloroplast have been found in \"Helicosporidium\" even if nobody's seen the chloroplast itself.\n\"Paulinella chromatophora\".\nWhile most chloroplasts originate from that first set of endosymbiotic events, \"Paulinella chromatophora\" is an exception that acquired a photosynthetic cyanobacterial endosymbiont more recently. It is not clear whether that symbiont is closely related to the ancestral chloroplast of other eukaryotes. Being in the early stages of endosymbiosis, \"Paulinella chromatophora\" can offer some insights into how chloroplasts evolved. \"Paulinella\" cells contain one or two sausage shaped blue-green photosynthesizing structures called chromatophores, descended from the cyanobacterium \"Synechococcus\". Chromatophores cannot survive outside their host. Chromatophore DNA is about a million base pairs long, containing around 850 protein encoding genes\u2014far less than the three million base pair \"Synechococcus\" genome, but much larger than the approximately 150,000 base pair genome of the more assimilated chloroplast. Chromatophores have transferred much less of their DNA to the nucleus of their host. About 0.3\u20130.8% of the nuclear DNA in \"Paulinella\" is from the chromatophore, compared with 11\u201314% from the chloroplast in plants.\nSecondary and tertiary endosymbiosis.\nMany other organisms obtained chloroplasts from the primary chloroplast lineages through secondary endosymbiosis\u2014engulfing a red or green alga that contained a chloroplast. These chloroplasts are known as secondary plastids.\nWhile primary chloroplasts have a double membrane from their cyanobacterial ancestor, secondary chloroplasts have additional membranes outside of the original two, as a result of the secondary endosymbiotic event, when a nonphotosynthetic eukaryote engulfed a chloroplast-containing alga but failed to digest it\u2014much like the cyanobacterium at the beginning of this story. The engulfed alga was broken down, leaving only its chloroplast, and sometimes its cell membrane and nucleus, forming a chloroplast with three or four membranes\u2014the two cyanobacterial membranes, sometimes the eaten alga's cell membrane, and the phagosomal vacuole from the host's cell membrane.\nThe genes in the phagocytosed eukaryote's nucleus are often transferred to the secondary host's nucleus.\nCryptomonads and chlorarachniophytes retain the phagocytosed eukaryote's nucleus, an object called a nucleomorph, located between the second and third membranes of the chloroplast.\nAll secondary chloroplasts come from green and red algae\u2014no secondary chloroplasts from glaucophytes have been observed, probably because glaucophytes are relatively rare in nature, making them less likely to have been taken up by another eukaryote.\nGreen algal derived chloroplasts.\nGreen algae have been taken up by the euglenids, chlorarachniophytes, a lineage of dinoflagellates, and possibly the ancestor of the CASH lineage (cryptomonads, alveolates, stramenopiles and haptophytes) in three or four separate engulfments. Many green algal derived chloroplasts contain pyrenoids, but unlike chloroplasts in their green algal ancestors, storage product collects in granules outside the chloroplast.\nEuglenophytes.\nEuglenophytes are a group of common flagellated protists that contain chloroplasts derived from a green alga. Euglenophyte chloroplasts have three membranes\u2014it is thought that the membrane of the primary endosymbiont was lost, leaving the cyanobacterial membranes, and the secondary host's phagosomal membrane. Euglenophyte chloroplasts have a pyrenoid and thylakoids stacked in groups of three. Photosynthetic product is stored in the form of paramylon, which is contained in membrane-bound granules in the cytoplasm of the euglenophyte.\nChlorarachniophytes.\nChlorarachniophytes are a rare group of organisms that also contain chloroplasts derived from green algae, though their story is more complicated than that of the euglenophytes. The ancestor of chlorarachniophytes is thought to have been a eukaryote with a \"red\" algal derived chloroplast. It is then thought to have lost its first red algal chloroplast, and later engulfed a green alga, giving it its second, green algal derived chloroplast.\nChlorarachniophyte chloroplasts are bounded by four membranes, except near the cell membrane, where the chloroplast membranes fuse into a double membrane. Their thylakoids are arranged in loose stacks of three. Chlorarachniophytes have a form of polysaccharide called chrysolaminarin, which they store in the cytoplasm, often collected around the chloroplast pyrenoid, which bulges into the cytoplasm.\nChlorarachniophyte chloroplasts are notable because the green alga they are derived from has not been completely broken down\u2014its nucleus still persists as a nucleomorph found between the second and third chloroplast membranes\u2014the periplastid space, which corresponds to the green alga's cytoplasm.\nPrasinophyte-derived dinophyte chloroplast.\n\"Lepidodinium viride\" and its close relatives are dinophytes (see below) that lost their original peridinin chloroplast and replaced it with a green algal derived chloroplast (more specifically, a prasinophyte). \"Lepidodinium\" is the only dinophyte that has a chloroplast that's not from the rhodoplast lineage. The chloroplast is surrounded by two membranes and has no nucleomorph\u2014all the nucleomorph genes have been transferred to the dinophyte nucleus. The endosymbiotic event that led to this chloroplast was serial secondary endosymbiosis rather than tertiary endosymbiosis\u2014the endosymbiont was a green alga containing a primary chloroplast (making a secondary chloroplast).\nRed algal derived chloroplasts.\nCryptophytes.\nCryptophytes, or cryptomonads are a group of algae that contain a red-algal derived chloroplast. Cryptophyte chloroplasts contain a nucleomorph that superficially resembles that of the chlorarachniophytes. Cryptophyte chloroplasts have four membranes, the outermost of which is continuous with the rough endoplasmic reticulum. They synthesize ordinary starch, which is stored in granules found in the periplastid space\u2014outside the original double membrane, in the place that corresponds to the red alga's cytoplasm. Inside cryptophyte chloroplasts is a pyrenoid and thylakoids in stacks of two.\nTheir chloroplasts do not have phycobilisomes, but they do have phycobilin pigments which they keep in their thylakoid space, rather than anchored on the outside of their thylakoid membranes.\nCryptophytes may have played a key role in the spreading of red algal based chloroplasts.\nHaptophytes.\nHaptophytes are similar and closely related to cryptophytes or heterokontophytes. Their chloroplasts lack a nucleomorph, their thylakoids are in stacks of three, and they synthesize chrysolaminarin sugar, which they store completely outside of the chloroplast, in the cytoplasm of the haptophyte.\nHeterokontophytes (stramenopiles).\nThe heterokontophytes, also known as the stramenopiles, are a very large and diverse group of eukaryotes. The photoautotrophic lineage, Ochrophyta, including the diatoms and the brown algae, golden algae, and yellow-green algae, also contains red algal derived chloroplasts.\nHeterokont chloroplasts are very similar to haptophyte chloroplasts, containing a pyrenoid, triplet thylakoids, and with some exceptions, having four layer plastidic envelope, the outermost epiplastid membrane connected to the endoplasmic reticulum. Like haptophytes, heterokontophytes store sugar in chrysolaminarin granules in the cytoplasm. Heterokontophyte chloroplasts contain chlorophyll \"a\" and with a few exceptions chlorophyll \"c\", but also have carotenoids which give them their many colors.\nApicomplexans, chromerids, and dinophytes.\nThe alveolates are a major clade of unicellular eukaryotes of both autotrophic and heterotrophic members. The most notable shared characteristic is the presence of cortical (outer-region) alveoli (sacs). These are flattened vesicles (sacs) packed into a continuous layer just under the membrane and supporting it, typically forming a flexible pellicle (thin skin). In dinoflagellates they often form armor plates. Many members contain a red-algal derived plastid. One notable characteristic of this diverse group is the frequent loss of photosynthesis. However, a majority of these heterotrophs continue to process a non-photosynthetic plastid.\nApicomplexans are a group of alveolates. Like the helicosproidia, they're parasitic, and have a nonphotosynthetic chloroplast. They were once thought to be related to the helicosproidia, but it is now known that the helicosproida are green algae rather than part of the CASH lineage. The apicomplexans include \"Plasmodium\", the malaria parasite. Many apicomplexans keep a vestigial red algal derived chloroplast called an apicoplast, which they inherited from their ancestors. Other apicomplexans like \"Cryptosporidium\" have lost the chloroplast completely. Apicomplexans store their energy in amylopectin granules that are located in their cytoplasm, even though they are nonphotosynthetic.\nApicoplasts have lost all photosynthetic function, and contain no photosynthetic pigments or true thylakoids. They are bounded by four membranes, but the membranes are not connected to the endoplasmic reticulum. The fact that apicomplexans still keep their nonphotosynthetic chloroplast around demonstrates how the chloroplast carries out important functions other than photosynthesis. Plant chloroplasts provide plant cells with many important things besides sugar, and apicoplasts are no different\u2014they synthesize fatty acids, isopentenyl pyrophosphate, iron-sulfur clusters, and carry out part of the heme pathway. This makes the apicoplast an attractive target for drugs to cure apicomplexan-related diseases. The most important apicoplast function is isopentenyl pyrophosphate synthesis\u2014in fact, apicomplexans die when something interferes with this apicoplast function, and when apicomplexans are grown in an isopentenyl pyrophosphate-rich medium, they dump the organelle.\nThe Chromerida is a newly discovered group of algae from Australian corals which comprises some close photosynthetic relatives of the apicomplexans. The first member, \"Chromera velia\", was discovered and first isolated in 2001. The discovery of \"Chromera velia\" with similar structure to the apicomplexanss, provides an important link in the evolutionary history of the apicomplexans and dinophytes. Their plastids have four membranes, lack chlorophyll c and use the type II form of RuBisCO obtained from a horizontal transfer event.\nThe dinoflagellates are yet another very large and diverse group of protists, around half of which are (at least partially) photosynthetic.\nMost dinophyte chloroplasts are secondary red algal derived chloroplasts. Many other dinophytes have lost the chloroplast (becoming the nonphotosynthetic kind of dinoflagellate), or replaced it though \"tertiary\" endosymbiosis\u2014the engulfment of another eukaryotic algae containing a red algal derived chloroplast. Others replaced their original chloroplast with a green algal derived one.\nMost dinophyte chloroplasts contain form II RuBisCO, at least the photosynthetic pigments chlorophyll \"a\", chlorophyll \"c2\", \"beta\"-carotene, and at least one dinophyte-unique xanthophyll (peridinin, dinoxanthin, or diadinoxanthin), giving many a golden-brown color. All dinophytes store starch in their cytoplasm, and most have chloroplasts with thylakoids arranged in stacks of three.\nThe most common dinophyte chloroplast is the peridinin-type chloroplast, characterized by the carotenoid pigment peridinin in their chloroplasts, along with chlorophyll \"a\" and chlorophyll \"c\"2. Peridinin is not found in any other group of chloroplasts. The peridinin chloroplast is bounded by three membranes (occasionally two), having lost the red algal endosymbiont's original cell membrane. The outermost membrane is not connected to the endoplasmic reticulum. They contain a pyrenoid, and have triplet-stacked thylakoids. Starch is found outside the chloroplast. An important feature of these chloroplasts is that their chloroplast DNA is highly reduced and fragmented into many small circles. Most of the genome has migrated to the nucleus, and only critical photosynthesis-related genes remain in the chloroplast.\nThe peridinin chloroplast is thought to be the dinophytes' \"original\" chloroplast, which has been lost, reduced, replaced, or has company in several other dinophyte lineages.\nFucoxanthin-containing (haptophyte-derived) dinophyte chloroplasts.\nThe fucoxanthin dinophyte lineages (including \"Karlodinium\" and \"Karenia\") lost their original red algal derived chloroplast, and replaced it with a new chloroplast derived from a haptophyte endosymbiont. \"Karlodinium\" and \"Karenia\" probably took up different heterokontophytes. Because the haptophyte chloroplast has four membranes, tertiary endosymbiosis would be expected to create a six membraned chloroplast, adding the haptophyte's cell membrane and the dinophyte's phagosomal vacuole. However, the haptophyte was heavily reduced, stripped of a few membranes and its nucleus, leaving only its chloroplast (with its original double membrane), and possibly one or two additional membranes around it.\nFucoxanthin-containing chloroplasts are characterized by having the pigment fucoxanthin (actually 19\u2032-hexanoyloxy-fucoxanthin and/or 19\u2032-butanoyloxy-fucoxanthin) and no peridinin. Fucoxanthin is also found in haptophyte chloroplasts, providing evidence of ancestry.\nDiatom-derived dinophyte chloroplasts.\nSome dinophytes, like \"Kryptoperidinium\" and \"Durinskia\" have a diatom (heterokontophyte) derived chloroplast. These chloroplasts are bounded by up to \"five\" membranes, (depending on whether the entire diatom endosymbiont is counted as the chloroplast, or just the red algal derived chloroplast inside it). The diatom endosymbiont has been reduced relatively little\u2014it still retains its original mitochondria, and has endoplasmic reticulum, ribosomes, a nucleus, and of course, red algal derived chloroplasts\u2014practically a complete cell, all inside the host's endoplasmic reticulum lumen. However the diatom endosymbiont can't store its own food\u2014its storage polysaccharide is found in granules in the dinophyte host's cytoplasm instead. The diatom endosymbiont's nucleus is present, but it probably can't be called a nucleomorph because it shows no sign of genome reduction, and might have even been \"expanded\". Diatoms have been engulfed by dinoflagellates at least three times.\nThe diatom endosymbiont is bounded by a single membrane, inside it are chloroplasts with four membranes. Like the diatom endosymbiont's diatom ancestor, the chloroplasts have triplet thylakoids and pyrenoids.\nIn some of these genera, the diatom endosymbiont's chloroplasts aren't the only chloroplasts in the dinophyte. The original three-membraned peridinin chloroplast is still around, converted to an eyespot.\nKleptoplastidy.\nIn some groups of mixotrophic protists, like some dinoflagellates (e.g. \"Dinophysis\"), chloroplasts are separated from a captured alga and used temporarily. These klepto chloroplasts may only have a lifetime of a few days and are then replaced.\nCryptophyte-derived dinophyte chloroplast.\nMembers of the genus \"Dinophysis\" have a phycobilin-containing chloroplast taken from a cryptophyte. However, the cryptophyte is not an endosymbiont\u2014only the chloroplast seems to have been taken, and the chloroplast has been stripped of its nucleomorph and outermost two membranes, leaving just a two-membraned chloroplast. Cryptophyte chloroplasts require their nucleomorph to maintain themselves, and \"Dinophysis\" species grown in cell culture alone cannot survive, so it is possible (but not confirmed) that the \"Dinophysis\" chloroplast is a kleptoplast\u2014if so, \"Dinophysis\" chloroplasts wear out and \"Dinophysis\" species must continually engulf cryptophytes to obtain new chloroplasts to replace the old ones.\nChloroplast DNA.\nChloroplasts have their own DNA, often abbreviated as ctDNA, or cpDNA. It is also known as the plastome. Its existence was first proved in 1962, and first sequenced in 1986\u2014when two Japanese research teams sequenced the chloroplast DNA of liverwort and tobacco. Since then, hundreds of chloroplast DNAs from various species have been sequenced, but they are mostly those of land plants and green algae\u2014glaucophytes, red algae, and other algal groups are extremely underrepresented, potentially introducing some bias in views of \"typical\" chloroplast DNA structure and content.\nMolecular structure.\nWith few exceptions, most chloroplasts have their entire chloroplast genome combined into a single large circular DNA molecule, typically 120,000\u2013170,000 base pairs long. They can have a contour length of around 30\u201360 micrometers, and have a mass of about 80\u2013130 million daltons.\nWhile usually thought of as a circular molecule, there is some evidence that chloroplast DNA molecules more often take on a linear shape.\nInverted repeats.\nMany chloroplast DNAs contain two \"inverted repeats\", which separate a long single copy section (LSC) from a short single copy section (SSC).\nWhile a given pair of inverted repeats are rarely completely identical, they are always very similar to each other, apparently resulting from concerted evolution.\nThe inverted repeats vary wildly in length, ranging from 4,000 to 25,000 base pairs long each and containing as few as four or as many as over 150 genes. Inverted repeats in plants tend to be at the upper end of this range, each being 20,000\u201325,000 base pairs long.\nThe inverted repeat regions are highly conserved among land plants, and accumulate few mutations. Similar inverted repeats exist in the genomes of cyanobacteria and the other two chloroplast lineages (glaucophyta and rhodophyceae), suggesting that they predate the chloroplast, though some chloroplast DNAs have since lost or flipped the inverted repeats (making them direct repeats). It is possible that the inverted repeats help stabilize the rest of the chloroplast genome, as chloroplast DNAs which have lost some of the inverted repeat segments tend to get rearranged more.\nNucleoids.\nNew chloroplasts may contain up to 100 copies of their DNA, though the number of chloroplast DNA copies decreases to about 15\u201320 as the chloroplasts age. They are usually packed into nucleoids, which can contain several identical chloroplast DNA rings. Many nucleoids can be found in each chloroplast.\nIn primitive red algae, the chloroplast DNA nucleoids are clustered in the center of the chloroplast, while in green plants and green algae, the nucleoids are dispersed throughout the stroma.\nThough chloroplast DNA is not associated with true histones, in red algae, similar proteins that tightly pack each chloroplast DNA ring into a nucleoid have been found.\nDNA repair.\nIn chloroplasts of the moss \"Physcomitrella patens\", the DNA mismatch repair protein Msh1 interacts with the recombinational repair proteins RecA and RecG to maintain chloroplast genome stability. In chloroplasts of the plant \"Arabidopsis thaliana\" the RecA protein maintains the integrity of the chloroplast's DNA by a process that likely involves the recombinational repair of DNA damage.\nDNA replication.\nThe leading model of cpDNA replication.\nThe mechanism for chloroplast DNA (cpDNA) replication has not been conclusively determined, but two main models have been proposed. Scientists have attempted to observe chloroplast replication via electron microscopy since the 1970s. The results of the microscopy experiments led to the idea that chloroplast DNA replicates using a double displacement loop (D-loop). As the D-loop moves through the circular DNA, it adopts a theta intermediary form, also known as a Cairns replication intermediate, and completes replication with a rolling circle mechanism. Transcription starts at specific points of origin. Multiple replication forks open up, allowing replication machinery to transcribe the DNA. As replication continues, the forks grow and eventually converge. The new cpDNA structures separate, creating daughter cpDNA chromosomes.\nIn addition to the early microscopy experiments, this model is also supported by the amounts of deamination seen in cpDNA. Deamination occurs when an amino group is lost and is a mutation that often results in base changes. When adenine is deaminated, it becomes hypoxanthine. Hypoxanthine can bind to cytosine, and when the XC base pair is replicated, it becomes a GC (thus, an A \u2192 G base change). \nDeamination.\nIn cpDNA, there are several A \u2192 G deamination gradients. DNA becomes susceptible to deamination events when it is single stranded. When replication forks form, the strand not being copied is single stranded, and thus at risk for A \u2192 G deamination. Therefore, gradients in deamination indicate that replication forks were most likely present and the direction that they initially opened (the highest gradient is most likely nearest the start site because it was single stranded for the longest amount of time). This mechanism is still the leading theory today; however, a second theory suggests that most cpDNA is actually linear and replicates through homologous recombination. It further contends that only a minority of the genetic material is kept in circular chromosomes while the rest is in branched, linear, or other complex structures.\nAlternative model of replication.\nOne of competing model for cpDNA replication asserts that most cpDNA is linear and participates in homologous recombination and replication structures similar to the linear and circular DNA structures of bacteriophage T4. It has been established that some plants have linear cpDNA, such as maize, and that more species still contain complex structures that scientists do not yet understand. When the original experiments on cpDNA were performed, scientists did notice linear structures; however, they attributed these linear forms to broken circles. If the branched and complex structures seen in cpDNA experiments are real and not artifacts of concatenated circular DNA or broken circles, then a D-loop mechanism of replication is insufficient to explain how those structures would replicate. At the same time, homologous recombination does not expand the multiple A --&gt; G gradients seen in plastomes. Because of the failure to explain the deamination gradient as well as the numerous plant species that have been shown to have circular cpDNA, the predominant theory continues to hold that most cpDNA is circular and most likely replicates via a D loop mechanism.\nGene content and protein synthesis.\nThe chloroplast genome most commonly includes around 100 genes that code for a variety of things, mostly to do with the protein pipeline and photosynthesis. As in prokaryotes, genes in chloroplast DNA are organized into operons. Unlike prokaryotic DNA molecules, chloroplast DNA molecules contain introns (plant mitochondrial DNAs do too, but not human mtDNAs).\nAmong land plants, the contents of the chloroplast genome are fairly similar.\nChloroplast genome reduction and gene transfer.\nOver time, many parts of the chloroplast genome were transferred to the nuclear genome of the host, a process called \"endosymbiotic gene transfer\". As a result, the chloroplast genome is heavily reduced compared to that of free-living cyanobacteria. Chloroplasts may contain 60\u2013100 genes whereas cyanobacteria often have more than 1500 genes in their genome. Recently, a plastid without a genome was found, demonstrating chloroplasts can lose their genome during endosymbiotic the gene transfer process.\nEndosymbiotic gene transfer is how we know about the lost chloroplasts in many CASH lineages. Even if a chloroplast is eventually lost, the genes it donated to the former host's nucleus persist, providing evidence for the lost chloroplast's existence. For example, while diatoms (a heterokontophyte) now have a red algal derived chloroplast, the presence of many green algal genes in the diatom nucleus provide evidence that the diatom ancestor had a green algal derived chloroplast at some point, which was subsequently replaced by the red chloroplast.\nIn land plants, some 11\u201314% of the DNA in their nuclei can be traced back to the chloroplast, up to 18% in \"Arabidopsis\", corresponding to about 4,500 protein-coding genes. There have been a few recent transfers of genes from the chloroplast DNA to the nuclear genome in land plants.\nOf the approximately 3000 proteins found in chloroplasts, some 95% of them are encoded by nuclear genes. Many of the chloroplast's protein complexes consist of subunits from both the chloroplast genome and the host's nuclear genome. As a result, protein synthesis must be coordinated between the chloroplast and the nucleus. The chloroplast is mostly under nuclear control, though chloroplasts can also give out signals regulating gene expression in the nucleus, called \"retrograde signaling\".\nProtein synthesis.\nProtein synthesis within chloroplasts relies on two RNA polymerases. One is coded by the chloroplast DNA, the other is of nuclear origin. The two RNA polymerases may recognize and bind to different kinds of promoters within the chloroplast genome. The ribosomes in chloroplasts are similar to bacterial ribosomes.\nProtein targeting and import.\nBecause so many chloroplast genes have been moved to the nucleus, many proteins that would originally have been translated in the chloroplast are now synthesized in the cytoplasm of the plant cell. These proteins must be directed back to the chloroplast, and imported through at least two chloroplast membranes.\nCuriously, around half of the protein products of transferred genes aren't even targeted back to the chloroplast. Many became exaptations, taking on new functions like participating in cell division, protein routing, and even disease resistance. A few chloroplast genes found new homes in the mitochondrial genome\u2014most became nonfunctional pseudogenes, though a few tRNA genes still work in the mitochondrion. Some transferred chloroplast DNA protein products get directed to the secretory pathway, though many secondary plastids are bounded by an outermost membrane derived from the host's cell membrane, and therefore topologically outside of the cell because to reach the chloroplast from the cytosol, the cell membrane must be crossed, which signifies entrance into the extracellular space. In those cases, chloroplast-targeted proteins do initially travel along the secretory pathway.\nBecause the cell acquiring a chloroplast already had mitochondria (and peroxisomes, and a cell membrane for secretion), the new chloroplast host had to develop a unique protein targeting system to avoid having chloroplast proteins being sent to the wrong organelle.\nIn most, but not all cases, nuclear-encoded chloroplast proteins are translated with a \"cleavable transit peptide\" that's added to the N-terminus of the protein precursor. Sometimes the transit sequence is found on the C-terminus of the protein, or within the functional part of the protein.\nTransport proteins and membrane translocons.\nAfter a chloroplast polypeptide is synthesized on a ribosome in the cytosol, an enzyme specific to chloroplast proteins phosphorylates, or adds a phosphate group to many (but not all) of them in their transit sequences.\nPhosphorylation helps many proteins bind the polypeptide, keeping it from folding prematurely. This is important because it prevents chloroplast proteins from assuming their active form and carrying out their chloroplast functions in the wrong place\u2014the cytosol. At the same time, they have to keep just enough shape so that they can be recognized by the chloroplast. These proteins also help the polypeptide get imported into the chloroplast.\nFrom here, chloroplast proteins bound for the stroma must pass through two protein complexes\u2014the TOC complex, or translocon on the outer chloroplast membrane\", and the TIC translocon, or translocon on the inner chloroplast membrane translocon\". Chloroplast polypeptide chains probably often travel through the two complexes at the same time, but the TIC complex can also retrieve preproteins lost in the intermembrane space.\nStructure.\nIn land plants, chloroplasts are generally lens-shaped, 3\u201310 \u03bcm in diameter and 1\u20133 \u03bcm thick. Corn seedling chloroplasts are \u224820 \u00b5m3 in volume. Greater diversity in chloroplast shapes exists among the algae, which often contain a single chloroplast that can be shaped like a net (e.g., \"Oedogonium\"), a cup (e.g., \"Chlamydomonas\"), a ribbon-like spiral around the edges of the cell (e.g., \"Spirogyra\"), or slightly twisted bands at the cell edges (e.g., \"Sirogonium\"). Some algae have two chloroplasts in each cell; they are star-shaped in \"Zygnema\", or may follow the shape of half the cell in order Desmidiales. In some algae, the chloroplast takes up most of the cell, with pockets for the nucleus and other organelles, for example, some species of \"Chlorella\" have a cup-shaped chloroplast that occupies much of the cell.\nAll chloroplasts have at least three membrane systems\u2014the outer chloroplast membrane, the inner chloroplast membrane, and the thylakoid system. Chloroplasts that are the product of secondary endosymbiosis may have additional membranes surrounding these three. Inside the outer and inner chloroplast membranes is the chloroplast stroma, a semi-gel-like fluid that makes up much of a chloroplast's volume, and in which the thylakoid system floats.\nThere are some common misconceptions about the outer and inner chloroplast membranes. The fact that chloroplasts are surrounded by a double membrane is often cited as evidence that they are the descendants of endosymbiotic cyanobacteria. This is often interpreted as meaning the outer chloroplast membrane is the product of the host's cell membrane infolding to form a vesicle to surround the ancestral cyanobacterium\u2014which is not true\u2014both chloroplast membranes are homologous to the cyanobacterium's original double membranes.\nThe chloroplast double membrane is also often compared to the mitochondrial double membrane. This is not a valid comparison\u2014the inner mitochondria membrane is used to run proton pumps and carry out oxidative phosphorylation across to generate ATP energy. The only chloroplast structure that can considered analogous to it is the internal thylakoid system. Even so, in terms of \"in-out\", the direction of chloroplast H ion flow is in the opposite direction compared to oxidative phosphorylation in mitochondria. In addition, in terms of function, the inner chloroplast membrane, which regulates metabolite passage and synthesizes some materials, has no counterpart in the mitochondrion.\nOuter chloroplast membrane.\nThe outer chloroplast membrane is a semi-porous membrane that small molecules and ions can easily diffuse across. However, it is not permeable to larger proteins, so chloroplast polypeptides being synthesized in the cell cytoplasm must be transported across the outer chloroplast membrane by the TOC complex, or \"translocon on the outer chloroplast\" membrane.\nThe chloroplast membranes sometimes protrude out into the cytoplasm, forming a stromule, or stroma-containing tubule. Stromules are very rare in chloroplasts, and are much more common in other plastids like chromoplasts and amyloplasts in petals and roots, respectively. They may exist to increase the chloroplast's surface area for cross-membrane transport, because they are often branched and tangled with the endoplasmic reticulum. When they were first observed in 1962, some plant biologists dismissed the structures as artifactual, claiming that stromules were just oddly shaped chloroplasts with constricted regions or dividing chloroplasts. However, there is a growing body of evidence that stromules are functional, integral features of plant cell plastids, not merely artifacts.\nIntermembrane space and peptidoglycan wall.\nUsually, a thin intermembrane space about 10\u201320 nanometers thick exists between the outer and inner chloroplast membranes.\nGlaucophyte algal chloroplasts have a peptidoglycan layer between the chloroplast membranes. It corresponds to the peptidoglycan cell wall of their cyanobacterial ancestors, which is located between their two cell membranes. These chloroplasts are called \"muroplasts\" (from Latin \"mura\", meaning \"wall\"). Other chloroplasts have lost the cyanobacterial wall, leaving an intermembrane space between the two chloroplast envelope membranes.\nInner chloroplast membrane.\nThe inner chloroplast membrane borders the stroma and regulates passage of materials in and out of the chloroplast. After passing through the TOC complex in the outer chloroplast membrane, polypeptides must pass through the TIC complex \"(translocon on the inner chloroplast membrane)\" which is located in the inner chloroplast membrane.\nIn addition to regulating the passage of materials, the inner chloroplast membrane is where fatty acids, lipids, and carotenoids are synthesized.\nPeripheral reticulum.\nSome chloroplasts contain a structure called the chloroplast peripheral reticulum. It is often found in the chloroplasts of plants, though it has also been found in some angiosperms, and even some gymnosperms. The chloroplast peripheral reticulum consists of a maze of membranous tubes and vesicles continuous with the inner chloroplast membrane that extends into the internal stromal fluid of the chloroplast. Its purpose is thought to be to increase the chloroplast's surface area for cross-membrane transport between its stroma and the cell cytoplasm. The small vesicles sometimes observed may serve as transport vesicles to shuttle stuff between the thylakoids and intermembrane space.\nStroma.\nThe protein-rich, alkaline, aqueous fluid within the inner chloroplast membrane and outside of the thylakoid space is called the stroma, which corresponds to the cytosol of the original cyanobacterium. Nucleoids of chloroplast DNA, chloroplast ribosomes, the thylakoid system with plastoglobuli, starch granules, and many proteins can be found floating around in it. The Calvin cycle, which fixes CO into G3P takes place in the stroma.\nChloroplast ribosomes.\nChloroplasts have their own ribosomes, which they use to synthesize a small fraction of their proteins. Chloroplast ribosomes are about two-thirds the size of cytoplasmic ribosomes (around 17 nm vs 25 nm). They take mRNAs transcribed from the chloroplast DNA and translate them into protein. While similar to bacterial ribosomes, chloroplast translation is more complex than in bacteria, so chloroplast ribosomes include some chloroplast-unique features.\nSmall subunit ribosomal RNAs in several Chlorophyta and euglenid chloroplasts lack motifs for shine-dalgarno sequence recognition, which is considered essential for translation initiation in most chloroplasts and prokaryotes. Such loss is also rarely observed in other plastids and prokaryotes.\nPlastoglobuli.\nPlastoglobuli (singular \"plastoglobulus\", sometimes spelled \"plastoglobule(s)\"), are spherical bubbles of lipids and proteins about 45\u201360 nanometers across. They are surrounded by a lipid monolayer. Plastoglobuli are found in all chloroplasts, but become more common when the chloroplast is under oxidative stress, or when it ages and transitions into a gerontoplast. Plastoglobuli also exhibit a greater size variation under these conditions. They are also common in etioplasts, but decrease in number as the etioplasts mature into chloroplasts.\nPlastoglubuli contain both structural proteins and enzymes involved in lipid synthesis and metabolism. They contain many types of lipids including plastoquinone, vitamin E, carotenoids and chlorophylls.\nPlastoglobuli were once thought to be free-floating in the stroma, but it is now thought that they are permanently attached either to a thylakoid or to another plastoglobulus attached to a thylakoid, a configuration that allows a plastoglobulus to exchange its contents with the thylakoid network. In normal green chloroplasts, the vast majority of plastoglobuli occur singularly, attached directly to their parent thylakoid. In old or stressed chloroplasts, plastoglobuli tend to occur in linked groups or chains, still always anchored to a thylakoid.\nPlastoglobuli form when a bubble appears between the layers of the lipid bilayer of the thylakoid membrane, or bud from existing plastoglubuli\u2014though they never detach and float off into the stroma. Practically all plastoglobuli form on or near the highly curved edges of the thylakoid disks or sheets. They are also more common on stromal thylakoids than on granal ones.\nStarch granules.\nStarch granules are very common in chloroplasts, typically taking up 15% of the organelle's volume, though in some other plastids like amyloplasts, they can be big enough to distort the shape of the organelle. Starch granules are simply accumulations of starch in the stroma, and are not bounded by a membrane.\nStarch granules appear and grow throughout the day, as the chloroplast synthesizes sugars, and are consumed at night to fuel respiration and continue sugar export into the phloem, though in mature chloroplasts, it is rare for a starch granule to be completely consumed or for a new granule to accumulate.\nStarch granules vary in composition and location across different chloroplast lineages. In red algae, starch granules are found in the cytoplasm rather than in the chloroplast. In plants, mesophyll chloroplasts, which do not synthesize sugars, lack starch granules.\nRuBisCO.\nThe chloroplast stroma contains many proteins, though the most common and important is RuBisCO, which is probably also the most abundant protein on the planet. RuBisCO is the enzyme that fixes CO into sugar molecules. In plants, RuBisCO is abundant in all chloroplasts, though in plants, it is confined to the bundle sheath chloroplasts, where the Calvin cycle is carried out in plants.\nPyrenoids.\nThe chloroplasts of some hornworts and algae contain structures called pyrenoids. They are not found in higher plants. Pyrenoids are roughly spherical and highly refractive bodies which are a site of starch accumulation in plants that contain them. They consist of a matrix opaque to electrons, surrounded by two hemispherical starch plates. The starch is accumulated as the pyrenoids mature. In algae with carbon concentrating mechanisms, the enzyme RuBisCO is found in the pyrenoids. Starch can also accumulate around the pyrenoids when CO2 is scarce. Pyrenoids can divide to form new pyrenoids, or be produced \"de novo\".\nThylakoid system.\nThylakoids (sometimes spelled \"thylako\u00efds\"), are small interconnected sacks which contain the membranes that the light reactions of photosynthesis take place on. The word \"thylakoid\" comes from the Greek word \"thylakos\" which means \"sack\".\nSuspended within the chloroplast stroma is the thylakoid system, a highly dynamic collection of membranous sacks called thylakoids where chlorophyll is found and the light reactions of photosynthesis happen.\nIn most vascular plant chloroplasts, the thylakoids are arranged in stacks called grana, though in certain plant chloroplasts and some algal chloroplasts, the thylakoids are free floating.\nThylakoid structure.\nUsing a light microscope, it is just barely possible to see tiny green granules\u2014which were named grana. With electron microscopy, it became possible to see the thylakoid system in more detail, revealing it to consist of stacks of flat thylakoids which made up the grana, and long interconnecting stromal thylakoids which linked different grana.\nIn the transmission electron microscope, thylakoid membranes appear as alternating light-and-dark bands, 8.5 nanometers thick.\nFor a long time, the three-dimensional structure of the thylakoid membrane system had been unknown or disputed. Many models have been proposed, the most prevalent being the helical model, in which granum stacks of thylakoids are wrapped by helical stromal thylakoids. Another model known as the 'bifurcation model', which was based on the first electron tomography study of plant thylakoid membranes, depicts the stromal membranes as wide lamellar sheets perpendicular to the grana columns which bifurcates into multiple parallel discs forming the granum-stroma assembly. The helical model was supported by several additional works, but ultimately it was determined in 2019 that features from both the helical and bifurcation models are consolidated by newly-discovered left-handed helical membrane junctions. Likely for ease, the thylakoid system is still commonly depicted by older \"hub and spoke\" models where the grana are connected to each other by tubes of stromal thylakoids.\nGrana consist of a stacks of flattened circular granal thylakoids that resemble pancakes. Each granum can contain anywhere from two to a hundred thylakoids, though grana with 10\u201320 thylakoids are most common. Wrapped around the grana are multiple parallel right-handed helical stromal thylakoids, also known as frets or lamellar thylakoids. The helices ascend at an angle of ~20\u00b0, connecting to each granal thylakoid at a bridge-like slit junction.\nThe stroma lamellae extend as large sheets perpendicular to the grana columns. These sheets are connected to the right-handed helices either directly or through bifurcations that form left-handed helical membrane surfaces. The left-handed helical surfaces have a similar tilt angle to the right-handed helices (~20\u00b0), but \u00bc the pitch. Approximately 4 left-handed helical junctions are present per granum, resulting in a pitch-balanced array of right- and left-handed helical membrane surfaces of different radii and pitch that consolidate the network with minimal surface and bending energies. While different parts of the thylakoid system contain different membrane proteins, the thylakoid membranes are continuous and the thylakoid space they enclose form a single continuous labyrinth.\nThylakoid composition.\nEmbedded in the thylakoid membranes are important protein complexes which carry out the light reactions of photosynthesis. Photosystem II and photosystem I contain light-harvesting complexes with chlorophyll and carotenoids that absorb light energy and use it to energize electrons. Molecules in the thylakoid membrane use the energized electrons to pump hydrogen ions into the thylakoid space, decreasing the pH and turning it acidic. ATP synthase is a large protein complex that harnesses the concentration gradient of the hydrogen ions in the thylakoid space to generate ATP energy as the hydrogen ions flow back out into the stroma\u2014much like a dam turbine.\nThere are two types of thylakoids\u2014granal thylakoids, which are arranged in grana, and stromal thylakoids, which are in contact with the stroma. Granal thylakoids are pancake-shaped circular disks about 300\u2013600 nanometers in diameter. Stromal thylakoids are helicoid sheets that spiral around grana. The flat tops and bottoms of granal thylakoids contain only the relatively flat photosystem II protein complex. This allows them to stack tightly, forming grana with many layers of tightly appressed membrane, called granal membrane, increasing stability and surface area for light capture.\nIn contrast, photosystem I and ATP synthase are large protein complexes which jut out into the stroma. They can't fit in the appressed granal membranes, and so are found in the stromal thylakoid membrane\u2014the edges of the granal thylakoid disks and the stromal thylakoids. These large protein complexes may act as spacers between the sheets of stromal thylakoids.\nThe number of thylakoids and the total thylakoid area of a chloroplast is influenced by light exposure. Shaded chloroplasts contain larger and more grana with more thylakoid membrane area than chloroplasts exposed to bright light, which have smaller and fewer grana and less thylakoid area. Thylakoid extent can change within minutes of light exposure or removal.\nPigments and chloroplast colors.\nInside the photosystems embedded in chloroplast thylakoid membranes are various photosynthetic pigments, which absorb and transfer light energy. The types of pigments found are different in various groups of chloroplasts, and are responsible for a wide variety of chloroplast colorations.\n box-shadow: 1px 1px 3px rgba(0,0,0,0.2);\"&gt;\nPaper chroma-tography of some spinach leaf extract shows the various pigments present in their chloroplasts.\nXanthophylls\nChlorophyll \"a\"\nChlorophyll \"b\"\nChlorophylls.\nChlorophyll \"a\" is found in all chloroplasts, as well as their cyanobacterial ancestors. Chlorophyll \"a\" is a blue-green pigment partially responsible for giving most cyanobacteria and chloroplasts their color. Other forms of chlorophyll exist, such as the accessory pigments chlorophyll \"b\", chlorophyll \"c\", chlorophyll \"d\", and chlorophyll \"f\".\nChlorophyll \"b\" is an olive green pigment found only in the chloroplasts of plants, green algae, any secondary chloroplasts obtained through the secondary endosymbiosis of a green alga, and a few cyanobacteria. It is the chlorophylls \"a\" and \"b\" together that make most plant and green algal chloroplasts green.\nChlorophyll \"c\" is mainly found in secondary endosymbiotic chloroplasts that originated from a red alga, although it is not found in chloroplasts of red algae themselves. Chlorophyll \"c\" is also found in some green algae and cyanobacteria.\nChlorophylls \"d\" and \"f\" are pigments found only in some cyanobacteria.\nCarotenoids.\nIn addition to chlorophylls, another group of yellow\u2013orange pigments called carotenoids are also found in the photosystems. There are about thirty photosynthetic carotenoids. They help transfer and dissipate excess energy, and their bright colors sometimes override the chlorophyll green, like during the fall, when the leaves of some land plants change color. \u03b2-carotene is a bright red-orange carotenoid found in nearly all chloroplasts, like chlorophyll \"a\". Xanthophylls, especially the orange-red zeaxanthin, are also common. Many other forms of carotenoids exist that are only found in certain groups of chloroplasts.\nPhycobilins.\nPhycobilins are a third group of pigments found in cyanobacteria, and glaucophyte, red algal, and cryptophyte chloroplasts. Phycobilins come in all colors, though phycoerytherin is one of the pigments that makes many red algae red. Phycobilins often organize into relatively large protein complexes about 40 nanometers across called phycobilisomes. Like photosystem I and ATP synthase, phycobilisomes jut into the stroma, preventing thylakoid stacking in red algal chloroplasts. Cryptophyte chloroplasts and some cyanobacteria don't have their phycobilin pigments organized into phycobilisomes, and keep them in their thylakoid space instead.\nSpecialized chloroplasts in plants.\nTo fix carbon dioxide into sugar molecules in the process of photosynthesis, chloroplasts use an enzyme called RuBisCO. RuBisCO has a problem\u2014it has trouble distinguishing between carbon dioxide and oxygen, so at high oxygen concentrations, RuBisCO starts accidentally adding oxygen to sugar precursors. This has the end result of ATP energy being wasted and being released, all with no sugar being produced. This is a big problem, since O is produced by the initial light reactions of photosynthesis, causing issues down the line in the Calvin cycle which uses RuBisCO.\nplants evolved a way to solve this\u2014by spatially separating the light reactions and the Calvin cycle. The light reactions, which store light energy in ATP and NADPH, are done in the mesophyll cells of a leaf. The Calvin cycle, which uses the stored energy to make sugar using RuBisCO, is done in the bundle sheath cells, a layer of cells surrounding a vein in a leaf.\nAs a result, chloroplasts in mesophyll cells and bundle sheath cells are specialized for each stage of photosynthesis. In mesophyll cells, chloroplasts are specialized for the light reactions, so they lack RuBisCO, and have normal grana and thylakoids, which they use to make ATP and NADPH, as well as oxygen. They store in a four-carbon compound, which is why the process is called \" photosynthesis\". The four-carbon compound is then transported to the bundle sheath chloroplasts, where it drops off and returns to the mesophyll. Bundle sheath chloroplasts do not carry out the light reactions, preventing oxygen from building up in them and disrupting RuBisCO activity. Because of this, they lack thylakoids organized into grana stacks\u2014though bundle sheath chloroplasts still have free-floating thylakoids in the stroma where they still carry out cyclic electron flow, a light-driven method of synthesizing ATP to power the Calvin cycle without generating oxygen. They lack photosystem II, and only have photosystem I\u2014the only protein complex needed for cyclic electron flow. Because the job of bundle sheath chloroplasts is to carry out the Calvin cycle and make sugar, they often contain large starch grains.\nBoth types of chloroplast contain large amounts of chloroplast peripheral reticulum, which they use to get more surface area to transport stuff in and out of them. Mesophyll chloroplasts have a little more peripheral reticulum than bundle sheath chloroplasts.\nLocation.\nDistribution in a plant.\nNot all cells in a multicellular plant contain chloroplasts. All green parts of a plant contain chloroplasts\u2014the chloroplasts, or more specifically, the chlorophyll in them are what make the photosynthetic parts of a plant green. The plant cells which contain chloroplasts are usually parenchyma cells, though chloroplasts can also be found in collenchyma tissue. A plant cell which contains chloroplasts is known as a chlorenchyma cell. A typical chlorenchyma cell of a land plant contains about 10 to 100 chloroplasts.\nIn some plants such as cacti, chloroplasts are found in the stems, though in most plants, chloroplasts are concentrated in the leaves. One square millimeter of leaf tissue can contain half a million chloroplasts. Within a leaf, chloroplasts are mainly found in the mesophyll layers of a leaf, and the guard cells of stomata. Palisade mesophyll cells can contain 30\u201370 chloroplasts per cell, while stomatal guard cells contain only around 8\u201315 per cell, as well as much less chlorophyll. Chloroplasts can also be found in the bundle sheath cells of a leaf, especially in C plants, which carry out the Calvin cycle in their bundle sheath cells. They are often absent from the epidermis of a leaf.\nCellular location.\nChloroplast movement.\nThe chloroplasts of plant and algal cells can orient themselves to best suit the available light. In low-light conditions, they will spread out in a sheet\u2014maximizing the surface area to absorb light. Under intense light, they will seek shelter by aligning in vertical columns along the plant cell's cell wall or turning sideways so that light strikes them edge-on. This reduces exposure and protects them from photooxidative damage. This ability to distribute chloroplasts so that they can take shelter behind each other or spread out may be the reason why land plants evolved to have many small chloroplasts instead of a few big ones.\nChloroplast movement is considered one of the most closely regulated stimulus-response systems that can be found in plants. Mitochondria have also been observed to follow chloroplasts as they move.\nIn higher plants, chloroplast movement is run by phototropins, blue light photoreceptors also responsible for plant phototropism. In some algae, mosses, ferns, and flowering plants, chloroplast movement is influenced by red light in addition to blue light, though very long red wavelengths inhibit movement rather than speeding it up. Blue light generally causes chloroplasts to seek shelter, while red light draws them out to maximize light absorption.\nStudies of \"Vallisneria gigantea\", an aquatic flowering plant, have shown that chloroplasts can get moving within five minutes of light exposure, though they don't initially show any net directionality. They may move along microfilament tracks, and the fact that the microfilament mesh changes shape to form a honeycomb structure surrounding the chloroplasts after they have moved suggests that microfilaments may help to anchor chloroplasts in place.\nFunction and chemistry.\nGuard cell chloroplasts.\nUnlike most epidermal cells, the guard cells of plant stomata contain relatively well-developed chloroplasts. However, exactly what they do is controversial.\nPlant innate immunity.\nPlants lack specialized immune cells\u2014all plant cells participate in the plant immune response. Chloroplasts, along with the nucleus, cell membrane, and endoplasmic reticulum, are key players in pathogen defense. Due to its role in a plant cell's immune response, pathogens frequently target the chloroplast.\nPlants have two main immune responses\u2014the hypersensitive response, in which infected cells seal themselves off and undergo programmed cell death, and systemic acquired resistance, where infected cells release signals warning the rest of the plant of a pathogen's presence.\nChloroplasts stimulate both responses by purposely damaging their photosynthetic system, producing reactive oxygen species. High levels of reactive oxygen species will cause the hypersensitive response. The reactive oxygen species also directly kill any pathogens within the cell. Lower levels of reactive oxygen species initiate systemic acquired resistance, triggering defense-molecule production in the rest of the plant.\nIn some plants, chloroplasts are known to move closer to the infection site and the nucleus during an infection.\nChloroplasts can serve as cellular sensors. After detecting stress in a cell, which might be due to a pathogen, chloroplasts begin producing molecules like salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species which can serve as defense-signals. As cellular signals, reactive oxygen species are unstable molecules, so they probably don't leave the chloroplast, but instead pass on their signal to an unknown second messenger molecule. All these molecules initiate retrograde signaling\u2014signals from the chloroplast that regulate gene expression in the nucleus.\nIn addition to defense signaling, chloroplasts, with the help of the peroxisomes, help synthesize an important defense molecule, jasmonate. Chloroplasts synthesize all the fatty acids in a plant cell\u2014linoleic acid, a fatty acid, is a precursor to jasmonate.\nPhotosynthesis.\nOne of the main functions of the chloroplast is its role in photosynthesis, the process by which light is transformed into chemical energy, to subsequently produce food in the form of sugars. Water (H2O) and carbon dioxide (CO2) are used in photosynthesis, and sugar and oxygen (O2) is made, using light energy. Photosynthesis is divided into two stages\u2014the light reactions, where water is split to produce oxygen, and the dark reactions, or Calvin cycle, which builds sugar molecules from carbon dioxide. The two phases are linked by the energy carriers adenosine triphosphate (ATP) and nicotinamide adenine dinucleotide phosphate (NADP+).\nLight reactions.\nThe light reactions take place on the thylakoid membranes. They take light energy and store it in NADPH, a form of NADP+, and ATP to fuel the dark reactions.\nEnergy carriers.\nATP is the phosphorylated version of adenosine diphosphate (ADP), which stores energy in a cell and powers most cellular activities. ATP is the energized form, while ADP is the (partially) depleted form. NADP+ is an electron carrier which ferries high energy electrons. In the light reactions, it gets reduced, meaning it picks up electrons, becoming NADPH.\nPhotophosphorylation.\nLike mitochondria, chloroplasts use the potential energy stored in an H+, or hydrogen ion gradient to generate ATP energy. The two photosystems capture light energy to energize electrons taken from water, and release them down an electron transport chain. The molecules between the photosystems harness the electrons' energy to pump hydrogen ions into the thylakoid space, creating a concentration gradient, with more hydrogen ions (up to a thousand times as many) inside the thylakoid system than in the stroma. The hydrogen ions in the thylakoid space then diffuse back down their concentration gradient, flowing back out into the stroma through ATP synthase. ATP synthase uses the energy from the flowing hydrogen ions to phosphorylate adenosine diphosphate into adenosine triphosphate, or ATP. Because chloroplast ATP synthase projects out into the stroma, the ATP is synthesized there, in position to be used in the dark reactions.\nNADP+ reduction.\nElectrons are often removed from the electron transport chains to charge NADP+ with electrons, reducing it to NADPH. Like ATP synthase, ferredoxin-NADP+ reductase, the enzyme that reduces NADP+, releases the NADPH it makes into the stroma, right where it is needed for the dark reactions.\nBecause NADP+ reduction removes electrons from the electron transport chains, they must be replaced\u2014the job of photosystem II, which splits water molecules (H2O) to obtain the electrons from its hydrogen atoms.\nCyclic photophosphorylation.\nWhile photosystem II photolyzes water to obtain and energize new electrons, photosystem I simply reenergizes depleted electrons at the end of an electron transport chain. Normally, the reenergized electrons are taken by NADP+, though sometimes they can flow back down more H+-pumping electron transport chains to transport more hydrogen ions into the thylakoid space to generate more ATP. This is termed cyclic photophosphorylation because the electrons are recycled. Cyclic photophosphorylation is common in plants, which need more ATP than NADPH.\nDark reactions.\nThe Calvin cycle, also known as the dark reactions, is a series of biochemical reactions that fixes CO2 into G3P sugar molecules and uses the energy and electrons from the ATP and NADPH made in the light reactions. The Calvin cycle takes place in the stroma of the chloroplast.\nWhile named \"the dark reactions\", in most plants, they take place in the light, since the dark reactions are dependent on the products of the light reactions.\nCarbon fixation and G3P synthesis.\nThe Calvin cycle starts by using the enzyme RuBisCO to fix CO2 into five-carbon Ribulose bisphosphate (RuBP) molecules. The result is unstable six-carbon molecules that immediately break down into three-carbon molecules called 3-phosphoglyceric acid, or 3-PGA.\nThe ATP and NADPH made in the light reactions is used to convert the 3-PGA into glyceraldehyde-3-phosphate, or G3P sugar molecules. Most of the G3P molecules are recycled back into RuBP using energy from more ATP, but one out of every six produced leaves the cycle\u2014the end product of the dark reactions.\nSugars and starches.\nGlyceraldehyde-3-phosphate can double up to form larger sugar molecules like glucose and fructose. These molecules are processed, and from them, the still larger sucrose, a disaccharide commonly known as table sugar, is made, though this process takes place outside of the chloroplast, in the cytoplasm.\nAlternatively, glucose monomers in the chloroplast can be linked together to make starch, which accumulates into the starch grains found in the chloroplast.\nUnder conditions such as high atmospheric CO2 concentrations, these starch grains may grow very large, distorting the grana and thylakoids. The starch granules displace the thylakoids, but leave them intact.\nWaterlogged roots can also cause starch buildup in the chloroplasts, possibly due to less sucrose being exported out of the chloroplast (or more accurately, the plant cell). This depletes a plant's free phosphate supply, which indirectly stimulates chloroplast starch synthesis.\nWhile linked to low photosynthesis rates, the starch grains themselves may not necessarily interfere significantly with the efficiency of photosynthesis, and might simply be a side effect of another photosynthesis-depressing factor.\nPhotorespiration.\nPhotorespiration can occur when the oxygen concentration is too high. RuBisCO cannot distinguish between oxygen and carbon dioxide very well, so it can accidentally add O2 instead of CO2 to RuBP. This process reduces the efficiency of photosynthesis\u2014it consumes ATP and oxygen, releases CO2, and produces no sugar. It can waste up to half the carbon fixed by the Calvin cycle. Several mechanisms have evolved in different lineages that raise the carbon dioxide concentration relative to oxygen within the chloroplast, increasing the efficiency of photosynthesis. These mechanisms are called carbon dioxide concentrating mechanisms, or CCMs. These include Crassulacean acid metabolism, carbon fixation, and pyrenoids. Chloroplasts in plants are notable as they exhibit a distinct chloroplast dimorphism.\npH.\nBecause of the H+ gradient across the thylakoid membrane, the interior of the thylakoid is acidic, with a pH around 4, while the stroma is slightly basic, with a pH of around 8.\nThe optimal stroma pH for the Calvin cycle is 8.1, with the reaction nearly stopping when the pH falls below 7.3.\nCO2 in water can form carbonic acid, which can disturb the pH of isolated chloroplasts, interfering with photosynthesis, even though CO2 is used in photosynthesis. However, chloroplasts in living plant cells are not affected by this as much.\nChloroplasts can pump K+ and H+ ions in and out of themselves using a poorly understood light-driven transport system.\nIn the presence of light, the pH of the thylakoid lumen can drop up to 1.5 pH units, while the pH of the stroma can rise by nearly one pH unit.\nAmino acid synthesis.\nChloroplasts alone make almost all of a plant cell's amino acids in their stroma except the sulfur-containing ones like cysteine and methionine. Cysteine is made in the chloroplast (the proplastid too) but it is also synthesized in the cytosol and mitochondria, probably because it has trouble crossing membranes to get to where it is needed. The chloroplast is known to make the precursors to methionine but it is unclear whether the organelle carries out the last leg of the pathway or if it happens in the cytosol.\nOther nitrogen compounds.\nChloroplasts make all of a cell's purines and pyrimidines\u2014the nitrogenous bases found in DNA and RNA. They also convert nitrite (NO2\u2212) into ammonia (NH3) which supplies the plant with nitrogen to make its amino acids and nucleotides.\nOther chemical products.\nThe plastid is the site of diverse and complex lipid synthesis in plants. The carbon used to form the majority of the lipid is from acetyl-CoA, which is the decarboxylation product of pyruvate. Pyruvate may enter the plastid from the cytosol by passive diffusion through the membrane after production in glycolysis. Pyruvate is also made in the plastid from phosphoenolpyruvate, a metabolite made in the cytosol from pyruvate or PGA. Acetate in the cytosol is unavailable for lipid biosynthesis in the plastid. The typical length of fatty acids produced in the plastid are 16 or 18 carbons, with 0-3 cis double bonds.\nThe biosynthesis of fatty acids from acetyl-CoA primarily requires two enzymes. Acetyl-CoA carboxylase creates malonyl-CoA, used in both the first step and the extension steps of synthesis. Fatty acid synthase (FAS) is a large complex of enzymes and cofactors including acyl carrier protein (ACP) which holds the acyl chain as it is synthesized. The initiation of synthesis begins with the condensation of malonyl-ACP with acetyl-CoA to produce ketobutyryl-ACP. 2 reductions involving the use of NADPH and one dehydration creates butyryl-ACP. Extension of the fatty acid comes from repeated cycles of malonyl-ACP condensation, reduction, and dehydration.\nOther lipids are derived from the methyl-erythritol phosphate (MEP) pathway and consist of gibberelins, sterols, abscisic acid, phytol, and innumerable secondary metabolites.\nDifferentiation, replication, and inheritance.\nChloroplasts are a special type of a plant cell organelle called a plastid, though the two terms are sometimes used interchangeably. There are many other types of plastids, which carry out various functions. All chloroplasts in a plant are descended from undifferentiated proplastids found in the zygote, or fertilized egg. Proplastids are commonly found in an adult plant's apical meristems. Chloroplasts do not normally develop from proplastids in root tip meristems\u2014instead, the formation of starch-storing amyloplasts is more common.\nIn shoots, proplastids from shoot apical meristems can gradually develop into chloroplasts in photosynthetic leaf tissues as the leaf matures, if exposed to the required light. This process involves invaginations of the inner plastid membrane, forming sheets of membrane that project into the internal stroma. These membrane sheets then fold to form thylakoids and grana.\nIf angiosperm shoots are not exposed to the required light for chloroplast formation, proplastids may develop into an etioplast stage before becoming chloroplasts. An etioplast is a plastid that lacks chlorophyll, and has inner membrane invaginations that form a lattice of tubes in their stroma, called a prolamellar body. While etioplasts lack chlorophyll, they have a yellow chlorophyll precursor stocked. Within a few minutes of light exposure, the prolamellar body begins to reorganize into stacks of thylakoids, and chlorophyll starts to be produced. This process, where the etioplast becomes a chloroplast, takes several hours. Gymnosperms do not require light to form chloroplasts.\nLight, however, does not guarantee that a proplastid will develop into a chloroplast. Whether a proplastid develops into a chloroplast some other kind of plastid is mostly controlled by the nucleus and is largely influenced by the kind of cell it resides in.\nPlastid interconversion.\nPlastid differentiation is not permanent, in fact many interconversions are possible. Chloroplasts may be converted to chromoplasts, which are pigment-filled plastids responsible for the bright colors seen in flowers and ripe fruit. Starch storing amyloplasts can also be converted to chromoplasts, and it is possible for proplastids to develop straight into chromoplasts. Chromoplasts and amyloplasts can also become chloroplasts, like what happens when a carrot or a potato is illuminated. If a plant is injured, or something else causes a plant cell to revert to a meristematic state, chloroplasts and other plastids can turn back into proplastids. Chloroplast, amyloplast, chromoplast, proplast, etc., are not absolute states\u2014intermediate forms are common.\nDivision.\nMost chloroplasts in a photosynthetic cell do not develop directly from proplastids or etioplasts. In fact, a typical shoot meristematic plant cell contains only 7\u201320 proplastids. These proplastids differentiate into chloroplasts, which divide to create the 30\u201370 chloroplasts found in a mature photosynthetic plant cell. If the cell divides, chloroplast division provides the additional chloroplasts to partition between the two daughter cells.\nIn single-celled algae, chloroplast division is the only way new chloroplasts are formed. There is no proplastid differentiation\u2014when an algal cell divides, its chloroplast divides along with it, and each daughter cell receives a mature chloroplast.\nAlmost all chloroplasts in a cell divide, rather than a small group of rapidly dividing chloroplasts. Chloroplasts have no definite S-phase\u2014their DNA replication is not synchronized or limited to that of their host cells.\nMuch of what we know about chloroplast division comes from studying organisms like \"Arabidopsis\" and the red alga \"Cyanidioschyzon merol\u00e6\".\nThe division process starts when the proteins FtsZ1 and FtsZ2 assemble into filaments, and with the help of a protein ARC6, form a structure called a Z-ring within the chloroplast's stroma. The Min system manages the placement of the Z-ring, ensuring that the chloroplast is cleaved more or less evenly. The protein MinD prevents FtsZ from linking up and forming filaments. Another protein ARC3 may also be involved, but it is not very well understood. These proteins are active at the poles of the chloroplast, preventing Z-ring formation there, but near the center of the chloroplast, MinE inhibits them, allowing the Z-ring to form.\nNext, the two plastid-dividing rings, or PD rings form. The inner plastid-dividing ring is located in the inner side of the chloroplast's inner membrane, and is formed first. The outer plastid-dividing ring is found wrapped around the outer chloroplast membrane. It consists of filaments about 5 nanometers across, arranged in rows 6.4 nanometers apart, and shrinks to squeeze the chloroplast. This is when chloroplast constriction begins. In a few species like \"Cyanidioschyzon merol\u00e6\", chloroplasts have a third plastid-dividing ring located in the chloroplast's intermembrane space.\nLate into the constriction phase, dynamin proteins assemble around the outer plastid-dividing ring, helping provide force to squeeze the chloroplast. Meanwhile, the Z-ring and the inner plastid-dividing ring break down. During this stage, the many chloroplast DNA plasmids floating around in the stroma are partitioned and distributed to the two forming daughter chloroplasts.\nLater, the dynamins migrate under the outer plastid dividing ring, into direct contact with the chloroplast's outer membrane, to cleave the chloroplast in two daughter chloroplasts.\nA remnant of the outer plastid dividing ring remains floating between the two daughter chloroplasts, and a remnant of the dynamin ring remains attached to one of the daughter chloroplasts.\nOf the five or six rings involved in chloroplast division, only the outer plastid-dividing ring is present for the entire constriction and division phase\u2014while the Z-ring forms first, constriction does not begin until the outer plastid-dividing ring forms.\nRegulation.\nIn species of algae that contain a single chloroplast, regulation of chloroplast division is extremely important to ensure that each daughter cell receives a chloroplast\u2014chloroplasts can't be made from scratch. In organisms like plants, whose cells contain multiple chloroplasts, coordination is looser and less important. It is likely that chloroplast and cell division are somewhat synchronized, though the mechanisms for it are mostly unknown.\nLight has been shown to be a requirement for chloroplast division. Chloroplasts can grow and progress through some of the constriction stages under poor quality green light, but are slow to complete division\u2014they require exposure to bright white light to complete division. Spinach leaves grown under green light have been observed to contain many large dumbbell-shaped chloroplasts. Exposure to white light can stimulate these chloroplasts to divide and reduce the population of dumbbell-shaped chloroplasts.\nChloroplast inheritance.\nLike mitochondria, chloroplasts are usually inherited from a single parent. Biparental chloroplast inheritance\u2014where plastid genes are inherited from both parent plants\u2014occurs in very low levels in some flowering plants.\nMany mechanisms prevent biparental chloroplast DNA inheritance, including selective destruction of chloroplasts or their genes within the gamete or zygote, and chloroplasts from one parent being excluded from the embryo. Parental chloroplasts can be sorted so that only one type is present in each offspring.\nGymnosperms, such as pine trees, mostly pass on chloroplasts paternally, while flowering plants often inherit chloroplasts maternally. Flowering plants were once thought to only inherit chloroplasts maternally. However, there are now many documented cases of angiosperms inheriting chloroplasts paternally.\nAngiosperms, which pass on chloroplasts maternally, have many ways to prevent paternal inheritance. Most of them produce sperm cells that do not contain any plastids. There are many other documented mechanisms that prevent paternal inheritance in these flowering plants, such as different rates of chloroplast replication within the embryo.\nAmong angiosperms, paternal chloroplast inheritance is observed more often in hybrids than in offspring from parents of the same species. This suggests that incompatible hybrid genes might interfere with the mechanisms that prevent paternal inheritance.\nTransplastomic plants.\nRecently, chloroplasts have caught attention by developers of genetically modified crops. Since, in most flowering plants, chloroplasts are not inherited from the male parent, transgenes in these plastids cannot be disseminated by pollen. This makes plastid transformation a valuable tool for the creation and cultivation of genetically modified plants that are biologically contained, thus posing significantly lower environmental risks. This biological containment strategy is therefore suitable for establishing the coexistence of conventional and organic agriculture. While the reliability of this mechanism has not yet been studied for all relevant crop species, recent results in tobacco plants are promising, showing a failed containment rate of transplastomic plants at 3 in 1,000,000."}
{"id": "6357", "revid": "1022092866", "url": "https://en.wikipedia.org/wiki?curid=6357", "title": "Camp David", "text": "Camp David is the country retreat for the president of the United States. It is located in the wooded hills of Catoctin Mountain Park, in Frederick County, Maryland, near the towns of Thurmont and Emmitsburg, about 62 miles (100\u00a0km) north-northwest of the national capital city of Washington, D.C. It is officially known as the Naval Support Facility Thurmont. Because it is technically a military installation, the staffing is primarily provided by the Seabees, Civil Engineer Corps (CEC), the United States Navy and the United States Marine Corps. Naval construction battalions are tasked with base construction and send detachments as needed.\nOriginally known as Hi-Catoctin, Camp David was built as a camp for federal government agents and their families by the Works Progress Administration. Construction started in 1935 and was completed in 1938. In 1942, President Franklin D. Roosevelt converted it to a presidential retreat and renamed it \"Shangri-La\", for the fictional Himalayan paradise in the 1933 novel \"Lost Horizon\" by British author James Hilton.\nCamp David received its present name in 1953 from Dwight D. Eisenhower, in honor of his father, and grandson, both named David. Eisenhower had the practice golf facility built at Camp David.\nThe Catoctin Mountain Park does not indicate the location of Camp David on park maps due to privacy and security concerns, although it can be seen through the use of publicly accessible satellite images.\nPresidential use.\nFranklin D. Roosevelt hosted Sir Winston Churchill at Shangri-La in May 1943. Dwight Eisenhower held his first cabinet meeting there on November 22, 1955 following hospitalization and convalescence he required after a heart attack suffered in Denver, Colorado on September 24. Eisenhower met Nikita Khrushchev there for two days of discussions in September 1959.\nJohn F. Kennedy and his family often enjoyed riding, golf and other recreational activities there, and Kennedy often allowed White House staff and Cabinet members to use the retreat when he or his family were not there. Lyndon B. Johnson met with advisors in this setting and hosted both Australian prime minister Harold Holt and Canadian prime minister Lester B. Pearson there. Richard Nixon was a frequent visitor. He personally directed the construction of a swimming pool and other improvements to Aspen Lodge. Gerald Ford hosted Indonesian president Suharto at Camp David.\nJimmy Carter initially favored closing Camp David in order to save money. Once Carter actually visited the retreat, he decided to keep it. Carter brokered the Camp David Accords there in September 1978 between Egyptian president Anwar al-Sadat and Israeli prime minister Menachem Begin. Ronald Reagan visited the retreat more than any other president. In 1984, Reagan hosted British prime minister Margaret Thatcher. Reagan restored the nature trails that Nixon paved over so he could horseback ride at Camp David. George H. W. Bush's daughter, Dorothy Bush Koch, was married there in 1992, in the first wedding held at Camp David. During his tenure as president, Bill Clinton spent every Thanksgiving at Camp David with his family. In July 2000, he hosted the 2000 Camp David Summit negotiations between Israeli prime minister Ehud Barak and Palestinian Authority chairman Yasser Arafat there.\nIn February 2001, George W. Bush held his first meeting with a European leader, UK prime minister Tony Blair, at Camp David, to discuss missile defense, Iraq, and NATO. During his two terms in office, Bush visited Camp David 149 times, for a total of 487 days, for hosting foreign visitors as well as a personal retreat. He met Blair there four times. Among the numerous other foreign leaders he hosted at Camp David were Russian president Vladimir Putin and President Musharraf of Pakistan in 2003, Danish prime minister Anders Fogh Rasmussen in June 2006, and British prime minister Gordon Brown in 2007.\nBarack Obama chose Camp David to host the 38th G8 summit in 2012. President Obama also hosted Russian prime minister Dmitry Medvedev at Camp David, as well as the GCC Summit there in 2015.\nDonald Trump hosted Mitch McConnell and Paul Ryan at Camp David while Republicans prepared to defend both houses of Congress in the 2018 midterm elections. The 46th G7 summit was to be held at Camp David on June 10\u201312, 2020, but was cancelled due to health concerns during the ongoing COVID-19 pandemic.\nPractice golf facility.\nTo be able to play his favorite sport, President Eisenhower had golf course architect Robert Trent Jones design a practice golf facility at Camp David. Around 1954, Jones built one golf hole\u00a0\u2013 a par 3\u00a0\u2013 with four different tees; Eisenhower added a 250-yard (228.6\u00a0m) driving range near the helicopter landing zone.\nSecurity issues.\nOn July 2, 2011, an F-15 intercepted a civilian aircraft approximately from Camp David, when President Obama was in the residence. The two-seater, which was out of radio communication, was escorted to nearby Hagerstown, Maryland, without incident.\nOn July 10, 2011, an F-15 intercepted another small plane near Camp David when Obama was again in the residence; a total of three were intercepted that weekend."}
{"id": "6359", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=6359", "title": "Crux", "text": "Crux () is a constellation centred on four stars in the southern sky in a bright portion of the Milky Way. It is among the most easily distinguished constellations as its hallmark (asterism) stars each have an apparent visual magnitude brighter than +2.8, even though it is the smallest of all 88 modern constellations. Its name is Latin for cross, and it is dominated by a cross-shaped or kite-like asterism that is commonly known as the Southern Cross. It has attained a high level of cultural significance in many Southern Hemisphere states &amp; nations.\nBlue-white \u03b1 Crucis (Acrux) is the most southerly member of the constellation and, at magnitude 0.8, also the brightest. The three remaining stars of the main cross asterism appear clockwise and in order of lessening magnitude: \u03b2 Crucis (Mimosa), \u03b3 Crucis (Gacrux), and \u03b4 Crucis (Imai). \u03b5 Crucis (Ginan) also lies within the cross asterism. Many of these brighter stars are members of the Scorpius\u2013Centaurus Association, a large but loose group of hot blue-white stars that appear to share common origins and motion across the southern Milky Way.\nCrux contains four Cepheid variables, each visible to the naked eye under optimum conditions. Crux also contains the bright and colourful open cluster known as the Jewel Box (NGC 4755) on its western border. To the southeast figures a large, relatively near dark nebula spanning 7\u00b0 by 5\u00b0 known as the Coalsack Nebula, portions of which are mapped in the neighbouring constellations of Centaurus and Musca.\nHistory.\nThe bright stars in Crux were known to the Ancient Greeks, where Ptolemy regarded them as part of the constellation Centaurus. They were entirely visible as far north as Britain in the fourth millennium BC. However, the precession of the equinoxes gradually lowered the stars below the European horizon, and they were eventually forgotten by the inhabitants of northern latitudes. By 400\u00a0CE, the stars in the constellation we now call Crux never rose above the horizon throughout most of Europe. Dante may have known about the constellation in the 14th century, as he describes an asterism of four bright stars in the southern sky in his \"Divine Comedy\". His description, however, may be allegorical, and the similarity to the constellation a coincidence.\nThe 15th\u00a0century Venetian navigator Alvise Cadamosto made note of what was probably the Southern Cross on exiting the Gambia River in 1455, calling it the \"carro dell'ostro\" (\"southern chariot\"). However, Cadamosto's accompanying diagram was inaccurate. Historians generally credit Jo\u00e3o Faras for being the first European to depict it correctly. Faras sketched and described the constellation (calling it \"Las Guardas\") in a letter written on the beaches of Brazil on 1\u00a0May 1500 to the Portuguese monarch.\nExplorer Amerigo Vespucci seems to have observed not only the Southern Cross but also the neighboring Coalsack Nebula on his second voyage in 1501\u20131502.\nAnother early modern description clearly describing Crux as a separate constellation is attributed to Andrea Corsali, an Italian navigator who from 1515\u20131517 sailed to China and the East Indies in an expedition sponsored by King Manuel\u00a0I. In 1516, Corsali wrote a letter to the monarch describing his observations of the southern sky, which included a rather crude map of the stars around the south celestial pole including the Southern Cross and the two Magellanic Clouds seen in an external orientation, as on a globe.\nEmery Molyneux and Petrus Plancius have also been cited as the first uranographers (sky mappers) to distinguish Crux as a separate constellation; their representations date from 1592, the former depicting it on his celestial globe and the latter in one of the small celestial maps on his large wall map. Both authors, however, depended on unreliable sources and placed Crux in the wrong position. Crux was first shown in its correct position on the celestial globes of Petrus Plancius and Jodocus Hondius in 1598 and 1600. Its stars were first catalogued separately from Centaurus by Frederick de Houtman in 1603. The constellation was later adopted by Jakob Bartsch in 1624 and Augustin Royer in 1679. Royer is sometimes wrongly cited as initially distinguishing Crux.\nCharacteristics.\nCrux is bordered by the constellations Centaurus (which surrounds it on three sides) on the east, north and west, and Musca to the south. Covering 68\u00a0square degrees and 0.165% of the night sky, it is the smallest of the 88 constellations. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is \"Cru\". The official constellation boundaries, as set by Belgian astronomer Eug\u00e8ne Delporte in 1930, are defined by a polygon of four segments. In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between \u221255.68\u00b0 and \u221264.70\u00b0. Its totality figures at least part of the year south of the 25th parallel north.\nIn tropical regions Crux can be seen in the sky from April to June. Crux is exactly opposite to Cassiopeia on the celestial sphere, and therefore it cannot appear in the sky with the latter at the same time. In this era, south of Cape Town, Adelaide and Buenos Aires (the 34th parallel south), Crux is circumpolar and thus always appears in the sky.\nCrux is sometimes confused with the nearby False Cross by stargazers. The False Cross is larger and dimmer, does not have a fifth star, and lacks the two prominent nearby \"Pointer Stars\". Between the two is the even larger and dimmer Diamond Cross.\nVisibility.\nCrux is easily visible from the southern hemisphere at practically any time of year. It is also visible near the horizon from tropical latitudes of the northern hemisphere for a few hours every night during the northern winter and spring. For instance, it is visible from Cancun or any other place at latitude 25\u00b0 N or less at around 10 pm at the end of April. There are 5 main stars.\nDue to precession, Crux will move closer to the South Pole in the next millennia, up to 67 degrees south declination for the middle of the constellation. However, by the year 14,000 Crux will be visible for most parts of Europe and continental United States which will extend to North Europe by the year 18,000 as it will be less than 30 degrees south declination.\nUse in navigation.\nIn the Southern Hemisphere, the Southern Cross is frequently used for navigation in much the same way that Polaris is used in the Northern Hemisphere. Projecting a line from \u03b3 to \u03b1 Crucis (the foot of the crucifix) approximately times beyond gives a point close to the Southern Celestial Pole which is also, coincidentally, where intersects a perpendicular line taken southwards from the east-west axis of Alpha Centauri to Beta Centauri, which are stars at an alike declination to Crux and of a similar width as the cross, but higher magnitude. Argentine gauchos are documented as using Crux for night orientation in the Pampas and Patagonia.\nAlpha and Beta Centauri are of similar declinations (thus distance from the pole) and are often referred as the \"Southern Pointers\" or just \"The Pointers\", allowing people to easily identify the Southern Cross, the constellation of Crux. Very few bright stars lie between Crux and the pole itself, although the constellation Musca is fairly easily recognised immediately south of Crux.\nCircumscribed limits around apparent bright stars.\nDown to apparent magnitude +2.5 are 92 stars that shine the brightest as viewed from the Earth. Three of these stars are in Crux making it the most densely populated as to those stars (this being 3.26% of these 92 stars, and in turn being 19.2 times more than the expected 0.17% that would result on a homogenous distribution of all bright stars and a randomised drawing of all 88 constellations, given its area, 0.17% of the sky).\nFeatures.\nStars.\nWithin the constellation's borders, there are 49 stars brighter than or equal to apparent magnitude 6.5. The four main stars that form the asterism are Alpha, Beta, Gamma, and Delta Crucis.\nThere is also a fifth star, that is often included with the Southern Cross.\nThere are several other naked-eye stars within the borders of Crux, especially:\nScorpius\u2013Centaurus Association members.\nUnusually, a total of 15 of the 23 brightest stars in Crux are spectrally blue-white B-type stars. Among the five main bright stars, Delta, and probably Alpha and Beta, are likely co-moving B-type members of the Scorpius\u2013Centaurus Association, the nearest OB association to the Sun. They are among the highest-mass stellar members of the Lower Centaurus-Crux subgroup of the association, with ages of roughly 10 to 20\u00a0million years. Other members include the blue-white stars Zeta, Lambda and both the components of the visual double star, Mu.\nVariable stars.\nCrux contains many variable stars. It boasts four Cepheid variables that may all reach naked eye visibility.\nOther well studied variable stars includes:\nHost star exoplanets in Crux.\nThe star HD 106906 has been found to have a planet\u2014HD 106906 b\u2014that has one of the widest orbits of any currently known planetary-mass companions..\nVisible objects beyond the Local Arm.\nCrux is backlit by the multitude of stars of the Scutum-Crux Arm (more commonly called the Scutum-Centaurus Arm) of the Milky Way. This is the main inner arm in the local radial quarter of the galaxy. Part-obscuring this is:\nA key feature of the Scutum-Crux Arm is:\nCultural significance.\nThe most prominent feature of Crux is the distinctive asterism known as the Southern Cross. It has great significance in the cultures of the southern hemisphere, particularly of Australia and New Zealand.\nFlags and symbols.\nSeveral southern countries and organisations have traditionally used Crux as a national or distinctive symbol. The four or five brightest stars of Crux appear, heraldically standardised in various ways, on the flags of Australia, Brazil, New Zealand, Papua New Guinea and Samoa. They also appear on the flags of the Australian state of Victoria, the Australian Capital Territory, the Northern Territory, as well as the flag of Magallanes Region of Chile, the flag of Londrina (Brazil) and several Argentine provincial flags and emblems (for example, \"Tierra del Fuego\" and \"Santa Cruz\"). The flag of the Mercosur trading zone displays the four brightest stars. Crux also appears on the Brazilian coat of arms and, , on the cover of Brazilian passports.\nFive stars appear in the logo of the Brazilian football team Cruzeiro Esporte Clube and in the insignia of the Order of the Southern Cross, and the cross has featured as name of the Brazilian currency (the \"cruzeiro\" from 1942 to 1986 and again from 1990 to 1994). All coins of the (1998) series of the Brazilian real display the constellation.\nSongs and literature reference the Southern Cross, including the Argentine epic poem \"Mart\u00edn Fierro\". The Argentinian singer Charly Garc\u00eda says that he is \"from the Southern Cross\" in the song \"No voy en tren\".\nThe Cross gets a mention in the lyrics of the Brazilian National Anthem (1909): \"A imagem do Cruzeiro resplandece\" (\"the image of the Cross shines\").\nThe Southern Cross is mentioned in the Australian National Anthem, \"\"Beneath our radiant Southern Cross we'll toil with hearts and hands\"\nThe Southern Cross features in the coat of arms of William Birdwood, 1st Baron Birdwood, the British officer who commanded the Australian and New Zealand Army Corps during the Gallipoli Campaign of the First World War.\nThe Southern Cross is also mentioned in the Samoan\nNational Anthem.\n\"Vaai 'i na fetu o lo'u a agiagia ai:\nLe faailoga lea o Iesu, na maliu ai mo Samoa.\"\" (\"Look at those stars that are waving on it:\nThis is the symbol of Jesus, who died on it for Samoa.\")\nThe 1952-53 NBC Television Series \"Victory At Sea\" contained a musical number entitled \"Beneath the Southern Cross\".\n\"Southern Cross\" is a single released by Crosby, Stills and Nash in 1981. It reached #18 on Billboard Hot 100 in late 1982.\nThe Order of the Southern Cross is a Brazilian order of chivalry awarded to \"those who have rendered significant service to the Brazilian nation\".\nIn \"O Sweet Saint Martin's Land\", the lyrics mention the Southern Cross: \"Thy Southern Cross the night\".\nA stylized version of Crux appears on the Australian Eureka Flag. The constellation was also used on the dark blue, shield-like patch worn by personnel of the U.S. Army's Americal Division, which was organized in the Southern Hemisphere, on the island of New Caledonia, and also on the blue diamond of the U.S. 1st Marine Division, which fought on the Southern Hemisphere islands of Guadalcanal and New Britain.\nThe \"Petersflagge\" flag of the German East Africa Company of 1885\u20131920, which included a constellation of five white five-pointed Crux \"stars\" on a red ground, later served as the model for symbolism associated with generic German colonial-oriented organisations: the Reichskolonialbund of 1936\u20131943 and the (1956/1983 to the present).\nSouthern Cross station is a major rail terminal in Melbourne, Australia.\nThe Personal Ordinariate of Our Lady of the Southern Cross is a personal ordinariate of the Roman Catholic Church primarily within the territory of the Australian Catholic Bishops Conference for groups of Anglicans who desire full communion with the Catholic Church in Australia and Asia.\nThe Knights of the Southern Cross (KSC) is a Catholic fraternal order throughout Australia.\nVarious cultures.\nIn Chinese, (), meaning \"Cross\", refers to an asterism consisting of \u03b3 Crucis, \u03b1 Crucis, \u03b2 Crucis and \u03b4 Crucis.\nIn Australian Aboriginal astronomy, Crux and the Coalsack mark the head of the 'Emu in the Sky' (which is seen in the dark spaces rather than in the patterns of stars) in several Aboriginal cultures, while Crux itself is said to be a possum sitting in a tree (Boorong people of the Wimmera region of northwestern Victoria), a representation of the sky deity Mirrabooka (Quandamooka people of Stradbroke Island), a stingray (Yolngu people of Arnhem Land), or an eagle (Kaurna people of the Adelaide Plains). Two Pacific constellations also included Gamma Centauri. Torres Strait Islanders in modern-day Australia saw Gamma Centauri as the handle and the four stars as the trident of Tagai's Fishing Spear. The Aranda people of central Australia saw the four Cross stars as the talon of an eagle and Gamma Centauri as its leg.\nVarious peoples in the East Indies and Brazil viewed the four main stars as the body of a ray. In both Indonesia and Malaysia, it is known as \"Bintang Pari\" and \"Buruj Pari\", respectively (\"ray stars\"). This aquatic theme is also shared by an archaic name of the constellation in Vietnam, where it was once known as \"sao C\u00e1 Li\u1ec7t\" (the ponyfish star).\nThe Javanese people of Indonesia called this constellation \"Gubug p\u00e8nc\u00e8ng\" (\"raking hut\") or \"lumbung\" (\"the granary\"), because the shape of the constellation was like that of a raking hut.\nThe Southern Cross (\u03b1, \u03b2, \u03b3 and \u03b4 Crucis) together with \u03bc Crucis is one of the asterisms used by Bugis sailors for navigation, called \"binto\u00e9ng bola k\u00e9ppang\", meaning \"incomplete house star\"\nThe M\u0101ori name for the Southern Cross is \"M\u0101hutonga\" and it is thought of as the anchor (\"Te Punga\") of Tama-rereti's \"waka\" (the Milky Way), while the Pointers are its rope. In Tonga it is known as \"Toloa\" (\"duck\"); it is depicted as a duck flying south, with one of his wings (\u03b4 Crucis) wounded because \"Ongo tangata\" (\"two men\", \u03b1 and \u03b2 Centauri) threw a stone at it. The Coalsack is known as \"Humu\" (the \"triggerfish\"), because of its shape. In Samoa the constellation is called \"Sumu\" (\"triggerfish\") because of its rhomboid shape, while \u03b1 and \u03b2 Centauri are called \"Luatagata\" (Two Men), just as they are in Tonga. The peoples of the Solomon Islands saw several figures in the Southern Cross. These included a knee protector and a net used to catch Palolo worms. Neighboring peoples in the Marshall Islands saw these stars as a fish.\nIn Mapudungun, the language of Patagonian Mapuches, the name of the Southern Cross is \"Melipal\", which means \"four stars\". In Quechua, the language of the Inca civilization, Crux is known as \"Chakana\", which means literally \"stair\" (\"chaka\", bridge, link; \"hanan\", high, above), but carries a deep symbolism within Quechua mysticism. Alpha and Beta Crucis make up one foot of the Great Rhea, a constellation encompassing Centaurus and Circinus along with the two bright stars. The Great Rhea was a constellation of the Bororo of Brazil. The Mocov\u00ed people of Argentina also saw a rhea including the stars of Crux. Their rhea is attacked by two dogs, represented by bright stars in Centaurus and Circinus. The dogs' heads are marked by Alpha and Beta Centauri. The rhea's body is marked by the four main stars of Crux, while its head is Gamma Centauri and its feet are the bright stars of Musca. The Bakairi people of Brazil had a sprawling constellation representing a bird snare. It included the bright stars of Crux, the southern part of Centaurus, Circinus, at least one star in Lupus, the bright stars of Musca, Beta and the optical double star Delta1,2 Chamaeleontis: and some of the stars of Volans, and Mensa. The Kalapalo people of Mato Grosso state in Brazil saw the stars of Crux as \"Aganagi\" angry bees having emerged from the Coalsack, which they saw as the beehive.\nAmong Tuaregs, the four most visible stars of Crux are considered \"iggaren\", i.e. four \"Maerua crassifolia\" trees. The Tswana people of Botswana saw the constellation as \"Dithutlwa\", two giraffes \u2013 Alpha and Beta Crucis forming a male, and Gamma and Delta forming the female."}
{"id": "6360", "revid": "87355", "url": "https://en.wikipedia.org/wiki?curid=6360", "title": "Cepheus", "text": "Cepheus (Ancient Greek: \u039a\u03b7\u03c6\u03b5\u03cd\u03c2 \"Kephe\u00fas\") may refer to:"}
{"id": "6361", "revid": "1024009936", "url": "https://en.wikipedia.org/wiki?curid=6361", "title": "Cassiopeia", "text": "Cassiopeia ( ) or Cassiopea may refer to:"}
{"id": "6362", "revid": "41581336", "url": "https://en.wikipedia.org/wiki?curid=6362", "title": "Cetus", "text": "Cetus () is a constellation, sometimes called 'the whale' in English. The Cetus was a sea monster in Greek mythology which both Perseus and Heracles needed to slay. Cetus is in the region of the sky that contains other water-related constellations: Aquarius, Pisces and Eridanus.\nFeatures.\nEcliptic.\nCetus is not among the 12 true zodiac constellations in the J2000 epoch, nor classical 12-part zodiac. The ecliptic passes less than 0.25\u00b0 from one of its corners. Thus the moon and planets will enter Cetus (occulting any stars as a foreground object) in 50% of their successive orbits briefly and the southern part of the sun appears in Cetus for about one day each year. Many asteroids in belts have longer phases occulting the north-western part of Cetus, that bulk with a slightly greater inclination to the ecliptic than the moon and planets.\nAs seen from Mars, the ecliptic (apparent plane of the sun and also the average plane of the planets which is almost the same) passes into it.\nStars.\nMira (\"wonderful\", named by Bayer: Omicron Ceti, a star of the neck of the asterism) was the first variable star to be discovered and the prototype of its class, Mira variables. Over a period of 332 days, it reaches a maximum apparent magnitude of 3 - visible to the naked eye - and dips to a minimum magnitude of 10, invisible to the unaided eye. Its seeming appearance and disappearance gave it its name. Mira pulsates with a minimum size of 400 solar diameters and a maximum size of 500 solar diameters. 420 light-years from Earth, it was discovered by David Fabricius in 1596.\n\u03b1 Ceti, traditionally called Menkar (\"the nose\"), is a red-hued giant star of magnitude 2.5, 220 light-years from Earth. It is a wide double star; the secondary is 93 Ceti, a blue-white hued star of magnitude 5.6, 440 light-years away. \u03b2 Ceti, also called Deneb Kaitos and Diphda is the brightest star in Cetus. It is an orange-hued giant star of magnitude 2.0, 96 light-years from Earth. The traditional name \"Deneb Kaitos\" means \"the whale's tail\". \u03b3 Ceti, Kaffaljidhma (\"head of the whale\") is a very close double star. The primary is a yellow-hued star of magnitude 3.5, 82 light-years from Earth, and the secondary is a blue-hued star of magnitude 6.6. Tau Ceti is noted for being a near Sun-like star at a distance of 11.9 light-years. It is a yellow-hued main-sequence star of magnitude 3.5.\nAA Ceti is a triple star system; the brightest member has a magnitude of 6.2. The primary and secondary are separated by 8.4 arcseconds at an angle of 304 degrees. The tertiary is not visible in telescopes. AA Ceti is an eclipsing variable star; the tertiary star passes in front of the primary and causes the system's apparent magnitude to decrease by 0.5 magnitudes. UV Ceti is an unusual binary variable star. 8.7 light-years from Earth, the system consists of two red dwarfs. Both of magnitude 13. One of the stars is a flare star, which are prone to sudden, random outbursts that last several minutes; these increase the pair's apparent brightness significantly - as high as magnitude 7.\nDeep-sky objects.\nCetus lies far from the galactic plane, so that many distant galaxies are visible, unobscured by dust from the Milky Way. Of these, the brightest is Messier 77 (NGC 1068), a 9th magnitude spiral galaxy near Delta Ceti. It appears face-on and has a clearly visible nucleus of magnitude 10. About 50 million light-years from Earth, M77 is also a Seyfert galaxy and thus a bright object in the radio spectrum. Recently, the galactic cluster JKCS\u00a0041 was confirmed to be the most distant cluster of galaxies yet discovered.\nThe massive cD galaxy Holmberg 15A is also found in Cetus. As is spiral galaxy NGC 1042 and ultra-diffuse galaxy NGC 1052-DF2.\nIC 1613 (Caldwell 51) is an irregular dwarf galaxy near the star 26 Ceti and is a member of the Local Group.\nNGC 246 (Caldwell 56), also called the Cetus Ring, is a planetary nebula with a magnitude of 8.0, 1600 light-years from Earth. Among some amateur astronomers, NGC 246 has garnered the nickname \"Pac-Man Nebula\" because of the arrangement of its central stars and the surrounding star field.\nHistory and mythology.\nCetus may have originally been associated with a whale, which would have had mythic status amongst Mesopotamian cultures. It is often now called the Whale, though it is most strongly associated with Cetus the sea-monster, who was slain by Perseus as he saved the princess Andromeda from Poseidon's wrath. It is in the middle of \"The Sea\" recognised by mythologists, a set of water-associated constellations, its other members being Eridanus, Pisces, Piscis Austrinus and Aquarius.\nCetus has been depicted in many ways throughout its history. In the 17th century, Cetus was depicted as a \"dragon fish\" by Johann Bayer. Both Willem Blaeu and Andreas Cellarius depicted Cetus as a whale-like creature in the same century. However, Cetus has also been variously depicted with animal heads attached to a piscine body.\nIn global astronomy.\nIn Chinese astronomy, the stars of Cetus are found among two areas: the Black Tortoise of the North (\u5317\u65b9\u7384\u6b66, \"B\u011bi F\u0101ng Xu\u00e1n W\u01d4\") and the White Tiger of the West (\u897f\u65b9\u767d\u864e, \"X\u012b F\u0101ng B\u00e1i H\u01d4\").\nThe Tukano and Kobeua people of the Amazon used the stars of Cetus to create a jaguar, representing the god of hurricanes and other violent storms. Lambda, Mu, Xi, Nu, Gamma, and Alpha Ceti represented its head; Omicron, Zeta, and Chi Ceti represented its body; Eta Eri, Tau Cet, and Upsilon Cet marked its legs and feet; and Theta, Eta, and Beta Ceti delineated its tail.\nIn Hawaii, the constellation was called \"Na Kuhi\", and Mira (Omicron Ceti) may have been called \"Kane\".\nNamesakes.\nUSS Cetus (AK-77) was a United States Navy Crater class cargo ship named after the constellation.\n\"Cetus\" is the title of a ragtime piano composition by Tom Brier on the album \"Constellations\"."}
{"id": "6363", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=6363", "title": "Carina (constellation)", "text": "Carina ( (U.S.) (Brit.)) is a constellation in the southern sky. Its name is Latin for the hull or keel of a ship, and it was the southern foundation of the larger constellation of Argo Navis (the ship \"Argo\") until it was divided into three pieces, the other two being Puppis (the poop deck), and Vela (the sails of the ship).\nHistory and mythology.\nCarina was once a part of Argo Navis, the great ship of Jason and the Argonauts who searched for the Golden Fleece. The constellation of Argo was introduced in ancient Greece. However, due to the massive size of Argo Navis and the sheer number of stars that required separate designation, Nicolas Louis de Lacaille divided Argo into three sections in 1763, including Carina (the hull or keel). In the 19th century, these three became established as separate constellations, and were formally included in the list of 88 modern IAU constellations in 1930. Lacaille kept a single set of Greek letters for the whole of Argo, and separate sets of Latin letter designations for each of the three sections. Therefore, Carina has the \u03b1, \u03b2 and \u03b5, Vela has \u03b3 and \u03b4, Puppis has \u03b6, and so on.\nNotable features.\nStars.\nCarina contains Canopus, a white-hued supergiant that is the second brightest star in the night sky at magnitude \u22120.72. Alpha Carinae, as Canopus is formally designated, is 313 light-years from Earth. Its traditional name comes from the mythological Canopus, who was a navigator for Menelaus, king of Sparta.\nThere are several other stars above magnitude 3 in Carina. Beta Carinae, traditionally called Miaplacidus, is a blue-white hued star of magnitude 1.7, 111 light-years from Earth. Epsilon Carinae is an orange-hued giant star similarly bright to Miaplacidus at magnitude 1.9; it is 630 light-years from Earth. Another fairly bright star is the blue-white hued Theta Carinae; it is a magnitude 2.7 star 440 light-years from Earth. Theta Carinae is also the most prominent member of the cluster IC 2602. Iota Carinae is a white-hued supergiant star of magnitude 2.2, 690 light-years from Earth.\nEta Carinae is the most prominent variable star in Carina; with a mass of approximately 100 solar masses and 4 million times as bright as the Sun. It was first discovered to be unusual in 1677, when its magnitude suddenly rose to 4, attracting the attention of Edmond Halley. Eta Carinae is inside NGC 3372, commonly called the Carina Nebula. It had a long outburst in 1827, when it brightened to magnitude 1, only fading to magnitude 1.5 in 1828. Its most prominent outburst made Eta Carinae the equal of Sirius; it brightened to magnitude \u22121.5 in 1843. In the decades following 1843 it appeared relatively placid, having a magnitude between 6.5 and 7.9. However, in 1998, it brightened again, though only to magnitude 5.0, a far less drastic outburst. Eta Carinae is a binary star, with a companion that has a period of 5.5 years; the two stars are surrounded by the Homunculus Nebula, which is composed of gas that was ejected in 1843.\nThere are several less prominent variable stars in Carina. l Carinae is a Cepheid variable noted for its brightness; it is the brightest Cepheid that is variable to the unaided eye. It is a yellow-hued supergiant star with a minimum magnitude of 4.2 and a maximum magnitude of 3.3; it has a period of 35.5 days.\nTwo bright Mira variable stars are in Carina: R Carinae and S Carinae; both stars are red giants. R Carinae has a minimum magnitude of 10.0 and a maximum magnitude of 4.0. Its period is 309 days and it is 416 light-years from Earth. S Carinae is similar, with a minimum magnitude of 10.0 and a maximum magnitude of 5.0. However, S Carinae has a shorter period\u00a0\u2013 150 days, though it is much more distant at 1300 light-years from Earth.\nCarina is home to several double stars and binary stars. Upsilon Carinae is a binary star with two blue-white hued giant components, 1600 light-years from Earth. The primary is of magnitude 3.0 and the secondary is of magnitude 6.0; the two components are distinguishable in a small amateur telescope.\nTwo asterisms are prominent in Carina. One is known as the 'Diamond Cross', which is larger than the Southern Cross (but fainter), and, from the perspective of the southern hemisphere viewer, upside down, the long axes of the two crosses being close to parallel. Another asterism in the constellation is the False Cross, often mistaken for the Southern Cross, which is an asterism in Crux. The False Cross consists of two stars in Carina, Iota Carinae and Epsilon Carinae, and two stars in Vela, Kappa Velorum and Delta Velorum.\nDeep-sky objects.\nCarina is known for its namesake nebula, NGC 3372, discovered by French astronomer Nicolas Louis de Lacaille in 1751, which contains several nebulae. The Carina Nebula overall is an extended emission nebula approximately 8,000 light-years away and 300 light-years wide that includes vast star-forming regions. It has an overall magnitude of 8.0 and an apparent diameter of over 2 degrees. Its central region is called the Keyhole, or the Keyhole Nebula. This was described in 1847 by John Herschel, and likened to a keyhole by Emma Converse in 1873. The Keyhole is about seven light-years wide and is composed mostly of ionized hydrogen, with two major star-forming regions. The Homunculus Nebula is a planetary nebula visible to the naked eye that is being ejected by the erratic luminous blue variable star Eta Carinae, the most massive visible star known. Eta Carinae is so massive that it has reached the theoretical upper limit for the mass of a star and is therefore unstable. It is known for its outbursts; in 1840 it briefly became one of the brightest stars in the sky due to a particularly massive outburst, which largely created the Homunculus Nebula. Because of this instability and history of outbursts, Eta Carinae is considered a prime supernova candidate for the next several hundred thousand years because it has reached the end of its estimated million-year life span.\nNGC 2516 is an open cluster that is both quite large (approximately half a degree square) and bright, visible to the unaided eye. It is located 1100 light-years from Earth and has approximately 80 stars, the brightest of which is a red giant star of magnitude 5.2. NGC 3114 is another open cluster approximately of the same size, though it is more distant at 3000 light-years from Earth. It is more loose and dim than NGC 2516, as its brightest stars are only 6th magnitude. The most prominent open cluster in Carina is IC 2602, also called the \"Southern Pleiades\". It contains Theta Carinae, along with several other stars visible to the unaided eye. In total, the cluster possesses approximately 60 stars. The Southern Pleiades is particularly large for an open cluster, with a diameter of approximately one degree. Like IC 2602, NGC 3532 is visible to the unaided eye and is of comparable size. It possesses approximately 150 stars that are arranged in an unusual shape, approximating an ellipse with a dark central area. Several prominent orange giants are among the cluster's bright stars, of the 7th magnitude. Superimposed on the cluster is Chi Carinae, a yellow-white hued star of magnitude 3.9, far more distant than NGC 3532.\nCarina also contains the naked-eye globular cluster NGC 2808. Epsilon Carinae and Upsilon Carinae are double stars visible in small telescopes.\nOne noted galaxy cluster is 1E 0657-56, the Bullet Cluster. At a distance of 4 billion light years (redshift 0.296), this galaxy cluster is named for the shock wave seen in the intracluster medium, which resembles the shock wave of a supersonic bullet. The bow shock visible is thought to be due to the smaller galaxy cluster moving through the intracluster medium at a relative speed of 3000\u20134000 kilometers per second to the larger cluster. Because this gravitational interaction has been ongoing for hundreds of millions of years, the smaller cluster is being destroyed and will eventually merge with the larger cluster.\nMeteors.\nCarina contains the radiant of the Eta Carinids meteor shower, which peaks around January 21 each year.\nEquivalents.\nFrom China (especially northern China), the stars of Carina can barely be seen. The star Canopus (the south polar star in Chinese astronomy) was located by Chinese astronomers in the Vermilion Bird of the South (\u5357\u65b9\u6731\u96c0, \"N\u00e1n F\u0101ng Zh\u016b Qu\u00e8\"). The rest of the stars were first classified by Xu Guanggi during the Ming Dynasty, based on the knowledge acquired from western star charts, and placed among The Southern Asterisms (\u8fd1\u5357\u6975\u661f\u5340, \"J\u00ecnn\u00e1nj\u00edx\u012bng\u014du\").\nPolynesian peoples had no name for the constellation in particular, though they had many names for Canopus.\nThe M\u0101ori name \"Ariki\" (\"High-born\"), . and the Hawaiian \"Ke Alii-o-kona-i-ka-lewa\", \"The Chief of the southern expanse\". both attest to the star's prominence in the southern sky, while the M\u0101ori \"Atutahi\", \"First-light\" or \"Single-light\", and the Tuamotu \"Te Tau-rari\" and \"Marere-te-tavahi\", \"He-who-stands-alone\". refer to the star's solitary nature.\nIt was also called \"Kapae-poto\", (\"Short horizon\"), because it rarely sets from the vantage point of New Zealand; and \"Kauanga\" (\"Solitary\"), when it was the last star visible before sunrise.\nFuture.\nCarina is in the southern sky quite near the south celestial pole, making it never set (circumpolar) for most of the southern hemisphere. Due to precession of Earth's axis, by the year 4700 the south celestial pole will be in Carina. Three bright stars in Carina will come within 1 degree of the southern celestial pole and take turns as the southern pole star: Omega Carinae (mag 3.29) in 5600, Upsilon Carinae (mag 2.97) in 6700, and Iota Carinae (mag 2.21) in 7900. About 13860, the bright Canopus (-0.7) will have a greater declination than -82\u00b0.\nNamesakes.\n was a United States Navy Crater class cargo ship named after the constellation."}
{"id": "6364", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=6364", "title": "Camelopardalis", "text": "Camelopardalis is a large but faint constellation of the northern sky representing a giraffe. The constellation was introduced in 1612 or 1613 by Petrus Plancius. Some older astronomy books give Camelopardalus or Camelopardus as alternative forms of the name, but the version recognized by the International Astronomical Union matches the genitive form, seen suffixed to most of its key stars.\nEtymology.\nFirst attested in English in 1785, the word \"camelopardalis\" comes from Latin, and it is the romanization of the Greek \"\u03ba\u03b1\u03bc\u03b7\u03bb\u03bf\u03c0\u03ac\u03c1\u03b4\u03b1\u03bb\u03b9\u03c2\" meaning \"giraffe\", from \"\u03ba\u03ac\u03bc\u03b7\u03bb\u03bf\u03c2\" (\"kam\u0113los\"), \"camel\" + \"\u03c0\u03ac\u03c1\u03b4\u03b1\u03bb\u03b9\u03c2\" (\"pardalis\"), \"spotted\", because it has a long neck like a camel and spots.\nFeatures.\nStars.\nAlthough Camelopardalis is the 18th largest constellation, it is not a particularly bright constellation, as the brightest stars are only of fourth magnitude. In fact, it only contains four stars brighter than magnitude 5.0.\nOther variable stars are U Camelopardalis, VZ Camelopardalis, and Mira variables T Camelopardalis, X Camelopardalis, and R Camelopardalis. RU Camelopardalis is one of the brighter Type II Cepheids visible in the night sky.\nIn 2011 a supernova was discovered in the constellation.\nDeep-sky objects.\nCamelopardalis is in the part of the celestial sphere facing away from the galactic plane. Accordingly, many distant galaxies are visible within its borders. \nMeteor showers.\nThe annual May meteor shower Camelopardalids from comet 209P/LINEAR have a radiant in Camelopardalis.\nSpace exploration.\nThe space probe \"Voyager 1\" is moving in the direction of this constellation, though it will not be nearing any of the stars in this constellation for many thousands of years, by which time its power source will be long dead.\nHistory.\nCamelopardalis is not one of Ptolemy's 48 constellations in the \"Almagest\". It was created by Petrus Plancius in 1613. It first appeared in a globe designed by him and produced by Pieter van den Keere. One year later, Jakob Bartsch featured it in his atlas. Johannes Hevelius depicted this constellation in his works which were so influential that it was referred to as Camelopardali Hevelii or abbreviated as Camelopard. Hevel.\nPart of the constellation was hived off to form the constellation Sciurus Volans, the Flying Squirrel, by William Croswell in 1810. However this was not taken up by later cartographers.\nEquivalents.\nIn Chinese astronomy, the stars of Camelopardalis are located within a group of circumpolar stars called the Purple Forbidden Enclosure (\u7d2b\u5fae\u57a3 \"Z\u01d0 W\u0113i Yu\u00e1n\")."}
{"id": "6365", "revid": "1018187701", "url": "https://en.wikipedia.org/wiki?curid=6365", "title": "Convention of Kanagawa", "text": "The Convention of Kanagawa, also known as the Kanagawa Treaty (, \"Kanagawa J\u014dyaku\") or the Japan\u2013US Treaty of Peace and Amity (, \"Nichibei Washin J\u014dyaku\"), was a treaty signed between the United States and the Tokugawa Shogunate on March 31, 1854. Signed under threat of force, it effectively meant the end of Japan's 220-year-old policy of national seclusion (\"sakoku\") by opening the ports of Shimoda and Hakodate to American vessels. It also ensured the safety of American castaways and established the position of an American consul in Japan. The treaty precipitated the signing of similar treaties establishing diplomatic relations with other Western powers.\nIsolation of Japan.\nSince the beginning of the 17th century, the Tokugawa shogunate pursued a policy of isolating the country from outside influences. Foreign trade was maintained only with the Dutch and the Chinese and was conducted exclusively at Nagasaki under a strict government monopoly. This \"Pax Tokugawa\" period is largely associated with domestic peace, social stability, commercial development, and expanded literacy. This policy had two main objectives:\nBy the early 19th century, this policy of isolation was increasingly under challenge. In 1844, King William II of the Netherlands sent a letter urging Japan to end the isolation policy on its own before change would be forced from the outside. In 1846, an official American expedition led by Commodore James Biddle arrived in Japan asking for ports to be opened for trade but was sent away.\nPerry expedition.\nIn 1853, United States Navy Commodore Matthew C. Perry was sent with a fleet of warships by U.S. President Millard Fillmore to force the opening of Japanese ports to American trade, through the use of gunboat diplomacy if necessary. The growing commerce between America and China, the presence of American whalers in waters offshore Japan, and the increasing monopolization of potential coaling stations by the British and French in Asia were all contributing factors. The Americans were also driven by concepts of manifest destiny and the desire to impose the benefits of western civilization and the Christian religion on what they perceived as backward Asian nations. From the Japanese perspective, increasing contacts with foreign warships and the increasing disparity between western military technology and the Japanese feudal armies created growing concern. The Japanese had been keeping abreast of world events via information gathered from Dutch traders in Dejima and had been forewarned by the Dutch of Perry's voyage. There was a considerable internal debate in Japan on how best to meet this potential threat to Japan's economic and political sovereignty in light of events occurring in China with the Opium Wars.\nPerry arrived with four warships at Uraga, at the mouth of Edo Bay on July 8, 1853. He blatantly refused Japanese demands that he proceed to Nagasaki, which was the designated port for foreign contact. After threatening to continue directly on to Edo, the nation's capital, and to burn it to the ground if necessary, he was allowed to land at nearby Kurihama on July 14 and to deliver his letter. Such refusal was intentional, as Perry wrote in his journal: \u201cTo show these princes how little I regarded their order for me to depart, on getting on board I immediately ordered the whole squadron underway, not to leave the bay\u2026 but to go higher up\u2026 would produce a decided influence upon the pride and conceit of the gov\u2019t, and cause a more favorable consideration of the President\u2019s letter.\" Perry\u2019s power front did not stop with refusing to land in Uraga, but he continued to push the boundaries of the Japanese. He ordered the squadron to survey Edo bay, which led to a stand-off between Japanese officers with swords and Americans with guns. By firing the guns into the water, Perry demonstrated their military might, which greatly affected Japanese perceptions of Perry and the United States. Namely, a perception of fear and disrespect.\nDespite years of debate on the isolation policy, Perry's letter created great controversy within the highest levels of the Tokugawa shogunate. The \"sh\u014dgun\" himself, Tokugawa Ieyoshi, died days after Perry's departure and was succeeded by his sickly young son, Tokugawa Iesada, leaving effective administration in the hands of the Council of Elders (\"r\u014dj\u016b\") led by Abe Masahiro. Abe felt that it was impossible for Japan to resist the American demands by military force and yet was reluctant to take any action on his own authority for such an unprecedented situation. Attempting to legitimize any decision taken, Abe polled all of the \"daimy\u014d\" for their opinions. This was the first time that the Tokugawa shogunate had allowed its decision-making to be a matter of public debate and had the unforeseen consequence of portraying the shogunate as weak and indecisive. The results of the poll also failed to provide Abe with an answer; of the 61 known responses, 19 were in favour of accepting the American demands and 19 were equally opposed. Of the remainder, 14 gave vague responses expressing concern of possible war, 7 suggested making temporary concessions and 2 advised that they would simply go along with whatever was decided.\nPerry returned again on February 11, 1854, with an even larger force of eight warships and made it clear that he would not be leaving until a treaty was signed. Perry continued his manipulation of the setting, such as keeping himself aloof from lower-ranking officials, implying the use of force, surveying the harbor, and refusing to meet in the designated negotiation sites. Negotiations began on March 8 and proceeded for around one month. Each party shared a performance when Perry arrived. The Americans had a technology demonstration, and the Japanese had a sumo wrestling show. While the new technology awed the Japanese people, Perry was unimpressed by the sumo wrestlers and perceived such performance as foolish and degrading: \u201cThis disgusting exhibition did not terminate until the whole twenty-five had, successively, in pairs, displayed their immense powers and savage qualities.\" The Japanese side gave in to almost all of Perry's demands, with the exception of a commercial agreement modelled after previous American treaties with China, which Perry agreed to defer to a later time. The main controversy centered on the selection of the ports to open, with Perry adamantly rejecting Nagasaki. The treaty, written in English, Dutch, Chinese and Japanese, was signed on March 31, 1854, at what is now Kaik\u014d Hiroba (Port Opening Square) Yokohama, a site adjacent to the current Yokohama Archives of History.\nTreaty of Peace and Amity (1854).\nThe \"Japan-US Treaty of Peace and Amity\" has twelve articles: \nAt the time, \"sh\u014dgun\" Tokugawa Iesada was the de facto ruler of Japan; for the Emperor of Japan to interact in any way with foreigners was out of the question. Perry concluded the treaty with representatives of the shogun, led by plenipotentiary and the text was endorsed subsequently, albeit reluctantly, by Emperor K\u014dmei.\nThe treaty was ratified on February 21, 1855.\nConsequences of the treaty.\nIn the short term, the U.S. was content with the agreement since Perry had achieved his primary objective of breaking Japan's \"sakoku\" policy and setting the grounds for protection of American citizens and an eventual commercial agreement. On the other hand, the Japanese were forced into this trade, and many saw it as a sign of weakness. The Tokugawa shogunate could point out that the treaty was not actually signed by the shogun, or indeed any of his \"r\u014dj\u016b\", and that it had at least temporarily averted the possibility of immediate military confrontation.\nExternally, the treaty led to the United States-Japan Treaty of Amity and Commerce, the \"Harris Treaty\" of 1858, which allowed the establishment of foreign concessions, extraterritoriality for foreigners, and minimal import taxes for foreign goods. The Japanese chafed under the \"unequal treaty system\" which characterized Asian and western relations during this period. The Kanagawa treaty was also followed by similar agreements with the United Kingdom (Anglo-Japanese Friendship Treaty, October 1854), Russia (Treaty of Shimoda, February 7, 1855), and France (Treaty of Amity and Commerce between France and Japan, October 9, 1858).\nInternally, the treaty had far-reaching consequences. Decisions to suspend previous restrictions on military activities led to re-armament by many domains and further weakened the position of the shogun. Debate over foreign policy and popular outrage over perceived appeasement to the foreign powers was a catalyst for the \"sonn\u014d j\u014di\" movement and a shift in political power from Edo back to the Imperial Court in Kyoto. The opposition of Emperor K\u014dmei to the treaties further lent support to the \"t\u014dbaku\" (overthrow the shogunate) movement, and eventually to the Meiji Restoration, which affected all realms of Japanese life. Following this period came an increase in foreign trade, the rise of Japanese military might, and the later rise of Japanese economic and technological advancement. Westernization at the time was a defense mechanism, but Japan has since found a balance between Western popular culture and Japanese tradition. "}
{"id": "6366", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=6366", "title": "Canis Major", "text": "Canis Major is a constellation in the southern celestial hemisphere. In the second century, it was included in Ptolemy's 48 constellations, and is counted among the 88 modern constellations. Its name is Latin for \"greater dog\" in contrast to Canis Minor, the \"lesser dog\"; both figures are commonly represented as following the constellation of Orion the hunter through the sky. The Milky Way passes through Canis Major and several open clusters lie within its borders, most notably M41.\nCanis Major contains Sirius, the brightest star in the night sky, known as the \"dog star\". It is bright because of its proximity to the Solar System. In contrast, the other bright stars of the constellation are stars of great distance and high luminosity. At magnitude 1.5, Epsilon Canis Majoris (Adhara) is the second-brightest star of the constellation and the brightest source of extreme ultraviolet radiation in the night sky. Next in brightness are the yellow-white supergiant Delta (Wezen) at 1.8, the blue-white giant Beta (Mirzam) at 2.0, blue-white supergiants Eta (Aludra) at 2.4 and Omicron2 at 3.0, and white spectroscopic binary Zeta (Furud), also at 3.0. The red hypergiant VY Canis Majoris is one of the largest stars known, while the neutron star RX J0720.4-3125 has a radius of a mere 5\u00a0km.\nHistory and mythology.\nIn western astronomy.\nIn ancient Mesopotamia, Sirius, named KAK.SI.DI by the Babylonians, was seen as an arrow aiming towards Orion, while the southern stars of Canis Major and a part of Puppis were viewed as a bow, named BAN in the \"Three Stars Each\" tablets, dating to around 1100 BC. In the later compendium of Babylonian astronomy and astrology titled \"MUL.APIN\", the arrow, Sirius, was also linked with the warrior Ninurta, and the bow with Ishtar, daughter of Enlil. Ninurta was linked to the later deity Marduk, who was said to have slain the ocean goddess Tiamat with a great bow, and worshipped as the principal deity in Babylon. The Ancient Greeks replaced the bow and arrow depiction with that of a dog.\nIn Greek Mythology, Canis Major represented the dog Laelaps, a gift from Zeus to Europa; or sometimes the hound of Procris, Diana's nymph; or the one given by Aurora to Cephalus, so famed for its speed that Zeus elevated it to the sky. It was also considered to represent one of Orion's hunting dogs, pursuing Lepus the Hare or helping Orion fight Taurus the Bull; and is referred to in this way by Aratos, Homer and Hesiod. The ancient Greeks refer only to one dog, but by Roman times, Canis Minor appears as Orion's second dog. Alternative names include Canis Sequens and Canis Alter. Canis Syrius was the name used in the 1521 \"Alfonsine tables\".\nThe Roman myth refers to Canis Major as \"Custos Europae\", the dog guarding Europa but failing to prevent her abduction by Jupiter in the form of a bull, and as \"Janitor Lethaeus\", \"the watchdog\". In medieval Arab astronomy, the constellation became \"al-Kalb al-Akbar\", \"the Greater Dog\", transcribed as \"Alcheleb Alachbar\" by 17th century writer Edmund Chilmead. Islamic scholar Ab\u016b Ray\u1e25\u0101n al-B\u012br\u016bn\u012b referred to Orion as \"Kalb al-Jabb\u0101r\", \"the Dog of the Giant\". Among the Merazig of Tunisia, shepherds note six constellations that mark the passage of the dry, hot season. One of them, called \"Merzem\", includes the stars of Canis Major and Canis Minor and is the herald of two weeks of hot weather.\nIn non-western astronomy.\nIn Chinese astronomy, the modern constellation of Canis Major is located in the Vermilion Bird (), where the stars were classified in several separate asterisms of stars. The Military Market () was a circular pattern of stars containing Nu3, Beta, Xi1 and Xi2, and some stars from Lepus. The Wild Cockerel () was at the centre of the Military Market, although it is uncertain which stars depicted what. Schlegel reported that the stars Omicron and Pi Canis Majoris might have been them, while Beta or Nu2 have also been proposed. Sirius was ' (), the Celestial Wolf, denoting invasion and plunder. Southeast of the Wolf was the asterism ' (), the celestial Bow and Arrow, which was interpreted as containing Delta, Epsilon, Eta and Kappa Canis Majoris and Delta Velorum. Alternatively, the arrow was depicted by Omicron2 and Eta and aiming at Sirius (the Wolf), while the bow comprised Kappa, Epsilon, Sigma, Delta and 164 Canis Majoris, and Pi and Omicron Puppis.\nBoth the M\u0101ori people and the people of the Tuamotus recognized the figure of Canis Major as a distinct entity, though it was sometimes absorbed into other constellations. ', also called ' and ', (\"The Assembly of \" or \"The Assembly of Sirius\") was a M\u0101ori constellation that included both Canis Minor and Canis Major, along with some surrounding stars. Related was ', also called ', the Mirror of , formed from an undefined group of stars in Canis Major. They called Sirius ' and ', corresponding to two of the names for the constellation, though ' was a name applied to other stars in various M\u0101ori groups and other Polynesian cosmologies. The Tuamotu people called Canis Major \"\", \"the abiding assemblage of \".\nThe Tharumba people of the Shoalhaven River saw three stars of Canis Major as ' (Bat) and his two wives ' (Mrs Brown Snake) and ' (Mrs Black Snake); bored of following their husband around, the women try to bury him while he is hunting a wombat down its hole. He spears them and all three are placed in the sky as the constellation '. To the Boorong people of Victoria, Sigma Canis Majoris was ' (which has become the official name of this star), and its flanking stars Delta and Epsilon were his two wives. The moon (', \"native cat\") sought to lure the further wife (Epsilon) away, but assaulted him and he has been wandering the sky ever since.\nCharacteristics.\nCanis Major is a constellation in the Southern Hemisphere's summer (or northern hemisphere's winter) sky, bordered by Monoceros (which lies between it and Canis Minor) to the north, Puppis to the east and southeast, Columba to the southwest, and Lepus to the west. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is \"CMa\". The official constellation boundaries, as set by Belgian astronomer Eug\u00e8ne Delporte in 1930, are defined by a quadrilateral; in the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between \u221211.03\u00b0 and \u221233.25\u00b0. Covering 380 square degrees or 0.921% of the sky, it ranks 43rd of the 88 currently-recognized constellations in size.\nFeatures.\nStars.\nCanis Major is a prominent constellation because of its many bright stars. These include Sirius (Alpha Canis Majoris), the brightest star in the night sky, as well as three other stars above magnitude 2.0. Furthermore, two other stars are thought to have previously outshone all others in the night sky\u2014Adhara (Epsilon Canis Majoris) shone at \u22123.99 around 4.7\u00a0million years ago, and Mirzam (Beta Canis Majoris) peaked at \u22123.65 around 4.42\u00a0million years ago. Another, NR Canis Majoris, will be brightest at magnitude \u22120.88 in about 2.87\u00a0million years' time.\nThe German cartographer Johann Bayer used the Greek letters Alpha through Omicron to label the most prominent stars in the constellation, including three adjacent stars as Nu and two further pairs as Xi and Omicron, while subsequent observers designated further stars in the southern parts of the constellation that were hard to discern from Central Europe. Bayer's countryman Johann Elert Bode later added Sigma, Tau and Omega; the French astronomer Nicolas Louis de Lacaille added lettered stars a to k (though none are in use today). John Flamsteed numbered 31 stars, with 3 Canis Majoris being placed by Lacaille into Columba as Delta Columbae (Flamsteed had not recognised Columba as a distinct constellation). He also labelled two stars\u2014his 10 and 13 Canis Majoris\u2014as Kappa1 and Kappa2 respectively, but subsequent cartographers such as Francis Baily and John Bevis dropped the fainter former star, leaving Kappa2 as the sole Kappa. Flamsteed's listing of Nu1, Nu2, Nu3, Xi1, Xi2, Omicron1 and Omicron2 have all remained in use.\nSirius is the brightest star in the night sky at apparent magnitude \u22121.46 and one of the closest stars to Earth at a distance of 8.6 light-years. Its name comes from the Greek word for \"scorching\" or \"searing\". Sirius is also a binary star; its companion Sirius B is a white dwarf with a magnitude of 8.4\u201310,000 times fainter than Sirius A to observers on Earth. The two orbit each other every 50 years. Their closest approach last occurred in 1993 and they will be at their greatest separation between 2020 and 2025. Sirius was the basis for the ancient Egyptian calendar. The star marked the Great Dog's mouth on Bayer's star atlas.\nFlanking Sirius are Beta and Gamma Canis Majoris. Also called Mirzam or Murzim, Beta is a blue-white Beta Cephei variable star of magnitude 2.0, which varies by a few hundredths of a magnitude over a period of six hours. Mirzam is 500 light-years from Earth, and its traditional name means \"the announcer\", referring to its position as the \"announcer\" of Sirius, as it rises a few minutes before Sirius does. Gamma, also known as Muliphein, is a fainter star of magnitude 4.12, in reality a blue-white bright giant of spectral type B8IIe located 441 light-years from earth. Iota Canis Majoris, lying between Sirius and Gamma, is another star that has been classified as a Beta Cephei variable, varying from magnitude 4.36 to 4.40 over a period of 1.92 hours. It is a remote blue-white supergiant star of spectral type B3Ib, around 46,000 times as luminous as the sun and, at 2500 light-years distant, 300 times further away than Sirius.\nEpsilon, Omicron2, Delta, and Eta Canis Majoris were called \"Al Adzari\" \"the virgins\" in medieval Arabic tradition. Marking the dog's right thigh on Bayer's atlas is Epsilon Canis Majoris, also known as Adhara. At magnitude 1.5, it is the second-brightest star in Canis Major and the 23rd-brightest star in the sky. It is a blue-white supergiant of spectral type B2Iab, around 404 light-years from Earth. This star is one of the brightest known extreme ultraviolet sources in the sky. It is a binary star; the secondary is of magnitude 7.4. Its traditional name means \"the virgins\", having been transferred from the group of stars to Epsilon alone. Nearby is Delta Canis Majoris, also called Wezen. It is a yellow-white supergiant of spectral type F8Iab and magnitude 1.84, around 1605 light-years from Earth. With a traditional name meaning \"the weight\", Wezen is 17 times as massive and 50,000 times as luminous as the Sun. If located in the centre of the Solar System, it would extend out to Earth as its diameter is 200 times that of the Sun. Only around 10\u00a0million years old, Wezen has stopped fusing hydrogen in its core. Its outer envelope is beginning to expand and cool, and in the next 100,000 years it will become a red supergiant as its core fuses heavier and heavier elements. Once it has a core of iron, it will collapse and explode as a supernova. Nestled between Adhara and Wezen lies Sigma Canis Majoris, known as Unurgunite to the Boorong and Wotjobaluk people, a red supergiant of spectral type K7Ib that varies irregularly between magnitudes 3.43 and 3.51.\nAlso called Aludra, Eta Canis Majoris is a blue-white supergiant of spectral type B5Ia with a luminosity 176,000 times and diameter around 80 times that of the Sun. Classified as an Alpha Cygni type variable star, Aludra varies in brightness from magnitude 2.38 to 2.48 over a period of 4.7 days. It is located 1120 light-years away. To the west of Adhara lies 3.0-magnitude Zeta Canis Majoris or Furud, around 362 light-years distant from Earth. It is a spectroscopic binary, whose components orbit each other every 1.85 years, the combined spectrum indicating a main star of spectral type B2.5V.\nBetween these stars and Sirius lie Omicron1, Omicron2, and Pi Canis Majoris. Omicron2 is a massive supergiant star about 21 times as massive as the Sun. Only 7\u00a0million years old, it has exhausted the supply of hydrogen at its core and is now processing helium. It is an Alpha Cygni variable that undergoes periodic non-radial pulsations, which cause its brightness to cycle from magnitude 2.93 to 3.08 over a 24.44-day interval. Omicron1 is an orange K-type supergiant of spectral type K2.5Iab that is an irregular variable star, varying between apparent magnitudes 3.78 and 3.99. Around 18 times as massive as the Sun, it shines with 65,000 times its luminosity.\nNorth of Sirius lie Theta and Mu Canis Majoris, Theta being the most northerly star with a Bayer designation in the constellation. Around 8\u00a0billion years old, it is an orange giant of spectral type K4III that is around as massive as the Sun but has expanded to 30 times the Sun's diameter. Mu is a multiple star system located around 1244 light-years distant, its components discernible in a small telescope as a 5.3-magnitude yellow-hued and 7.1-magnitude bluish star. The brighter star is a giant of spectral type K2III, while the companion is a main sequence star of spectral type B9.5V. Nu Canis Majoris is a yellow-hued giant star of magnitude 5.7, 278 light-years away; it is at the threshold of naked-eye visibility. It has a companion of magnitude 8.1.\nAt the southern limits of the constellation lie Kappa and Lambda Canis Majoris. Although of similar spectra and nearby each other as viewed from Earth, they are unrelated. Kappa is a Gamma Cassiopeiae variable of spectral type B2Vne, which brightened by 50% between 1963 and 1978, from magnitude 3.96 or so to 3.52. It is around 659 light-years distant. Lambda is a blue-white B-type main sequence dwarf with an apparent magnitude of 4.48 located around 423 light-years from Earth. It is 3.7 times as wide as and 5.5 times as massive as the Sun, and shines with 940 times its luminosity.\nCanis Major is also home to many variable stars. EZ Canis Majoris is a Wolf\u2013Rayet star of spectral type WN4 that varies between magnitudes 6.71 and 6.95 over a period of 3.766 days; the cause of its variability is unknown but thought to be related to its stellar wind and rotation. VY Canis Majoris is a remote red hypergiant located approximately 3,800 light-years away from Earth. It is one of largest stars known (sometimes described as the largest known) and is also one of most luminous with a radius varying from 1,420 to 2,200 times the Sun's radius, and a luminosity around 300,000 times greater than the Sun. Its current mass is about 17 \u00b1 8 solar masses, having shed material from an initial mass of 25\u201332 solar masses. VY CMa is also surrounded by a red reflection nebula that has been made by the material expelled by the strong stellar winds of its central star. W Canis Majoris is a type of red giant known as a carbon star\u2014a semiregular variable, it ranges between magnitudes 6.27 and 7.09 over a period of 160 days. A cool star, it has a surface temperature of around 2,900 K and a radius 234 times that of the Sun, its distance estimated at 1,444\u20131,450 light-years from Earth. At the other extreme in size is RX J0720.4-3125, a neutron star with a radius of around 5\u00a0km. Exceedingly faint, it has an apparent magnitude of 26.6. Its spectrum and temperature appear to be mysteriously changing over several years. The nature of the changes are unclear, but it is possible they were caused by an event such as the star's absorption of an accretion disc.\nTau Canis Majoris is a Beta Lyrae-type eclipsing multiple star system that varies from magnitude 4.32 to 4.37 over 1.28 days. Its four main component stars are hot O-type stars, with a combined mass 80 times that of the Sun and shining with 500,000 times its luminosity, but little is known of their individual properties. A fifth component, a magnitude 10 star, lies at a distance of . The system is only 5\u00a0million years old. UW Canis Majoris is another Beta Lyrae-type star 3000 light-years from Earth; it is an eclipsing binary that ranges in magnitude from a minimum of 5.3 to a maximum of 4.8. It has a period of 4.4 days; its components are two massive hot blue stars, one a blue supergiant of spectral type O7.5\u20138 Iab, while its companion is a slightly cooler, less evolved and less luminous supergiant of spectral type O9.7Ib. The stars are 200,000 and 63,000 times as luminous as the Sun. However the fainter star is the more massive at 19 solar masses to the primary's 16. R Canis Majoris is another eclipsing binary that varies from magnitude 5.7 to 6.34 over 1.13 days, with a third star orbiting these two every 93 years. The shortness of the orbital period and the low ratio between the two main components make this an unusual Algol-type system.\nSeven star systems have been found to have planets. Nu2 Canis Majoris is an ageing orange giant of spectral type K1III of apparent magnitude 3.91 located around 64 light-years distant. Around 1.5 times as massive and 11 times as luminous as the Sun, it is orbited over a period of 763 days by a planet 2.6 times as massive as Jupiter. HD 47536 is likewise an ageing orange giant found to have a planetary system\u2014echoing the fate of the Solar System in a few billion years as the Sun ages and becomes a giant. Conversely, HD 45364 is a star 107 light-years distant that is a little smaller and cooler than the Sun, of spectral type G8V, which has two planets discovered in 2008. With orbital periods of 228 and 342 days, the planets have a 3:2 orbital resonance, which helps stabilise the system. HD 47186 is another sunlike star with two planets; the inner\u2014HD 47186 b\u2014takes four days to complete an orbit and has been classified as a Hot Neptune, while the outer\u2014HD 47186 c\u2014has an eccentric 3.7-year period orbit and has a similar mass to Saturn. HD 43197 is a sunlike star around 183 light-years distant that has a Jupiter-size planet with an eccentric orbit.\nZ Canis Majoris is a star system a mere 300,000 years old composed of two pre-main-sequence stars\u2014a FU Orionis star and a Herbig Ae/Be star, which has brightened episodically by two magnitudes to magnitude 8 in 1987, 2000, 2004 and 2008. The more massive Herbig Ae/Be star is enveloped in an irregular roughly spherical cocoon of dust that has an inner diameter of and outer diameter of . The cocoon has a hole in it through which light shines that covers an angle of 5 to 10 degrees of its circumference. Both stars are surrounded by a large envelope of in-falling material left over from the original cloud that formed the system. Both stars are emitting jets of material, that of the Herbig Ae/Be star being much larger\u201411.7 light-years long. Meanwhile, FS Canis Majoris is another star with infra-red emissions indicating a compact shell of dust, but it appears to be a main-sequence star that has absorbed material from a companion. These stars are thought to be significant contributors to interstellar dust.\nDeep-sky objects.\nThe band of the Milky Way goes through Canis Major, with only patchy obscurement by interstellar dust clouds. It is bright in the northeastern corner of the constellation, as well as in a triangular area between Adhara, Wezen and Aludra, with many stars visible in binoculars. Canis Major boasts several open clusters. The only Messier object is M41 (NGC 2287), an open cluster with a combined visual magnitude of 4.5, around 2300 light-years from Earth. Located 4 degrees south of Sirius, it contains contrasting blue, yellow and orange stars and covers an area the apparent size of the full moon\u2014in reality around 25 light-years in diameter. Its most luminous stars have already evolved into giants. The brightest is a 6.3-magnitude star of spectral type K3. Located in the field is 12 Canis Majoris, though this star is only 670 light-years distant. NGC 2360, known as Caroline's Cluster after its discoverer Caroline Herschel, is an open cluster located 3.5 degrees west of Muliphein and has a combined apparent magnitude of 7.2. Around 15 light-years in diameter, it is located 3700 light-years away from Earth, and has been dated to around 2.2\u00a0billion years old. NGC 2362 is a small, compact open cluster, 5200 light-years from Earth. It contains about 60 stars, of which Tau Canis Majoris is the brightest member. Located around 3 degrees northeast of Wezen, it covers an area around 12 light-years in diameter, though the stars appear huddled around Tau when seen through binoculars. It is a very young open cluster as its member stars are only a few million years old. Lying 2 degrees southwest of NGC 2362 is NGC 2354 a fainter open cluster of magnitude 6.5, with around 15 member stars visible with binoculars. Located around 30' northeast of NGC 2360, NGC 2359 (Thor's Helmet or the Duck Nebula) is a relatively bright emission nebula in Canis Major, with an approximate magnitude of 10, which is 10,000 light-years from Earth. The nebula is shaped by HD 56925, an unstable Wolf\u2013Rayet star embedded within it.\nIn 2003, an overdensity of stars in the region was announced to be the Canis Major Dwarf, the closest satellite galaxy to Earth. However, there remains debate over whether it represents a disrupted dwarf galaxy or in fact a variation in the thin and thick disk and spiral arm populations of the Milky Way. Investigation of the area yielded only ten RR Lyrae variables\u2014consistent with the Milky Way's halo and thick disk populations rather than a separate dwarf spheroidal galaxy. On the other hand, a globular cluster in Puppis, NGC 2298\u2014which appears to be part of the Canis Major dwarf system\u2014is extremely metal-poor, suggesting it did not arise from the Milky Way's thick disk, and instead is of extragalactic origin.\nNGC 2207 and IC 2163 are a pair of face-on interacting spiral galaxies located 125\u00a0million light-years from Earth. About 40\u00a0million years ago, the two galaxies had a close encounter and are now moving farther apart; nevertheless, the smaller IC 2163 will eventually be incorporated into NGC 2207. As the interaction continues, gas and dust will be perturbed, sparking extensive star formation in both galaxies. Supernovae have been observed in NGC 2207 in 1975 (type Ia SN 1975a), 1999 (the type Ib SN 1999ec), 2003 (type 1b supernova SN 2003H), and 2013 (type II supernova SN 2013ai). Located 16\u00a0million light-years distant, ESO 489-056 is an irregular dwarf- and low-surface-brightness galaxy that has one of the lowest metallicities known."}
{"id": "6367", "revid": "3150196", "url": "https://en.wikipedia.org/wiki?curid=6367", "title": "Canis Minor", "text": "Canis Minor is a small constellation in the northern celestial hemisphere. In the second century, it was included as an asterism, or pattern, of two stars in Ptolemy's 48 constellations, and it is counted among the 88 modern constellations. Its name is Latin for \"lesser dog\", in contrast to Canis Major, the \"greater dog\"; both figures are commonly represented as following the constellation of Orion the hunter.\nCanis Minor contains only two stars brighter than the fourth magnitude, Procyon (Alpha Canis Minoris), with a magnitude of 0.34, and Gomeisa (Beta Canis Minoris), with a magnitude of 2.9. The constellation's dimmer stars were noted by Johann Bayer, who named eight stars including Alpha and Beta, and John Flamsteed, who numbered fourteen. Procyon is the seventh-brightest star in the night sky, as well as one of the closest. A yellow-white main sequence star, it has a white dwarf companion. Gomeisa is a blue-white main sequence star. Luyten's Star is a ninth-magnitude red dwarf and the Solar System's next closest stellar neighbour in the constellation after Procyon. The fourth-magnitude HD 66141, which has evolved into an orange giant towards the end of its life cycle, was discovered to have a planet in 2012. There are two faint deep-sky objects within the constellation's borders. The 11 Canis-Minorids are a meteor shower that can be seen in early December.\nHistory and mythology.\nThough strongly associated with the Classical Greek uranographic tradition, Canis Minor originates from ancient Mesopotamia. Procyon and Gomeisa were called \"MASH.TAB.BA\" or \"twins\" in the \"Three Stars Each\" tablets, dating to around 1100 BC. In the later \"MUL.APIN\", this name was also applied to the pairs of Pi3 and Pi4 Orionis and Zeta and Xi Orionis. The meaning of \"MASH.TAB.BA\" evolved as well, becoming the twin deities Lulal and Latarak, who are on the opposite side of the sky from \"Papsukal\", the True Shepherd of Heaven in Babylonian mythology. Canis Minor was also given the name \"DAR.LUGAL\", its position defined as \"the star which stands behind it [Orion]\", in the \"MUL.APIN\"; the constellation represents a rooster. This name may have also referred to the constellation Lepus. \"DAR.LUGAL\" was also denoted \"DAR.MU\u0160EN\" and \"DAR.LUGAL.MU\u0160EN\" in Babylonia. Canis Minor was then called \"tarlugallu\" in Akkadian astronomy.\nCanis Minor was one of the original 48 constellations formulated by Ptolemy in his second-century Almagest, in which it was defined as a specific pattern (asterism) of stars; Ptolemy identified only two stars and hence no depiction was possible. The Ancient Greeks called the constellation \u03c0\u03c1\u03bf\u03ba\u03c5\u03c9\u03bd/\"Procyon\", \"coming before the dog\", transliterated into Latin as \"Antecanis\", \"Praecanis\", or variations thereof, by Cicero and others. Roman writers also appended the descriptors \"parvus\", \"minor\" or \"minusculus\" (\"small\" or \"lesser\", for its faintness), \"septentrionalis\" (\"northerly\", for its position in relation to Canis Major), \"primus\" (rising \"first\") or \"sinister\" (rising to the \"left\") to its name \"Canis\".\nIn Greek mythology, Canis Minor was sometimes connected with the Teumessian Fox, a beast turned into stone with its hunter, Laelaps, by Zeus, who placed them in heaven as Canis Major (Laelaps) and Canis Minor (Teumessian Fox). Eratosthenes accompanied the Little Dog with Orion, while Hyginus linked the constellation with Maera, a dog owned by Icarius of Athens. On discovering the latter's death, the dog and Icarius' daughter Erigone took their lives and all three were placed in the sky\u2014Erigone as Virgo and Icarius as Bo\u00f6tes. As a reward for his faithfulness, the dog was placed along the \"banks\" of the Milky Way, which the ancients believed to be a heavenly river, where he would never suffer from thirst.\nThe medieval Arabic astronomers maintained the depiction of Canis Minor (\"al-Kalb al-Asghar\" in Arabic) as a dog; in his Book of the Fixed Stars, Abd al-Rahman al-Sufi included a diagram of the constellation with a canine figure superimposed. There was one slight difference between the Ptolemaic vision of Canis Minor and the Arabic; al-Sufi claims Mirzam, now assigned to Orion, as part of both Canis Minor\u2014the collar of the dog\u2014and its modern home. The Arabic names for both Procyon and Gomeisa alluded to their proximity and resemblance to Sirius, though they were not direct translations of the Greek; Procyon was called \"ash-Shi'ra ash-Shamiya\", the \"Syrian Sirius\" and Gomeisa was called \"ash-Shira al-Ghamisa\", the Sirius with bleary eyes. Among the Merazig of Tunisia, shepherds note six constellations that mark the passage of the dry, hot season. One of them, called \"Merzem\", includes the stars of Canis Minor and Canis Major and is the herald of two weeks of hot weather.\nThe ancient Egyptians thought of this constellation as Anubis, the jackal god.\nAlternative names have been proposed: Johann Bayer in the early 17th century termed the constellation \"Fovea\" \"The Pit\", and \"Morus\" \"Sycamine Tree\". Seventeenth-century German poet and author Philippus Caesius linked it to the dog of Tobias from the Apocrypha. Richard A. Proctor gave the constellation the name \"Felis\" \"the Cat\" in 1870 (contrasting with Canis Major, which he had abbreviated to \"Canis\" \"the Dog\"), explaining that he sought to shorten the constellation names to make them more manageable on celestial charts. Occasionally, Canis Minor is confused with Canis Major and given the name \"Canis Orionis\" (\"Orion's Dog\").\nIn non-Western astronomy.\nIn Chinese astronomy, the stars corresponding to Canis Minor lie in the Vermilion Bird of the South (\u5357\u65b9\u6731\u96c0, \"N\u00e1n F\u0101ng Zh\u016b Qu\u00e8\"). Procyon, Gomeisa and Eta Canis Minoris form an asterism known as N\u00e1nh\u00e9, the Southern River. With its counterpart, the Northern River Beihe (Castor and Pollux), N\u00e1nh\u00e9 was also associated with a gate or sentry. Along with Zeta and 8 Cancri, 6 Canis Minoris and 11 Canis Minoris formed the asterism \"Shuiwei\", which literally means \"water level\". Combined with additional stars in Gemini, Shuiwei represented an official who managed floodwaters or a marker of the water level. Neighboring Korea recognized four stars in Canis Minor as part of a different constellation, \"the position of the water\". This constellation was located in the Red Bird, the southern portion of the sky.\nPolynesian peoples often did not recognize Canis Minor as a constellation, but they saw Procyon as significant and often named it; in the Tuamotu Archipelago it was known as \"Hiro\", meaning \"twist as a thread of coconut fiber\", and \"Kopu-nui-o-Hiro\" (\"great paunch of Hiro\"), which was either a name for the modern figure of Canis Minor or an alternative name for Procyon. Other names included \"Vena\" (after a goddess), on Mangaia and \"Puanga-hori\" (false \"Puanga\", the name for Rigel), in New Zealand. In the Society Islands, Procyon was called \"Ana-tahua-vahine-o-toa-te-manava\", literally \"Aster the priestess of brave heart\", figuratively the \"pillar for elocution\". The Wardaman people of the Northern Territory in Australia gave Procyon and Gomeisa the names \"Magum\" and \"Gurumana\", describing them as humans who were transformed into gum trees in the dreamtime. Although their skin had turned to bark, they were able to speak with a human voice by rustling their leaves.\nThe Aztec calendar was related to their cosmology. The stars of Canis Minor were incorporated along with some stars of Orion and Gemini into an asterism associated with the day called \"Water\".\nCharacteristics.\nLying directly south of Gemini's bright stars Castor and Pollux, Canis Minor is a small constellation bordered by Monoceros to the south, Gemini to the north, Cancer to the northeast, and Hydra to the east. It does not border Canis Major; Monoceros is in between the two. Covering 183 square degrees, Canis Minor ranks seventy-first of the 88 constellations in size. It appears prominently in the southern sky during the Northern Hemisphere's winter. The constellation boundaries, as set by Belgian astronomer Eug\u00e8ne Delporte in 1930, are defined by a polygon of 14 sides. In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between and . Most visible in the evening sky from January to March, Canis Minor is most prominent at 10 PM during mid-February. It is then seen earlier in the evening until July, when it is only visible after sunset before setting itself, and rising in the morning sky before dawn. The constellation's three-letter abbreviation, as adopted by the International Astronomical Union in 1922, is \"CMi\".\nFeatures.\nStars.\nCanis Minor contains only two stars brighter than fourth magnitude. At magnitude 0.34, Procyon, or Alpha Canis Minoris, is the seventh-brightest star in the night sky, as well as one of the closest. Its name means \"before the dog\" or \"preceding the dog\" in Greek, as it rises an hour before the \"Dog Star\", Sirius, of Canis Major. It is a binary star system, consisting of a yellow-white main sequence star of spectral type F5\u00a0IV-V, named Procyon\u00a0A, and a faint white dwarf companion of spectral type DA, named Procyon\u00a0B. Procyon\u00a0B, which orbits the more massive star every 41 years, is of magnitude 10.7. Procyon\u00a0A is 1.4 times the Sun's mass, while its smaller companion is 0.6 times as massive as the Sun. The system is from Earth, the shortest distance to a northern-hemisphere star of the first magnitude. Gomeisa, or Beta Canis Minoris, with a magnitude of 2.89, is the second-brightest star in Canis Minor. Lying from the Solar System, it is a blue-white main sequence star of spectral class B8\u00a0Ve. Although fainter to Earth observers, it is much brighter than Procyon, and is 250 times as luminous and three times as massive as the Sun. Although its variations are slight, Gomeisa is classified as a shell star (Gamma Cassiopeiae variable), with a maximum magnitude of 2.84 and a minimum magnitude of 2.92. It is surrounded by a disk of gas which it heats and causes to emit radiation.\nJohann Bayer used the Greek letters Alpha to Eta to label the most prominent eight stars in the constellation, designating two stars as Delta (named Delta1 and Delta2). John Flamsteed numbered fourteen stars, discerning a third star he named Delta3; his star 12 Canis Minoris was not found subsequently. In Bayer's 1603 work \"Uranometria\", Procyon is located on the dog's belly, and Gomeisa on its neck. Gamma, Epsilon and Eta Canis Minoris lie nearby, marking the dog's neck, crown and chest respectively. Although it has an apparent magnitude of 4.34, Gamma Canis Minoris is an orange K-type giant of spectral class K3-III C, which lies away. Its colour is obvious when seen through binoculars. It is a multiple system, consisting of the spectroscopic binary Gamma A and three optical companions, Gamma B, magnitude 13; Gamma C, magnitude 12; and Gamma D, magnitude 10. The two components of Gamma A orbit each other every 389.2 days, with an eccentric orbit that takes their separation between 2.3 and 1.4 astronomical units (AU). Epsilon Canis Minoris is a yellow bright giant of spectral class G6.5IIb of magnitude of 4.99. It lies from Earth, with 13 times the diameter and 750 times the luminosity of the Sun. Eta Canis Minoris is a giant of spectral class F0III of magnitude 5.24, which has a yellowish hue when viewed through binoculars as well as a faint companion of magnitude 11.1. Located 4 arcseconds from the primary, the companion star is actually around 440 AU from the main star and takes around 5000 years to orbit it.\nNear Procyon, three stars share the name Delta Canis Minoris. Delta1 is a yellow-white F-type giant of magnitude 5.25 located around from Earth. About 360 times as luminous and 3.75 times as massive as the Sun, it is expanding and cooling as it ages, having spent much of its life as a main sequence star of spectrum B6V. Also known as 8 Canis Minoris, Delta2 is an F-type main-sequence star of spectral type F2V and magnitude 5.59 which is distant. The last of the trio, Delta3 (also known as 9 Canis Minoris), is a white main sequence star of spectral type A0Vnn and magnitude 5.83 which is distant. These stars mark the paws of the Lesser Dog's left hind leg, while magnitude 5.13 Zeta marks the right. A blue-white bright giant of spectral type B8II, Zeta lies around away from the Solar System.\nLying 222 \u00b1 7 light-years away with an apparent magnitude of 4.39, HD 66141 is 6.8\u00a0billion years old and has evolved into an orange giant of spectral type K2III with a diameter around 22 times that of the Sun, and weighing 1.1 solar masses. It is 174 times as luminous as the Sun, with an absolute magnitude of \u22120.15. HD 66141 was mistakenly named 13 Puppis, as its celestial coordinates were recorded incorrectly when catalogued and hence mistakenly thought to be in the constellation of Puppis; Bode gave it the name Lambda Canis Minoris, which is now obsolete. The orange giant is orbited by a planet, HD 66141b, which was detected in 2012 by measuring the star's radial velocity. The planet has a mass around 6 times that of Jupiter and a period of 480 days.\nA red giant of spectral type M4III, BC Canis Minoris lies around distant from the Solar System. It is a semiregular variable star that varies between a maximum magnitude of 6.14 and minimum magnitude of 6.42. Periods of 27.7, 143.3 and 208.3 days have been recorded in its pulsations. AZ, AD and BI Canis Minoris are Delta Scuti variables\u2014short period (six hours at most) pulsating stars that have been used as standard candles and as subjects to study astroseismology. AZ is of spectral type A5IV, and ranges between magnitudes 6.44 and 6.51 over a period of 2.3 hours. AD has a spectral type of F2III, and has a maximum magnitude of 9.21 and minimum of 9.51, with a period of approximately 2.95 hours. BI is of spectral type F2 with an apparent magnitude varying around 9.19 and a period of approximately 2.91 hours.\nAt least three red giants are Mira variables in Canis Minor. S Canis Minoris, of spectral type M7e, is the brightest, ranging from magnitude 6.6 to 13.2 over a period of 332.94 days. V Canis Minoris ranges from magnitude 7.4 to 15.1 over a period of 366.1 days. Similar in magnitude is R Canis Minoris, which has a maximum of 7.3, but a significantly brighter minimum of 11.6. An S-type star, it has a period of 337.8 days.\nYZ Canis Minoris is a red dwarf of spectral type M4.5V and magnitude 11.2, roughly three times the size of Jupiter and from Earth. It is a flare star, emitting unpredictable outbursts of energy for mere minutes, which might be much more powerful analogues of solar flares. Luyten's Star (GJ 273) is a red dwarf star of spectral type M3.5V and close neighbour of the Solar System. Its visual magnitude of 9.9 renders it too faint to be seen with the naked eye, even though it is only away. Fainter still is PSS 544-7, an eighteenth-magnitude red dwarf around 20 percent the mass of the Sun, located from Earth. First noticed in 1991, it is thought to be a cannonball star, shot out of a star cluster and now moving rapidly through space directly away from the galactic disc.\nThe WZ Sagittae-type dwarf nova DY Canis Minoris (also known as VSX J074727.6+065050) flared up to magnitude 11.4 over January and February 2008 before dropping eight magnitudes to around 19.5 over approximately 80 days. It is a remote binary star system where a white dwarf and low-mass star orbit each other close enough for the former star to draw material off the latter and form an accretion disc. This material builds up until it erupts dramatically.\nDeep-sky objects.\nThe Milky Way passes through much of Canis Minor, yet it has few deep-sky objects. William Herschel recorded four objects in his 1786 work \"Catalogue of Nebulae and Clusters of Stars\", including two he mistakenly believed were star clusters. NGC 2459 is a group of five thirteenth- and fourteenth-magnitude stars that appear to lie close together in the sky but are not related. A similar situation has occurred with NGC 2394, also in Canis Minor. This is a collection of fifteen unrelated stars of ninth-magnitude and fainter.\nHerschel also observed three faint galaxies, two of which are interacting with each other. NGC 2508 is a lenticular galaxy of thirteenth-magnitude, estimated at 205\u00a0million light-years (63\u00a0million parsecs) distance with a diameter of 80 thousand light-years (25 thousand parsecs). Named as a single object by Herschel, NGC 2402 is actually a pair of near-adjacent galaxies that appear to be interacting with each other. Only of fourteenth- and fifteenth-magnitudes respectively, the elliptical and spiral galaxy are thought to be approximately 245\u00a0million light-years distant, and each measure 55,000 light-years in diameter.\nMeteor showers.\nThe 11 Canis-Minorids, also called the Beta Canis Minorids, are a meteor shower that arise near the fifth-magnitude star 11 Canis Minoris and were discovered in 1964 by Keith Hindley, who investigated their trajectory and proposed a common origin with the comet D/1917 F1 Mellish. However, this conclusion has been refuted subsequently as the number of orbits analysed was low and their trajectories too disparate to confirm a link. They last from 4 to 15 December, peaking over 10 and 11 December."}
{"id": "6368", "revid": "22041646", "url": "https://en.wikipedia.org/wiki?curid=6368", "title": "Choshu", "text": ""}
{"id": "6371", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=6371", "title": "Centaurus", "text": "Centaurus is a bright constellation in the southern sky. One of the largest constellations, Centaurus was included among the 48 constellations listed by the 2nd-century astronomer Ptolemy, and it remains one of the 88 modern constellations. In Greek mythology, Centaurus represents a centaur; a creature that is half human, half horse (another constellation named after a centaur is one from the zodiac: Sagittarius). Notable stars include Alpha Centauri, the nearest star system to the Solar System, its neighbour in the sky Beta Centauri, and V766 Centauri, one of the largest stars yet discovered. The constellation also contains Omega Centauri, the brightest globular cluster as visible from Earth and the largest identified in the Milky Way, possibly a remnant of a dwarf galaxy.\nNotable features.\nStars.\nCentaurus contains several very bright stars. Its alpha and beta stars are used as \"pointer stars\" to help observers find the constellation Crux. Centaurus has 281 stars above magnitude 6.5, meaning that they are visible to the unaided eye, the most of any constellation. Alpha Centauri, the closest star system to the Sun, has a high proper motion; it will be a mere half-degree from Beta Centauri in approximately 4000 years.\nAlpha Centauri is a triple star system composed of a binary system (Rigil Kentaurus and Toliman) orbited by Proxima Centauri, currently the nearest star to the Sun. Traditionally called Rigil Kentaurus or Toliman, meaning \"foot of the centaur\", the system has an overall magnitude of \u22120.28 and is 4.4 light-years from Earth. The primary and secondary are both yellow-hued stars; the first is of magnitude \u22120.01 and the second: 1.35. Proxima, the tertiary star, is a red dwarf of magnitude 11.0; it appears almost 2 degrees away from the close pairing of Alpha and has a period of approximately one million years. Also a flare star, Proxima has minutes-long outbursts where it brightens by over a magnitude. The Alpha couple revolve in 80-year periodicity and will next appear closest as seen from Earth's telescopes in 2037 and 2038, together as they appear to the naked eye they present the third-brightest \"star\" in the night sky.\nOne other first magnitude star Beta Centauri is in the constellation in a position beyond Proxima and toward the narrow axis of Crux, thus with Alpha forming a far-south limb of the constellation. Also called Hadar and Agena, it is a double star; the primary is a blue-hued giant star of magnitude 0.6, 525 light-years from Earth. The secondary is of magnitude 4.0 and has a modest separation, appearing only under intense magnification due to its distance.\nThe northerly star Theta Centauri, officially named Menkent, is an orange giant star of magnitude 2.06. It is the only bright star of Centaurus that is easily visible from mid-northern latitudes.\nThe next bright object is Gamma Centauri, a binary star which appears to the naked eye at magnitude 2.2. The primary and secondary are both blue-white hued stars of magnitude 2.9; their period is 84 years.\nCentaurus also has many dimmer double stars and binary stars. 3 Centauri is a double star with a blue-white hued primary of magnitude 4.5 and a secondary of magnitude 6.0. The primary is 344 light-years away.\nCentaurus is home to many variable stars. R Centauri is a Mira variable star with a minimum magnitude of 11.8 and a maximum magnitude of 5.3; it is about 1,250 light-years from Earth and has a period of 18 months. V810 Centauri is a semiregular variable.\nBPM 37093 is a white dwarf star whose carbon atoms are thought to have formed a crystalline structure. Since diamond also consists of carbon arranged in a crystalline lattice (though of a different configuration), scientists have nicknamed this star \"Lucy\" after the Beatles song \"\"Lucy in the Sky with Diamonds\".\"\nPDS 70, (V1032 Centauri) a low mass T Tauri star is found in the constellation Centauras. In July 2018 astronomers captured the first conclusive image of a protoplanetary disk containing a nascent exoplanet, named PDS 70b.\nDeep-sky objects.\n\u03c9 Centauri (NGC 5139), despite being listed as the constellation's \"omega\" star, is in fact a naked-eye globular cluster, 17,000 light-years away with a diameter of 150 light-years. It is the largest and brightest globular cluster in the Milky Way; at ten times the size of the next-largest cluster, it has a magnitude of 3.7. It is also the most luminous globular cluster in the Milky Way, at over one million solar luminosities. Omega Centauri is classified as a Shapley class VIII cluster, which means that its center is loosely concentrated. It is also the only globular cluster to be designated with a Bayer letter; the globular cluster 47 Tucanae is the only one designated with a Flamsteed number. It contains several million stars, most of which are yellow dwarf stars, but also possesses red giants and blue-white stars; the stars have an average age of 12 billion years. This has prompted suspicion that Omega Centauri was the core of a dwarf galaxy that had been absorbed by the Milky Way. Omega Centauri was determined to be nonstellar in 1677 by the English astronomer Edmond Halley, though it was visible as a star to the ancients. Its status as a globular cluster was determined by James Dunlop in 1827. To the unaided eye, Omega Centauri appears fuzzy and is obviously non-circular; it is approximately half a degree in diameter, the same size as the full Moon.\nCentaurus is also home to open clusters. NGC 3766 is an open cluster 6,300 light-years from Earth that is visible to the unaided eye. It contains approximately 100 stars, the brightest of which are 7th magnitude. NGC 5460 is another naked-eye open cluster, 2,300 light-years from Earth, that has an overall magnitude of 6 and contains approximately 40 stars.\nThere is one bright planetary nebula in Centaurus, NGC 3918, also known as the Blue Planetary. It has an overall magnitude of 8.0 and a central star of magnitude 11.0; it is 2600 light-years from Earth. The Blue Planetary was discovered by John Herschel and named for its color's similarity to Uranus, though the nebula is apparently three times larger than the planet.\nCentaurus is rich in galaxies as well. NGC 4622 is a face-on spiral galaxy located 200 million light-years from Earth (redshift 0.0146). Its spiral arms wind in both directions, which makes it nearly impossible for astronomers to determine the rotation of the galaxy. Astronomers theorize that a collision with a smaller companion galaxy near the core of the main galaxy could have led to the unusual spiral structure. NGC 5253, a peculiar irregular galaxy, is located near the border with Hydra and M83, with which it likely had a close gravitational interaction 1\u20132 billion years ago. This may have sparked the galaxy's high rate of star formation, which continues today and contributes to its high surface brightness. NGC 5253 includes a large nebula and at least 12 large star clusters. In the eyepiece, it is a small galaxy of magnitude 10 with dimensions of 5 arcminutes by 2 arcminutes and a bright nucleus. NGC 4945 is a spiral galaxy seen edge-on from Earth, 13 million light-years away. It is visible with any amateur telescope, as well as binoculars under good conditions; it has been described as \"shaped like a candle flame\", being long and thin (16' by 3'). In the eyepiece of a large telescope, its southeastern dust lane becomes visible. Another galaxy is NGC 5102, found by star-hopping from Iota Centauri. In the eyepiece, it appears as an elliptical object 9 arcminutes by 2.5 arcminutes tilted on a southwest-northeast axis.\nOne of the closest active galaxies to Earth is the Centaurus A galaxy, NGC 5128, at 11 million light-years away (redshift 0.00183). It has a supermassive black hole at its core, which expels massive jets of matter that emit radio waves due to synchrotron radiation. Astronomers posit that its dust lanes, not common in elliptical galaxies, are due to a previous merger with another galaxy, probably a spiral galaxy. NGC 5128 appears in the optical spectrum as a fairly large elliptical galaxy with a prominent dust lane. Its overall magnitude is 7.0 and it has been seen under perfect conditions with the naked eye, making it one of the most distant objects visible to the unaided observer. In equatorial and southern latitudes, it is easily found by star hopping from Omega Centauri. In small telescopes, the dust lane is not visible; it begins to appear with about 4 inches of aperture under good conditions. In large amateur instruments, above about 12 inches in aperture, the dust lane's west-northwest to east-southeast direction is easily discerned. Another dim dust lane on the east side of the 12-arcminute-by-15-arcminute galaxy is also visible. ESO 270-17, also called the Fourcade-Figueroa Object, is a low-surface brightness object believed to be the remnants of a galaxy; it does not have a core and is very difficult to observe with an amateur telescope. It measures 7 arcminutes by 1 arcminute. It likely originated as a spiral galaxy and underwent a catastrophic gravitational interaction with Centaurus A around 500 million years ago, stopping its rotation and destroying its structure.\nNGC 4650A is a polar-ring galaxy 136 million light-years from Earth (redshift 0.01). It has a central core made of older stars that resembles an elliptical galaxy, and an outer ring of young stars that orbits around the core. The plane of the outer ring is distorted, which suggests that NGC 4650A is the result of a galaxy collision about a billion years ago. This galaxy has also been cited in studies of dark matter, because the stars in the outer ring orbit too quickly for their collective mass. This suggests that the galaxy is surrounded by a dark matter halo, which provides the necessary mass.\nOne of the closest galaxy clusters to Earth is the Centaurus Cluster at 160 million light-years away, having redshift 0.0114. It has a cooler, denser central region of gas and a hotter, more diffuse outer region. The intracluster medium in the Centaurus Cluster has a high concentration of metals (elements heavier than helium) due to a large number of supernovae. This cluster also possesses a plume of gas whose origin is unknown.\nHistory.\nWhile Centaurus now has a high southern latitude, at the dawn of civilization it was an equatorial constellation. Precession has been slowly shifting it southward for millennia, and it is now close to its maximal southern declination. In a little over 7000 years it will be at maximum visibility for those in the northern hemisphere, visible at times in the year up to quite a high northern latitude.\nThe figure of Centaurus can be traced back to a Babylonian constellation known as the Bison-man (MUL.GUD.ALIM). This being was depicted in two major forms: firstly, as a 4-legged bison with a human head, and secondly, as a being with a man's head and torso attached to the rear legs and tail of a bull or bison. It has been closely associated with the Sun god Utu-Shamash from very early times.\nThe Greeks depicted the constellation as a centaur and gave it its current name. It was mentioned by Eudoxus in the 4th century BC and Aratus in the 3rd century BC. In the 2nd century AD, Claudius Ptolemy catalogued 37 stars in Centaurus, including Alpha Centauri. Large as it is now, in earlier times it was even larger, as the constellation Lupus was treated as an asterism within Centaurus, portrayed in illustrations as an unspecified animal either in the centaur's grasp or impaled on its spear. The Southern Cross, which is now regarded as a separate constellation, was treated by the ancients as a mere asterism formed of the stars composing the centaur's legs. Additionally, what is now the minor constellation Circinus was treated as undefined stars under the centaur's front hooves.\nAccording to the Roman poet Ovid (\"Fasti\" v.379), the constellation honors the centaur Chiron, who was tutor to many of the earlier Greek heroes including Heracles (Hercules), Theseus, and Jason, the leader of the Argonauts. It is not to be confused with the more warlike centaur represented by the zodiacal constellation Sagittarius. The legend associated with Chiron says that he was accidentally poisoned with an arrow shot by Hercules, and was subsequently placed in the heavens.\nEquivalents.\nIn Chinese astronomy, the stars of Centaurus are found in three areas: the Azure Dragon of the East (\u6771\u65b9\u9752\u9f8d, \"D\u014dng F\u0101ng Q\u012bng L\u00f3ng\"), the Vermillion Bird of the South (\u5357\u65b9\u6731\u96c0, \"N\u00e1n F\u0101ng Zh\u016b Qu\u00e8\"), and the Southern Asterisms (\u8fd1\u5357\u6975\u661f\u5340, \"J\u00ecnn\u00e1nj\u00edx\u012bng\u014du\"). Not all of the stars of Centaurus can be seen from China, and the unseen stars were classified among the Southern Asterisms by Xu Guangqi, based on his study of western star charts. However, most of the brightest stars of Centaurus, including \u03b1 Centauri, \u03b8 Centauri (or Menkent), \u03b5 Centauri and \u03b7 Centauri, can be seen in the Chinese sky.\nSome Polynesian peoples considered the stars of Centaurus to be a constellation as well. On Pukapuka, Centaurus had two names: \"Na Mata-o-te-tokolua\" and \"Na Lua-mata-o-Wua-ma-Velo\". In Tonga, the constellation was called by four names: \"O-nga-tangata\", \"Tautanga-ufi\", \"Mamangi-Halahu\", and \"Mau-kuo-mau\". Alpha and Beta Centauri were not named specifically by the people of Pukapuka or Tonga, but they were named by the people of Hawaii and the Tuamotus. In Hawaii, the name for Alpha Centauri was either \"Melemele\" or \"Ka Maile-hope\" and the name for Beta Centauri was either \"Polapola\" or \"Ka Maile-mua\". In the Tuamotu islands, Alpha was called \"Na Kuhi\" and Beta was called \"Tere\".\nThe Pointer (\u03b1 Centauri and \u03b2 Centauri) is one of the asterisms used by Bugis sailors for navigation, called \"binto\u00e9ng balu\u00e9\", meaning \"the widowed-before-marriage\". It is also called \"binto\u00e9ng sallatang\" meaning \"southern star\" \nNamesakes.\nTwo United States Navy ships, and , were named after Centaurus, the constellation."}
{"id": "6416", "revid": "1728614", "url": "https://en.wikipedia.org/wiki?curid=6416", "title": "Impact crater", "text": "An impact crater is an approximately circular depression in the surface of a planet, moon, or other solid body in the Solar System or elsewhere, formed by the hypervelocity impact of a smaller body. In contrast to volcanic craters, which result from explosion or internal collapse, impact craters typically have raised rims and floors that are lower in elevation than the surrounding terrain. Impact craters range from small, simple, bowl-shaped depressions to large, complex, multi-ringed impact basins. Meteor Crater is a well-known example of a small impact crater on Earth.\nImpact craters are the dominant geographic features on many solid Solar System objects including the Moon, Mercury, Callisto, Ganymede and most small moons and asteroids. On other planets and moons that experience more active surface geological processes, such as Earth, Venus, Mars, Europa, Io and Titan, visible impact craters are less common because they become eroded, buried or transformed by tectonics over time. Where such processes have destroyed most of the original crater topography, the terms impact structure or astrobleme are more commonly used. In early literature, before the significance of impact cratering was widely recognised, the terms cryptoexplosion or cryptovolcanic structure were often used to describe what are now recognised as impact-related features on Earth.\nThe cratering records of very old surfaces, such as Mercury, the Moon, and the southern highlands of Mars, record a period of intense early bombardment in the inner Solar System around 3.9 billion years ago. The rate of crater production on Earth has since been considerably lower, but it is appreciable nonetheless; Earth experiences from one to three impacts large enough to produce a crater about once every million years on average. This indicates that there should be far more relatively young craters on the planet than have been discovered so far. The cratering rate in the inner solar system fluctuates as a consequence of collisions in the asteroid belt that create a family of fragments that are often sent cascading into the inner solar system. Formed in a collision 80 million years ago, the Baptistina family of asteroids is thought to have caused a large spike in the impact rate. Note that the rate of impact cratering in the outer Solar System could be different from the inner Solar System.\nAlthough Earth's active surface processes quickly destroy the impact record, about 190 terrestrial impact craters have been identified. These range in diameter from a few tens of meters up to about , and they range in age from recent times (e.g. the Sikhote-Alin craters in Russia whose creation was witnessed in 1947) to more than two billion years, though most are less than 500 million years old because geological processes tend to obliterate older craters. They are also selectively found in the stable interior regions of continents. Few undersea craters have been discovered because of the difficulty of surveying the sea floor, the rapid rate of change of the ocean bottom, and the subduction of the ocean floor into Earth's interior by processes of plate tectonics.\nImpact craters are not to be confused with landforms that may appear similar, including calderas, sinkholes, glacial cirques, ring dikes, salt domes, and others.\nHistory.\nDaniel M. Barringer, a mining engineer, was convinced already in 1903 that the crater he owned, Meteor Crater, was of cosmic origin. Yet most geologists at the time assumed it formed as the result of a volcanic steam eruption.\nIn the 1920s, the American geologist Walter H. Bucher studied a number of sites now recognized as impact craters in the United States. He concluded they had been created by some great explosive event, but believed that this force was probably volcanic in origin. However, in 1936, the geologists John D. Boon and Claude C. Albritton Jr. revisited Bucher's studies and concluded that the craters that he studied were probably formed by impacts.\nGrove Karl Gilbert suggested in 1893 that the Moon's craters were formed by large asteroid impacts. Ralph Baldwin in 1949 wrote that the Moon's craters were mostly of impact origin. Around 1960, Gene Shoemaker revived the idea. According to David H. Levy, Gene \"saw the craters on the Moon as logical impact sites that were formed not gradually, in eons, but explosively, in seconds.\" For his Ph.D. degree at Princeton (1960), under the guidance of Harry Hammond Hess, Shoemaker studied the impact dynamics of Barringer Meteor Crater. Shoemaker noted Meteor Crater had the same form and structure as two explosion craters created from atomic bomb tests at the Nevada Test Site, notably Jangle U in 1951 and Teapot Ess in 1955. In 1960, Edward C. T. Chao and Shoemaker identified coesite (a form of silicon dioxide) at Meteor Crater, proving the crater was formed from an impact generating extremely high temperatures and pressures. They followed this discovery with the identification of coesite within suevite at N\u00f6rdlinger Ries, proving its impact origin.\nArmed with the knowledge of shock-metamorphic features, Carlyle S. Beals and colleagues at the Dominion Astrophysical Observatory in Victoria, British Columbia, Canada and Wolf von Engelhardt of the University of T\u00fcbingen in Germany began a methodical search for impact craters. By 1970, they had tentatively identified more than 50. Although their work was controversial, the American Apollo Moon landings, which were in progress at the time, provided supportive evidence by recognizing the rate of impact cratering on the Moon. Because the processes of erosion on the Moon are minimal, craters persist. Since the Earth could be expected to have roughly the same cratering rate as the Moon, it became clear that the Earth had suffered far more impacts than could be seen by counting evident craters.\nCrater formation.\nImpact cratering involves high velocity collisions between solid objects, typically much greater than the speed of sound in those objects. Such hyper-velocity impacts produce physical effects such as melting and vaporization that do not occur in familiar sub-sonic collisions. On Earth, ignoring the slowing effects of travel through the atmosphere, the lowest impact velocity with an object from space is equal to the gravitational escape velocity of about 11\u00a0km/s. The fastest impacts occur at about 72\u00a0km/s in the \"worst case\" scenario in which an object in a retrograde near-parabolic orbit hits Earth. The median impact velocity on Earth is about 20\u00a0km/s.\nHowever, the slowing effects of travel through the atmosphere rapidly decelerate any potential impactor, especially in the lowest 12 kilometres where 90% of the earth's atmospheric mass lies. Meteorites of up to 7,000\u00a0kg lose all their cosmic velocity due to atmospheric drag at a certain altitude (retardation point), and start to accelerate again due to Earth's gravity until the body reaches its terminal velocity of 0.09 to 0.16\u00a0km/s. The larger the meteoroid (i.e. asteroids and comets) the more of its initial cosmic velocity it preserves. While an object of 9,000\u00a0kg maintains about 6% of its original velocity, one of 900,000\u00a0kg already preserves about 70%. Extremely large bodies (about 100,000 tonnes) are not slowed by the atmosphere at all, and impact with their initial cosmic velocity if no prior disintegration occurs.\nImpacts at these high speeds produce shock waves in solid materials, and both impactor and the material impacted are rapidly compressed to high density. Following initial compression, the high-density, over-compressed region rapidly depressurizes, exploding violently, to set in train the sequence of events that produces the impact crater. Impact-crater formation is therefore more closely analogous to cratering by high explosives than by mechanical displacement. Indeed, the energy density of some material involved in the formation of impact craters is many times higher than that generated by high explosives. Since craters are caused by explosions, they are nearly always circular \u2013 only very low-angle impacts cause significantly elliptical craters.\nThis describes impacts on solid surfaces. Impacts on porous surfaces, such as that of Hyperion, may produce internal compression without ejecta, punching a hole in the surface without filling in nearby craters. This may explain the 'sponge-like' appearance of that moon.\nIt is convenient to divide the impact process conceptually into three distinct stages: (1) initial contact and compression, (2) excavation, (3) modification and collapse. In practice, there is overlap between the three processes with, for example, the excavation of the crater continuing in some regions while modification and collapse is already underway in others.\nContact and compression.\nIn the absence of atmosphere, the impact process begins when the impactor first touches the target surface. This contact accelerates the target and decelerates the impactor. Because the impactor is moving so rapidly, the rear of the object moves a significant distance during the short-but-finite time taken for the deceleration to propagate across the impactor. As a result, the impactor is compressed, its density rises, and the pressure within it increases dramatically. Peak pressures in large impacts exceed 1 TPa to reach values more usually found deep in the interiors of planets, or generated artificially in nuclear explosions.\nIn physical terms, a shock wave originates from the point of contact. As this shock wave expands, it decelerates and compresses the impactor, and it accelerates and compresses the target. Stress levels within the shock wave far exceed the strength of solid materials; consequently, both the impactor and the target close to the impact site are irreversibly damaged. Many crystalline minerals can be transformed into higher-density phases by shock waves; for example, the common mineral quartz can be transformed into the higher-pressure forms coesite and stishovite. Many other shock-related changes take place within both impactor and target as the shock wave passes through, and some of these changes can be used as diagnostic tools to determine whether particular geological features were produced by impact cratering.\nAs the shock wave decays, the shocked region decompresses towards more usual pressures and densities. The damage produced by the shock wave raises the temperature of the material. In all but the smallest impacts this increase in temperature is sufficient to melt the impactor, and in larger impacts to vaporize most of it and to melt large volumes of the target. As well as being heated, the target near the impact is accelerated by the shock wave, and it continues moving away from the impact behind the decaying shock wave.\nExcavation.\nContact, compression, decompression, and the passage of the shock wave all occur within a few tenths of a second for a large impact. The subsequent excavation of the crater occurs more slowly, and during this stage the flow of material is largely subsonic. During excavation, the crater grows as the accelerated target material moves away from the point of impact. The target's motion is initially downwards and outwards, but it becomes outwards and upwards. The flow initially produces an approximately hemispherical cavity that continues to grow, eventually producing a paraboloid (bowl-shaped) crater in which the centre has been pushed down, a significant volume of material has been ejected, and a topographically elevated crater rim has been pushed up. When this cavity has reached its maximum size, it is called the transient cavity.\nThe depth of the transient cavity is typically a quarter to a third of its diameter. Ejecta thrown out of the crater do not include material excavated from the full depth of the transient cavity; typically the depth of maximum excavation is only about a third of the total depth. As a result, about one third of the volume of the transient crater is formed by the ejection of material, and the remaining two thirds is formed by the displacement of material downwards, outwards and upwards, to form the elevated rim. For impacts into highly porous materials, a significant crater volume may also be formed by the permanent compaction of the pore space. Such compaction craters may be important on many asteroids, comets and small moons.\nIn large impacts, as well as material displaced and ejected to form the crater, significant volumes of target material may be melted and vaporized together with the original impactor. Some of this impact melt rock may be ejected, but most of it remains within the transient crater, initially forming a layer of impact melt coating the interior of the transient cavity. In contrast, the hot dense vaporized material expands rapidly out of the growing cavity, carrying some solid and molten material within it as it does so. As this hot vapor cloud expands, it rises and cools much like the archetypal mushroom cloud generated by large nuclear explosions. In large impacts, the expanding vapor cloud may rise to many times the scale height of the atmosphere, effectively expanding into free space.\nMost material ejected from the crater is deposited within a few crater radii, but a small fraction may travel large distances at high velocity, and in large impacts it may exceed escape velocity and leave the impacted planet or moon entirely. The majority of the fastest material is ejected from close to the center of impact, and the slowest material is ejected close to the rim at low velocities to form an overturned coherent flap of ejecta immediately outside the rim. As ejecta escapes from the growing crater, it forms an expanding curtain in the shape of an inverted cone. The trajectory of individual particles within the curtain is thought to be largely ballistic.\nSmall volumes of un-melted and relatively un-shocked material may be spalled at very high relative velocities from the surface of the target and from the rear of the impactor. Spalling provides a potential mechanism whereby material may be ejected into inter-planetary space largely undamaged, and whereby small volumes of the impactor may be preserved undamaged even in large impacts. Small volumes of high-speed material may also be generated early in the impact by jetting. This occurs when two surfaces converge rapidly and obliquely at a small angle, and high-temperature highly shocked material is expelled from the convergence zone with velocities that may be several times larger than the impact velocity.\nModification and collapse.\nIn most circumstances, the transient cavity is not stable and collapses under gravity. In small craters, less than about 4\u00a0km diameter on Earth, there is some limited collapse of the crater rim coupled with debris sliding down the crater walls and drainage of impact melts into the deeper cavity. The resultant structure is called a simple crater, and it remains bowl-shaped and superficially similar to the transient crater. In simple craters, the original excavation cavity is overlain by a lens of collapse breccia, ejecta and melt rock, and a portion of the central crater floor may sometimes be flat.\nAbove a certain threshold size, which varies with planetary gravity, the collapse and modification of the transient cavity is much more extensive, and the resulting structure is called a complex crater. The collapse of the transient cavity is driven by gravity, and involves both the uplift of the central region and the inward collapse of the rim. The central uplift is not the result of \"elastic rebound\", which is a process in which a material with elastic strength attempts to return to its original geometry; rather the collapse is a process in which a material with little or no strength attempts to return to a state of gravitational equilibrium.\nComplex craters have uplifted centers, and they have typically broad flat shallow crater floors, and terraced walls. At the largest sizes, one or more exterior or interior rings may appear, and the structure may be labeled an \"impact basin\" rather than an impact crater. Complex-crater morphology on rocky planets appears to follow a regular sequence with increasing size: small complex craters with a central topographic peak are called \"central peak craters\", for example Tycho; intermediate-sized craters, in which the central peak is replaced by a ring of peaks, are called \"peak-ring craters\", for example Schr\u00f6dinger; and the largest craters contain multiple concentric topographic rings, and are called \"multi-ringed basins\", for example Orientale. On icy (as opposed to rocky) bodies, other morphological forms appear that may have central pits rather than central peaks, and at the largest sizes may contain many concentric rings. Valhalla on Callisto is an example of this type.\nIdentifying impact craters.\nNon-explosive volcanic craters can usually be distinguished from impact craters by their irregular shape and the association of volcanic flows and other volcanic materials. Impact craters produce melted rocks as well, but usually in smaller volumes with different characteristics.\nThe distinctive mark of an impact crater is the presence of rock that has undergone shock-metamorphic effects, such as shatter cones, melted rocks, and crystal deformations. The problem is that these materials tend to be deeply buried, at least for simple craters. They tend to be revealed in the uplifted center of a complex crater, however.\nImpacts produce distinctive shock-metamorphic effects that allow impact sites to be distinctively identified. Such shock-metamorphic effects can include:\nEconomic importance of impacts.\nOn Earth impact craters have resulted in useful minerals. Some of the ores produced from impact related effects on Earth include ores of iron, uranium, gold, copper, and nickel. It is estimated that the value of materials mined from impact structures is five billion dollars/year just for North America. \nThe eventual usefulness of impact craters depends on several factors especially the nature of the materials that were impacted and when the materials were affected. In some cases the deposits were already in place and the impact brought them to the surface. These are called \u201cprogenetic economic deposits.\u201d Others were created during the actual impact. The great energy involved caused melting. Useful minerals formed as a result of this energy are classified as \u201csyngenetic deposits.\u201d The third type, called \u201cepigenetic deposits,\u201d is caused by the creation of a basin from the impact.\nMany of the minerals that our modern lives depend on are associated with impacts in the past. The Vredeford Dome in the center of the Witwatersrand Basin is the largest goldfield in the world which has supplied about 40% of all the gold ever mined in an impact structure (though the gold did not come from the bolide). The asteroid that struck the region was wide. The Sudbury Basin was caused by an impacting body over in diameter. This basin is famous for its deposits of nickel, copper, and Platinum Group Elements. An impact was involved in making the Carswell structure in Saskatchewan, Canada; it contains uranium deposits.\nHydrocarbons are common around impact structures. Fifty percent of impact structures in North America in hydrocarbon-bearing sedimentary basins contain oil/gas fields.\nMartian craters.\nBecause of the many missions studying Mars since the 1960s, there is good coverage of its surface which contains large numbers of craters. Many of the craters on Mars differ from those on the Moon and other moons since Mars contains ice under the ground, especially in the higher latitudes. Some of the types of craters that have special shapes due to impact into ice-rich ground are pedestal craters, rampart craters, expanded craters, and LARLE craters.\nLists of craters.\nImpact craters on Earth.\nOn Earth, the recognition of impact craters is a branch of geology, and is related to planetary geology in the study of other worlds. Out of many proposed craters, relatively few are confirmed. The following twenty are a sample of articles of confirmed and well-documented impact sites.\nSee the Earth Impact Database, a website concerned with 190 () scientifically-confirmed impact craters on Earth.\nLargest named craters in the Solar System.\nThere are approximately twelve more impact craters/basins larger than 300\u00a0km on the Moon, five on Mercury, and four on Mars. Large basins, some unnamed but mostly smaller than 300\u00a0km, can also be found on Saturn's moons Dione, Rhea and Iapetus."}
{"id": "6417", "revid": "1741963", "url": "https://en.wikipedia.org/wiki?curid=6417", "title": "Corvus (disambiguation)", "text": "Corvus is a genus of birds including species commonly known as crows, ravens, rooks and jackdaws.\nCorvus may also refer to:"}
{"id": "6420", "revid": "18404174", "url": "https://en.wikipedia.org/wiki?curid=6420", "title": "Corona Borealis", "text": "Corona Borealis is a small constellation in the Northern Celestial Hemisphere. It is one of the 48\u00a0constellations listed by the 2nd-century astronomer Ptolemy, and remains one of the 88 modern constellations. Its brightest stars form a semicircular arc. Its Latin name, inspired by its shape, means \"northern crown\". In classical mythology Corona Borealis generally represented the crown given by the god Dionysus to the Cretan princess Ariadne and set by her in the heavens. Other cultures likened the pattern to a circle of elders, an eagle's nest, a bear's den, or even a smokehole. Ptolemy also listed a southern counterpart, Corona Australis, with a similar pattern.\nThe brightest star is the magnitude\u00a02.2 Alpha Coronae Borealis. The yellow supergiant R Coronae Borealis is the prototype of a rare class of giant stars\u2014the R Coronae Borealis variables\u2014that are extremely hydrogen deficient, and thought to result from the merger of two white dwarfs. T Coronae Borealis, also known as the Blaze Star, is another unusual type of variable star known as a recurrent nova. Normally of magnitude\u00a010, it last flared up to magnitude\u00a02 in 1946. ADS 9731 and Sigma Coronae Borealis are multiple star systems with six and five components respectively. Five star systems have been found to have Jupiter-sized exoplanets. Abell 2065 is a highly concentrated galaxy cluster one billion light-years from the Solar System containing more than 400 members, and is itself part of the larger Corona Borealis Supercluster.\nCharacteristics.\nCovering 179 square degrees and hence 0.433% of the sky, Corona Borealis ranks 73rd of the 88 modern constellations by area. Its position in the Northern Celestial Hemisphere means that the whole constellation is visible to observers north of 50\u00b0S. It is bordered by Bo\u00f6tes to the north and west, Serpens Caput to the south, and Hercules to the east. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is \"CrB\". The official constellation boundaries, as set by Belgian astronomer Eug\u00e8ne Delporte in 1930, are defined by a polygon of eight segments (\"illustrated in infobox\"). In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between 39.71\u00b0 and 25.54\u00b0. It has a counterpart\u2014Corona Australis\u2014in the Southern Celestial Hemisphere.\nFeatures.\nStars.\nThe seven stars that make up the constellation's distinctive crown-shaped pattern are all 4th-magnitude stars except for the brightest of them, Alpha Coronae Borealis. The other six stars are Theta, Beta, Gamma, Delta, Epsilon and Iota Coronae Borealis. The German cartographer Johann Bayer gave twenty stars in Corona Borealis Bayer designations from Alpha to Upsilon in his 1603 star atlas \"Uranometria\". Zeta Coronae Borealis was noted to be a double star by later astronomers and its components designated Zeta1 and Zeta2. John Flamsteed did likewise with Nu Coronae Borealis; classed by Bayer as a single star, it was noted to be two close stars by Flamsteed. He named them 20 and 21 Coronae Borealis in his catalogue, alongside the designations Nu1 and Nu2 respectively. Chinese astronomers deemed nine stars to make up the asterism, adding Pi and Rho Coronae Borealis. Within the constellation's borders, there are 37 stars brighter than or equal to apparent magnitude\u00a06.5.\nAlpha Coronae Borealis (officially named Alphecca by the IAU, but sometimes also known as Gemma) appears as a blue-white star of magnitude\u00a02.2. In fact, it is an Algol-type eclipsing binary that varies by 0.1\u00a0magnitude with a period of 17.4\u00a0days. The primary is a white main-sequence star of spectral type A0V that is 2.91\u00a0times the mass of the Sun () and 57 times as luminous (), and is surrounded by a debris disk out to a radius of around 60\u00a0astronomical units (AU). The secondary companion is a yellow main-sequence star of spectral type G5V that is a little smaller (0.9 times) the diameter of the Sun. Lying 75\u00b10.5\u00a0light-years from Earth, Alphecca is believed to be a member of the Ursa Major Moving Group of stars that have a common motion through space.\nLocated 112\u00b13\u00a0light-years away, Beta Coronae Borealis or Nusakan is a spectroscopic binary system whose two components are separated by 10\u00a0AU and orbit each other every 10.5 years. The brighter component is a rapidly oscillating Ap star, pulsating with a period of 16.2\u00a0minutes. Of spectral type A5V with a surface temperature of around 7980\u00a0K, it has around , 2.6\u00a0solar radii (), and . The smaller star is of spectral type F2V with a surface temperature of around 6750\u00a0K, and has around , , and between 4 and . Near Nusakan is Theta Coronae Borealis, a binary system that shines with a combined magnitude of 4.13 located 380\u00b120 light-years distant. The brighter component, Theta Coronae Borealis A, is a blue-white star that spins extremely rapidly\u2014at a rate of around 393\u00a0km per second. A Be star, it is surrounded by a debris disk.\nFlanking Alpha to the east is Gamma Coronae Borealis, yet another binary star system, whose components orbit each other every 92.94\u00a0years and are roughly as far apart from each other as the Sun and Neptune. The brighter component has been classed as a Delta Scuti variable star, though this view is not universal. The components are main sequence stars of spectral types B9V and A3V. Located 170\u00b12\u00a0light-years away, 4.06-magnitude Delta Coronae Borealis is a yellow giant star of spectral type G3.5III that is around and has swollen to . It has a surface temperature of 5180\u00a0K. For most of its existence, Delta Coronae Borealis was a blue-white main-sequence star of spectral type B before it ran out of hydrogen fuel in its core. Its luminosity and spectrum suggest it has just crossed the Hertzsprung gap, having finished burning core hydrogen and just begun burning hydrogen in a shell that surrounds the core.\nZeta Coronae Borealis is a double star with two blue-white components 6.3\u00a0arcseconds apart that can be readily separated at 100x\u00a0magnification. The primary is of magnitude\u00a05.1 and the secondary is of magnitude\u00a06.0. Nu Coronae Borealis is an optical double, whose components are a similar distance from Earth but have different radial velocities, hence are assumed to be unrelated. The primary, Nu1 Coronae Borealis, is a red giant of spectral type M2III and magnitude 5.2, lying 640\u00b130 light-years distant, and the secondary, Nu2 Coronae Borealis, is an orange-hued giant star of spectral type K5III and magnitude 5.4, estimated to be 590\u00b130\u00a0light-years away. Sigma Coronae Borealis, on the other hand, is a true multiple star system divisible by small amateur telescopes. It is actually a complex system composed of two stars around as massive as the Sun that orbit each other every 1.14\u00a0days, orbited by a third Sun-like star every 726\u00a0years. The fourth and fifth components are a binary red dwarf system that is 14,000\u00a0AU distant from the other three stars. ADS 9731 is an even rarer multiple system in the constellation, composed of six stars, two of which are spectroscopic binaries.\nCorona Borealis is home to two remarkable variable stars. T Coronae Borealis is a cataclysmic variable star also known as the Blaze Star. Normally placid around magnitude 10\u2014it has a minimum of 10.2 and maximum of 9.9\u2014it brightens to magnitude 2 in a period of hours, caused by a nuclear chain reaction and the subsequent explosion. T Coronae Borealis is one of a handful of stars called recurrent novae, which include T Pyxidis and U Scorpii. An outburst of T Coronae Borealis was first recorded in 1866; its second recorded outburst was in February 1946. T Coronae Borealis is a binary star with a red-hued giant primary and a white dwarf secondary, the two stars orbiting each other over a period of approximately 8 months. R Coronae Borealis is a yellow-hued variable supergiant star, over 7000 light-years from Earth, and prototype of a class of stars known as R Coronae Borealis variables. Normally of magnitude 6, its brightness periodically drops as low as magnitude 15 and then slowly increases over the next several months. These declines in magnitude come about as dust that has been ejected from the star obscures it. Direct imaging with the Hubble Space Telescope shows extensive dust clouds out to a radius of around 2000\u00a0AU from the star, corresponding with a stream of fine dust (composed of grains 5\u00a0nm in diameter) associated with the star's stellar wind and coarser dust (composed of grains with a diameter of around 0.14\u00a0\u00b5m) ejected periodically.\nThere are several other variables of reasonable brightness for amateur astronomer to observe, including three Mira-type long period variables: S Coronae Borealis ranges between magnitudes 5.8 and 14.1 over a period of 360\u00a0days. Located around 1946\u00a0light-years distant, it shines with a luminosity 16,643\u00a0times that of the Sun and has a surface temperature of 3033 K. One of the reddest stars in the sky, V Coronae Borealis is a cool star with a surface temperature of 2877\u00a0K that shines with a luminosity 102,831 times that of the Sun and is a remote 8810\u00a0light-years distant from Earth. Varying between magnitudes 6.9 and 12.6 over a period of 357\u00a0days, it is located near the junction of the border of Corona Borealis with Hercules and Bootes. Located 1.5\u00b0 northeast of Tau Coronae Borealis, W Coronae Borealis ranges between magnitudes 7.8 and 14.3 over a period of 238\u00a0days. Another red giant, RR Coronae Borealis is a M3-type semiregular variable star that varies between magnitudes 7.3 and 8.2 over 60.8 days. RS Coronae Borealis is yet another semiregular variable red giant, which ranges between magnitudes 8.7 to 11.6 over 332 days. It is unusual in that it is a red star with a high proper motion (greater than 50 milliarcseconds a year). Meanwhile, U Coronae Borealis is an Algol-type eclipsing binary star system whose magnitude varies between 7.66 and 8.79 over a period of 3.45 days\nTY Coronae Borealis is a pulsating white dwarf (of ZZ Ceti) type, which is around 70% as massive as the Sun, yet has only 1.1% of its diameter. Discovered in 1990, UW Coronae Borealis is a low-mass X-ray binary system composed of a star less massive than the Sun and a neutron star surrounded by an accretion disk that draws material from the companion star. It varies in brightness in an unusually complex manner: the two stars orbit each other every 111 minutes, yet there is another cycle of 112.6 minutes, which corresponds to the orbit of the disk around the degenerate star. The beat period of 5.5 days indicates the time the accretion disk\u2014which is asymmetrical\u2014takes to precess around the star.\nExtrasolar planetary systems.\nExtrasolar planets have been confirmed in five star systems, four of which were found by the radial velocity method. The spectrum of Epsilon Coronae Borealis was analysed for seven years from 2005 to 2012, revealing a planet around 6.7\u00a0times as massive as Jupiter () orbiting every 418\u00a0days at an average distance of around 1.3\u00a0AU. Epsilon itself is a orange giant of spectral type K2III that has swollen to and . Kappa Coronae Borealis is a spectral type K1IV orange subgiant nearly twice as massive as the Sun; around it lie a dust debris disk, and one planet with a period of 3.4\u00a0years. This planet's mass is estimated at . The dimensions of the debris disk indicate it is likely there is a second substellar companion. Omicron Coronae Borealis is a K-type clump giant with one confirmed planet with a mass of that orbits every 187\u00a0days\u2014one of the two least massive planets known around clump giants. HD 145457 is an orange giant of spectral type K0III found to have one planet of . Discovered by the Doppler method in 2010, it takes 176\u00a0days to complete an orbit. XO-1 is a magnitude 11 yellow main-sequence star located approximately light-years away, of spectral type G1V with a mass and radius similar to the Sun. In 2006 the hot Jupiter exoplanet XO-1b was discovered orbiting XO-1 by the transit method using the XO Telescope. Roughly the size of Jupiter, it completes an orbit around its star every three days.\nThe discovery of a Jupiter-sized planetary companion was announced in 1997 via analysis of the radial velocity of Rho Coronae Borealis, a yellow main sequence star and Solar analog of spectral type G0V, around 57 light-years distant from Earth. More accurate measurement of data from the Hipparcos satellite subsequently showed it instead to be a low-mass star somewhere between 100 and 200 times the mass of Jupiter. Possible stable planetary orbits in the habitable zone were calculated for the binary star Eta Coronae Borealis, which is composed of two stars\u2014yellow main sequence stars of spectral type G1V and G3V respectively\u2014similar in mass and spectrum to the Sun. No planet has been found, but a brown dwarf companion about 63 times as massive as Jupiter with a spectral type of L8 was discovered at a distance of 3640\u00a0AU from the pair in 2001.\nDeep-sky objects.\nCorona Borealis contains few galaxies observable with amateur telescopes. NGC 6085 and 6086 are a faint spiral and elliptical galaxy respectively close enough to each other to be seen in the same visual field through a telescope. Abell 2142 is a huge (six million light-year diameter), X-ray luminous galaxy cluster that is the result of an ongoing merger between two galaxy clusters. It has a redshift of 0.0909 (meaning it is moving away from us at 27,250\u00a0km/s) and a visual magnitude of 16.0. It is about 1.2 billion light-years away. Another galaxy cluster in the constellation, RX\u00a0J1532.9+3021, is approximately 3.9 billion light-years from Earth. At the cluster's center is a large elliptical galaxy containing one of the most massive and most powerful supermassive black holes yet discovered. Abell 2065 is a highly concentrated galaxy cluster containing more than 400 members, the brightest of which are 16th magnitude; the cluster is more than one billion light-years from Earth. On a larger scale still, Abell\u00a02065, along with Abell 2061, Abell 2067, Abell 2079, Abell 2089, and Abell 2092, make up the Corona Borealis Supercluster. Another galaxy cluster, Abell 2162, is a member of the Hercules Superclusters.\nMythology.\nIn Greek mythology, Corona Borealis was linked to the legend of Theseus and the minotaur. It was generally considered to represent a crown given by Dionysus to Ariadne, the daughter of Minos of Crete, after she had been abandoned by the Athenian prince Theseus. When she wore the crown at her marriage to Dionysus, he placed it in the heavens to commemorate their wedding. An alternate version has the besotted Dionysus give the crown to Ariadne, who in turn gives it to Theseus after he arrives in Crete to kill the minotaur that the Cretans have demanded tribute from Athens to feed. The hero uses the crown's light to escape the labyrinth after disposing of the creature, and Dionysus later sets it in the heavens. The Latin author Hyginus linked it to a crown or wreath worn by Bacchus (Dionysus) to disguise his appearance when first approaching Mount Olympus and revealing himself to the gods, having been previously hidden as yet another child of Jupiter's trysts with a mortal, in this case Semele. Corona Borealis was one of the 48 constellations mentioned in the \"Almagest\" of classical astronomer Ptolemy.\nIn Welsh mythology, it was called Caer Arianrhod, \"the Castle of the Silver Circle\", and was the heavenly abode of the Lady Arianrhod. To the ancient Balts, Corona Borealis was known as \"Dar\u017eelis\", the \"flower garden.\"\nThe Arabs called the constellation Alphecca (a name later given to Alpha Coronae Borealis), which means \"separated\" or \"broken up\" ( '), a reference to the resemblance of the stars of Corona Borealis to a loose string of jewels. This was also interpreted as a broken dish. Among the Bedouins, the constellation was known as ' (), or \"the dish/bowl of the poor people\".\nThe Skidi people of Native Americans saw the stars of Corona Borealis representing a council of stars whose chief was Polaris. The constellation also symbolised the smokehole over a fireplace, which conveyed their messages to the gods, as well as how chiefs should come together to consider matters of importance. The Shawnee people saw the stars as the \"Heavenly Sisters\", who descended from the sky every night to dance on earth. Alphecca signifies the youngest and most comely sister, who was seized by a hunter who transformed into a field mouse to get close to her. They married though she later returned to the sky, with her heartbroken husband and son following later. The Mi'kmaq of eastern Canada saw Corona Borealis as \"Mskegw\u01d2m\", the den of the celestial bear (Alpha, Beta, Gamma and Delta Ursae Majoris).\nPolynesian peoples often recognized Corona Borealis; the people of the Tuamotus named it \"Na Kaua-ki-tokerau\" and probably \"Te Hetu\". The constellation was likely called \"Kaua-mea\" in Hawaii, \"Rangawhenua\" in New Zealand, and \"Te Wale-o-Awitu\" in the Cook Islands atoll of Pukapuka. Its name in Tonga was uncertain; it was either called \"Ao-o-Uvea\" or \"Kau-kupenga\".\nIn Australian Aboriginal astronomy, the constellation is called \"womera\" (\"the boomerang\") due to the shape of the stars. The Wailwun people of northwestern New South Wales saw Corona Borealis as \"mullion wollai\" \"eagle's nest\", with Altair and Vega\u2014each called \"mullion\"\u2014the pair of eagles accompanying it. The Wardaman people of northern Australia held the constellation to be a gathering point for Men's Law, Women's Law and Law of both sexes come together and consider matters of existence.\nLater references.\nCorona Borealis was renamed Corona Firmiana in honour of the Archbishop of Salzburg in the 1730 Atlas \"Mercurii Philosophicii Firmamentum Firminianum Descriptionem\" by Corbinianus Thomas, but this was not taken up by subsequent cartographers. The constellation was featured as a main plot ingredient in the short story \"Hypnos\" by H. P. Lovecraft, published in 1923; it is the object of fear of one of the protagonists in the short story. Finnish band Cadacross released an album titled \"Corona Borealis\" in 2002."}
{"id": "6421", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=6421", "title": "Cygnus (constellation)", "text": "Cygnus is a northern constellation lying on the plane of the Milky Way, deriving its name from the Latinized Greek word for swan. Cygnus is one of the most recognizable constellations of the northern summer and autumn, and it features a prominent asterism known as the Northern Cross (in contrast to the Southern Cross). Cygnus was among the 48 constellations listed by the 2nd century astronomer Ptolemy, and it remains one of the 88 modern constellations.\nCygnus contains Deneb (\u0630\u0646\u0628, translit. \"\u1e0fanab,\" tail)one of the brightest stars in the night sky and the most distant first-magnitude staras its \"tail star\" and one corner of the Summer Triangle. It also has some notable X-ray sources and the giant stellar association of Cygnus OB2. Cygnus is also known as the Northern Cross. One of the stars of this association, NML Cygni, is one of the largest stars currently known. The constellation is also home to Cygnus X-1, a distant X-ray binary containing a supergiant and unseen massive companion that was the first object widely held to be a black hole. Many star systems in Cygnus have known planets as a result of the Kepler Mission observing one patch of the sky, an area around Cygnus.\nMost of the east has part of the Hercules\u2013Corona Borealis Great Wall in the deep sky, a giant galaxy filament that is the largest known structure in the observable universe, covering most of the northern sky.\nHistory and mythology.\nIn Eastern and World Astronomy.\nIn Hinduism, the period of time (or Muhurta) between 4:24 AM to 5:12 AM is called the Brahmamuhurtha, which means \"the moment of the Universe\"; the star system in correlation is the Cygnus constellation. This is believed to be a highly auspicious time to meditate, do any task, or start the day.\nIn Polynesia, Cygnus was often recognized as a separate constellation. In Tonga it was called \"Tuula-lupe\", and in the Tuamotus it was called \"Fanui-tai\". In New Zealand it was called \"Mara-tea\", in the Society Islands it was called \"Pirae-tea\" or \"Taurua-i-te-haapa-raa-manu\", and in the Tuamotus it was called \"Fanui-raro\". Beta Cygni was named in New Zealand; it was likely called \"Whetu-kaupo\". Gamma Cygni was called \"Fanui-runga\" in the Tuamotus.\nDeneb was also often a given name, in the Islamic world of astronomy. The name \"Deneb\" comes from the Arabic name \"dhaneb\", meaning \"tail\", from the phrase \"Dhanab ad-Daj\u0101jah\", which means \u201cthe tail of the hen\u201d.\nIn Western and Greek Astronomy.\nIn Greek mythology, Cygnus has been identified with several different legendary swans. Zeus disguised himself as a swan to seduce Leda, Spartan king Tyndareus's wife, who gave birth to the Gemini, Helen of Troy, and Clytemnestra; Orpheus was transformed into a swan after his murder, and was said to have been placed in the sky next to his lyre (Lyra); and the King Cygnus was transformed into a swan.\nThe Greeks also associated this constellation with the tragic story of Phaethon, the son of Helios the sun god, who demanded to ride his father's sun chariot for a day. Phaethon, however, was unable to control the reins, forcing Zeus to destroy the chariot (and Phaethon) with a thunderbolt, causing it to plummet to the earth into the river Eridanus. According to the myth, Phaethon's close friend or lover, Cygnus, grieved bitterly and spent many days diving into the river to collect Phaethon's bones to give him a proper burial. The gods were so touched by Cygnus's devotion that they turned him into a swan and placed him among the stars.\nIn Ovid's \"Metamorphoses\", there are three people named Cygnus, all of whom are transformed into swans. Alongside Cygnus, noted above, he mentions a boy from Tempe who commits suicide when Phyllius refuses to give him a tamed bull that he demands, but is transformed into a swan and flies away. He also mentions a son of Neptune who is an invulnerable warrior in the Trojan War who is eventually defeated by Achilles, but Neptune saves him by transforming him into a swan.\nTogether with other avian constellations near the summer solstice, Vultur cadens and Aquila, Cygnus may be a significant part of the origin of the myth of the Stymphalian Birds, one of The Twelve Labours of Hercules.\nCharacteristics.\nA very large constellation, Cygnus is bordered by Cepheus to the north and east, Draco to the north and west, Lyra to the west, Vulpecula to the south, Pegasus to the southeast and Lacerta to the east. The three-letter abbreviation for the constellation, as adopted by the IAU in 1922, is \"Cyg\". The official constellation boundaries, as set by Belgian astronomer Eug\u00e8ne Delporte in 1930, are defined as a polygon of 28 segments. In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between 27.73\u00b0 and 61.36\u00b0. Covering 804 square degrees and around 1.9% of the night sky, Cygnus ranks 16th of the 88 constellations in size.\nCygnus culminates at midnight on 29 June, and is most visible in the evening from the early summer to mid-autumn in the Northern Hemisphere.\nNormally, Cygnus is depicted with Delta and Epsilon Cygni as its wings. Deneb, the brightest in the constellation is at its tail, and Albireo as the tip of its beak.\nThere are several asterisms in Cygnus. In the 17th-century German celestial cartographer Johann Bayer's star atlas the \"Uranometria\", Alpha, Beta and Gamma Cygni form the pole of a cross, while Delta and Epsilon form the cross beam. The nova P Cygni was then considered to be the body of Christ.\nFeatures.\nStars.\nBayer catalogued many stars in the constellation, giving them the Bayer designations from Alpha to Omega and then using lowercase Roman letters to g. John Flamsteed added the Roman letters h, i, k, l and m (these stars were considered \"informes\" by Bayer as they lay outside the asterism of Cygnus), but were dropped by Francis Baily.\nThere are several bright stars in Cygnus. Alpha Cygni, called Deneb, is the brightest star in Cygnus. It is a white supergiant star of spectral type A2Iae that varies between magnitudes 1.21 and 1.29, one of the largest and most luminous A-class stars known. It is located about 3200 light-years away. Its traditional name means \"tail\" and refers to its position in the constellation. Albireo, designated Beta Cygni, is a celebrated binary star among amateur astronomers for its contrasting hues. The primary is an orange-hued giant star of magnitude 3.1 and the secondary is a blue-green hued star of magnitude 5.1. The system is 380 light-years away and is visible in large binoculars and all amateur telescopes. Gamma Cygni, traditionally named Sadr, is a yellow-tinged supergiant star of magnitude 2.2, 1500 light-years away. Its traditional name means \"breast\" and refers to its position in the constellation. Delta Cygni (the proper name is Fawaris) is another bright binary star in Cygnus, 171 light-years with a period of 800 years. The primary is a blue-white hued giant star of magnitude 2.9, and the secondary is a star of magnitude 6.6. The two components are visible in a medium-sized amateur telescope. The fifth star in Cygnus above magnitude 3 is Aljanah, designated Epsilon Cygni. It is an orange-hued giant star of magnitude 2.5, 72 light-years from Earth.\nThere are several other dimmer double and binary stars in Cygnus. Mu Cygni is a binary star with an optical tertiary component. The binary system has a period of 790 years and is 73 light-years from Earth. The primary and secondary, both white stars, are of magnitude 4.8 and 6.2, respectively. The unrelated tertiary component is of magnitude 6.9. Though the tertiary component is visible in binoculars, the primary and secondary currently require a medium-sized amateur telescope to split, as they will through the year 2020. The two stars will be closest between 2043 and 2050, when they will require a telescope with larger aperture to split. The stars 30 and 31 Cygni form a contrasting double star similar to the brighter Albireo. The two are visible in binoculars. The primary, 31 Cygni, is an orange-hued star of magnitude 3.8, 1400 light-years from Earth. The secondary, 30 Cygni, appears blue-green. It is of spectral type A5IIIn and magnitude 4.83, and is around 610 light-years from Earth. 31 Cygni itself is a binary star; the tertiary component is a blue star of magnitude 7.0. Psi Cygni is a binary star visible in small amateur telescopes, with two white components. The primary is of magnitude 5.0 and the secondary is of magnitude 7.5. 61 Cygni is a binary star visible in large binoculars or a small amateur telescope. It is 11.4 light-years from Earth and has a period of 750 years. Both components are orange-hued dwarf (main sequence) stars; the primary is of magnitude 5.2 and the secondary is of magnitude 6.1. 61 Cygni is significant because Friedrich Wilhelm Bessel determined its parallax in 1838, the first star to have a known parallax.\nLocated near Eta Cygni is the X-ray source Cygnus X-1, which is now thought to be caused by a black hole accreting matter in a binary star system. This was the first x-ray source widely believed to be a black hole.\nCygnus also contains several other noteworthy X-ray sources. Cygnus X-3 is a microquasar containing a Wolf\u2013Rayet star in orbit around a very compact object, with a period of only 4.8 hours. The system is one of the most intrinsically luminous X-ray sources observed. The system undergoes periodic outbursts of unknown nature, and during one such outburst, the system was found to be emitting muons, likely caused by neutrinos. While the compact object is thought to be a neutron star or possibly a black hole, it is possible that the object is instead a more exotic stellar remnant, possibly the first discovered quark star, hypothesized due to its production of cosmic rays that cannot be explained if the object is a normal neutron star. The system also emits cosmic rays and gamma rays, and has helped shed insight on to the formation of such rays. Cygnus X-2 is another X-ray binary, containing an A-type giant in orbit around a neutron star with a 9.8 day period. The system is interesting due to the rather small mass of the companion star, as most millisecond pulsars have much more massive companions. Another black hole in Cygnus is V404 Cygni, which consists of a K-type star orbiting around a black hole of around 12 solar masses. The black hole, similar to that of Cygnus X-3, has been hypothesized to be a quark star. 4U 2129+ 47 is another X-ray binary containing a neutron star which undergoes outbursts, as is EXO 2030+ 375.\nCygnus is also home to several variable stars. SS Cygni is a dwarf nova which undergoes outbursts every 7\u20138 weeks. The system's total magnitude varies from 12th magnitude at its dimmest to 8th magnitude at its brightest. The two objects in the system are incredibly close together, with an orbital period of less than 0.28 days. Chi Cygni is a red giant and the second-brightest Mira variable star at its maximum. It ranges between magnitudes 3.3 and 14.2, and spectral types S6,2e to S10,4e (MSe) over a period of 408 days; it has a diameter of 300 solar diameters and is 350 light-years from Earth. P Cygni is a luminous blue variable that brightened suddenly to 3rd magnitude in 1600 AD. Since 1715, the star has been of 5th magnitude, despite being more than 5000 light-years from Earth. The star's spectrum is unusual in that it contains very strong emission lines resulting from surrounding nebulosity. W Cygni is a semi-regular variable red giant star, 618 light-years from Earth.It has a maximum magnitude of 5.10 and a minimum magnitude 6.83; its period of 131 days. It is a red giant ranging between spectral types M4e-M6e(Tc:)III, NML Cygni is a red hypergiant semi-regular variable star located at 5,300 light-years away from Earth. It is one of largest stars currently known in the galaxy with a radius exceeding 1,000 solar radii. Its magnitude is around 16.6, its period is about 940 days.\nCygnus contains the binary star system KIC 9832227. It is predicted that the two stars will coalesce in about 2022, briefly forming a new naked-eye object. The star KIC 8462852 (Tabby's Star) has received widespread press coverage because of unusual light fluctuations.\nCygnus is one of the constellations that the Kepler satellite surveyed in its search for extrasolar planets, and as a result, there are about a hundred stars in Cygnus with known planets, the most of any constellation. One of the most notable systems is the Kepler-11 system, containing six transiting planets, all within a plane of approximately one degree. With a spectral type of G6V, the star is somewhat cooler than the Sun. The planets are very close to the star; all but the last planet are closer to Kepler-11 than Mercury is to the Sun, and all the planets are more massive than Earth. The naked-eye star 16 Cygni, a triple star approximately 70 light-years from Earth composed two Sun-like stars and a red dwarf, contains a planet orbiting one of the sun-like stars, found due to variations in the star's radial velocity. Gliese 777, another naked-eye multiple star system containing a yellow star and a red dwarf, also contains a planet. The planet is somewhat similar to Jupiter, but with slightly more mass and a more eccentric orbit. The Kepler-22 system is also notable for having the most Earth-like exoplanet when it was discovered in 2011.\nDeep-sky objects.\nThere is an abundance of deep-sky objects, with many open clusters, nebulae of various types and supernova remnants found in Cygnus due to its position on the Milky Way. Some open clusters can be difficult to make out from a rich background of stars.\nM39 (NGC 7092) is an open cluster 950 light-years from Earth that is visible to the unaided eye under dark skies. It is loose, with about 30 stars arranged over a wide area; their conformation appears triangular. The brightest stars of M39 are of the 7th magnitude. Another open cluster in Cygnus is NGC 6910, also called the Rocking Horse Cluster, possessing 16 stars with a diameter of 5 arcminutes visible in a small amateur instrument; it is of magnitude 7.4. The brightest of these are two gold-hued stars, which represent the bottom of the toy it is named for. A larger amateur instrument reveals 8 more stars, nebulosity to the east and west of the cluster, and a diameter of 9 arcminutes. The nebulosity in this region is part of the Gamma Cygni Nebula. The other stars, approximately 3700 light-years from Earth, are mostly blue-white and very hot.\nOther open clusters in Cygnus include Dolidze 9, Collinder 421, Dolidze 11, and Berkeley 90. Dolidze 9, 2800 light-years from Earth and relatively young at 20 million light-years old, is a faint open cluster with up to 22 stars visible in small and medium-sized amateur telescopes. Nebulosity is visible to the north and east of the cluster, which is 7 arcminutes in diameter. The brightest star appears in the eastern part of the cluster and is of the 7th magnitude; another bright star has a yellow hue. Dolidze 11 is an open cluster 400 million years old, farthest away of the three at 3700 light-years. More than 10 stars are visible in an amateur instrument in this cluster, of similar size to Dolidze 9 at 7 arcminutes in diameter, whose brightest star is of magnitude 7.5. It, too, has nebulosity in the east. Collinder 421 is a particularly old open cluster at an age of approximately 1 billion years; it is of magnitude 10.1. 3100 light-years from Earth, more than 30 stars are visible in a diameter of 8 arcseconds. The prominent star in the north of the cluster has a golden color, whereas the stars in the south of the cluster appear orange. Collinder 421 appears to be embedded in nebulosity, which extends past the cluster's borders to its west. Berkeley 90 is a smaller open cluster, with a diameter of 5 arcminutes. More than 16 members appear in an amateur telescope.\nNGC 6826, the Blinking Planetary Nebula, is a planetary nebula with a magnitude of 8.5, 3200 light-years from Earth. It appears to \"blink\" in the eyepiece of a telescope because its central star is unusually bright (10th magnitude). When an observer focuses on the star, the nebula appears to fade away. Less than one degree from the Blinking Planetary is the double star 16 Cygni.\nThe North America Nebula (NGC 7000) is one of the most well-known nebulae in Cygnus, because it is visible to the unaided eye under dark skies, as a bright patch in the Milky Way. However, its characteristic shape is only visible in long-exposure photographs \u2013 it is difficult to observe in telescopes because of its low surface brightness. It has low surface brightness because it is so large; at its widest, the North America Nebula is 2 degrees across. Illuminated by a hot embedded star of magnitude 6, NGC 7000 is 1500 light-years from Earth.\nTo the south of Epsilon Cygni is the Veil Nebula (NGC 6960, 6962, 6979, 6992, and 6995), a 5,000-year-old supernova remnant covering approximately 3 degrees of the sky - it is over 50 light-years long. Because of its appearance, it is also called the Cygnus Loop. The Loop is only visible in long-exposure astrophotographs. However, the brightest portion, NGC 6992, is faintly visible in binoculars, and a dimmer portion, NGC 6960, is visible in wide-angle telescopes.\nThe DR 6 cluster is also nicknamed the \"Galactic Ghoul\" because of the nebula's resemblance to a human face;\nThe Northern Coalsack Nebula, also called the Cygnus Rift, is a dark nebula located in the Cygnus Milky Way.\nThe Gamma Cygni Nebula (IC 1318) includes both bright and dark nebulae in an area of over 4 degrees. DWB 87 is another of the many bright emission nebulae in Cygnus, 7.8 by 4.3 arcminutes. It is in the Gamma Cygni area. Two other emission nebulae include Sharpless 2-112 and Sharpless 2-115. When viewed in an amateur telescope, Sharpless 2\u2013112 appears to be in a teardrop shape. More of the nebula's eastern portion is visible with an O III (doubly ionized oxygen) filter. There is an orange star of magnitude 10 nearby and a star of magnitude 9 near the nebula's northwest edge. Further to the northwest, there is a dark rift and another bright patch. The whole nebula measures 15 arcminutes in diameter. Sharpless 2\u2013115 is another emission nebula with a complex pattern of light and dark patches. Two pairs of stars appear in the nebula; it is larger near the southwestern pair. The open cluster Berkeley 90 is embedded in this large nebula, which measures 30 by 20 arcminutes.\nAlso of note is the Crescent Nebula (NGC 6888), located between Gamma and Eta Cygni, which was formed by the Wolf\u2013Rayet star HD 192163.\nIn recent years, amateur astronomers have made some notable Cygnus discoveries. The \"Soap bubble nebula\" (PN G75.5+1.7), near the Crescent nebula, was discovered on a digital image by Dave Jurasevich in 2007. In 2011, Austrian amateur Matthias Kronberger discovered a planetary nebula (Kronberger 61, now nicknamed \"The Soccer Ball\") on old survey photos, confirmed recently in images by the Gemini Observatory; both of these are likely too faint to be detected by eye in a small amateur scope.\nBut a much more obscure and relatively 'tiny' object\u2014one which is readily seen in dark skies by amateur telescopes, under good conditions\u2014is the newly discovered nebula (likely reflection type) associated with the star 4 Cygni (HD 183056): an approximately fan-shaped glowing region of several arcminutes' diameter, to the south and west of the fifth-magnitude star. It was first discovered visually near San Jose, California and publicly reported by amateur astronomer Stephen Waldee in 2007, and was confirmed photographically by Al Howard in 2010. California amateur astronomer Dana Patchick also says he detected it on the Palomar Observatory survey photos in 2005 but had not published it for others to confirm and analyze at the time of Waldee's first official notices and later 2010 paper.\nCygnus X is the largest star-forming region in the Solar neighborhood and includes not only some of the brightest and most massive stars known (such as Cygnus OB2-12), but also Cygnus OB2, a massive stellar association classified by some authors as a young globular cluster.\nMore supernovae have been seen in the Fireworks Galaxy (NGC 6946) than in any other galaxy.\nCygnus A is the first radio galaxy discovered; at a distance of 730 million light-years from Earth, it is the closest powerful radio galaxy. In the visible spectrum, it appears as an elliptical galaxy in a small cluster. It is classified as an active galaxy because the supermassive black hole at its nucleus is accreting matter, which produces two jets of matter from the poles. The jets' interaction with the interstellar medium creates radio lobes, one source of radio emissions.\nCygnus is also the apparent source of the WIMP-wind due to the orientation of the solar system's rotation through the galactic halo."}
{"id": "6422", "revid": "44062", "url": "https://en.wikipedia.org/wiki?curid=6422", "title": "Communion", "text": "Communion may refer to:"}
{"id": "6423", "revid": "1019081078", "url": "https://en.wikipedia.org/wiki?curid=6423", "title": "Calorie", "text": "The calorie is a unit of energy defined as the amount of heat needed to raise the temperature of a quantity of water by one degree.\nFor historical reasons, two main definitions of calorie are in wide use. The small calorie or gram calorie (usually denoted cal) is the amount of heat needed to raise the temperature of one \"gram\" of water by one degree Celsius (or one kelvin). The large calorie, food calorie, or kilocalorie (Cal, Calorie or kcal), most widely used in nutrition, is the amount of heat needed to cause the same increase in one \"kilogram\" of water. Thus, 1 kilocalorie (kcal) = 1000 calories (cal). By convention in food science, the large calorie is commonly called Calorie (with a capital C by some authors to distinguish from the smaller unit). In most countries, labels of industrialized food products are required to indicate the nutritional energy value in (kilo or large) calories per serving or per weight.\nCalorie relates directly to the metric system, and therefore to the SI system. It has been regarded as obsolete within the scientific community since the adoption of the SI system, but is still in some use. The SI unit of energy is the joule. One calorie is defined as exactly 4.184 J, and one Calorie (kilocalorie) is 4184 J.\nHistory.\nThe calorie was first introduced by Nicolas Cl\u00e9ment, as a unit of heat energy, in lectures during the years 1819\u20131824. This was the \"large\" calorie, viz. modern kilocalorie. \nThe term entered French and English dictionaries between 1841 and 1867. It comes .\nThe \"small\" calorie (modern calorie) was introduced by Pierre Antoine Favre (Chemist) and Johann T. Silbermann (Physicist) in 1852. \nIn 1879, Marcellin Berthelot distinguished between gram-calorie (modern calorie) and kilogram-calorie (modern kilocalorie). Berthelot also introduced the convention of capitalizing the kilogram-calorie, as \"Calorie\".\nThe use of the kilogram-calorie (kcal) for nutrition was introduced to the American public by Wilbur Olin Atwater, a professor at Wesleyan University, in 1887.\nThe modern calorie (cal) was first recognized as a unit of the cm-g-s system (cgs) in 1896,\nalongside the already-existing cgs unit of energy, the erg (first suggested by Clausius in 1864, under the name \"ergon\", and officially adopted in 1882).\nAlready in 1928 there were serious complaints about the possible confusion arising from the two main definitions of the calorie and whether the notion of using the capital letter to distinguish them was sound.\nUse of the calorie was officially deprecated by the ninth General Conference on Weights and Measures, in 1948.\nThe alternate spelling \"calory\" is archaic.\nDefinitions.\nThe modern (small) calorie is defined as the amount of energy needed to increase the temperature of 1\u00a0gram of water by 1\u00a0\u00b0C (or 1\u00a0K, which is the same increment).\nThe definition depends on the atmospheric pressure and the starting temperature. Accordingly, several different precise definitions of the calorie have been used.\nThe two definitions most common in older literature appear to be the \"15\u00a0\u00b0C calorie\" and the \"thermochemical calorie\". Until 1948, the latter was defined as 4.1833 international joules; the current standard of 4.184\u00a0J was chosen to have the new thermochemical calorie represent the same quantity of energy as before.\nThe calorie was first defined specifically to measure energy in the form of heat, especially in experimental calorimetry.\nNutrition.\nIn a nutritional context, the kilojoule (kJ) is the SI unit of food energy, although the \"calorie\" is commonly used. The word \"calorie\" is commonly used with the number of kilocalories (kcal) of nutritional energy measured.\nIn the United States, most nutritionists prefer the unit kilocalorie to the unit kilojoules, whereas most physiologists prefer to use kilojoules. In the majority of other countries, nutritionists prefer the kilojoule to the kilocalorie. US food labelling laws require the use of kilocalories (under the name of \"Calories\"); kilojoules are permitted to be included on food labels alongside kilocalories, but most food labels do not do so. In Australia, kilojoules are officially preferred over kilocalories, but kilocalories retain some degree of popular use. Australian and New Zealand food labelling laws require the use of kilojoules; kilocalories are allowed to be included on labels in addition to kilojoules, but are not required. EU food labelling laws require both kilojoules and kilocalories on all nutritional labels, with the kilojoules listed first.\nTo facilitate comparison, specific energy or energy density figures are often quoted as \"calories per serving\" or \"kcal per 100\u00a0g\". A nutritional requirement or consumption is often expressed in calories or kilocalories per day. \nFood nutrients as fat (lipids) contains 9 kilocalories per gram (kcal/g), while carbohydrate (sugar) or protein contains approximately 4 kcal/g. Alcohol in food contains 7 kcal/g. Food nutrients are also often quoted \"per 100 g\".\nChemistry.\nIn other scientific contexts, the term \"calorie\" almost always refers to the small calorie. Even though it is not an SI unit, it is still used in chemistry. For example, the energy released in a chemical reaction per mole of reagent is occasionally expressed in kilocalories per mole. Typically, this use was largely due to the ease with which it could be calculated in laboratory reactions, especially in aqueous solution: a volume of reagent dissolved in water forming a solution, with concentration expressed in moles per litre (1 litre weighing 1\u00a0kilogram), will induce a temperature change in degrees Celsius in the total volume of water solvent, and these quantities (volume, molar concentration and temperature change) can then be used to calculate energy per mole. It is also occasionally used to specify energy quantities that relate to reaction energy, such as enthalpy of formation and the size of activation barriers. However, its use is being superseded by the SI unit, the joule, and multiples thereof such as the kilojoule.\nMeasurement of energy content of food.\nIn the past, a bomb calorimeter was used to determine the energy content of food by burning a sample and measuring a temperature change in the surrounding water. Today, this method is not commonly used in the United States and has been replaced by calculating the energy content indirectly from adding up the energy provided by energy-containing nutrients of food (such as protein, carbohydrates, and fats), the Modified Atwater system. The fibre content is also subtracted to account for the fact that fibre is not digested by the body."}
{"id": "6424", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=6424", "title": "Corona Australis", "text": "Corona Australis is a constellation in the Southern Celestial Hemisphere. Its Latin name means \"southern crown\", and it is the southern counterpart of Corona Borealis, the northern crown. It is one of the 48 constellations listed by the 2nd-century astronomer Ptolemy, and it remains one of the 88 modern constellations. The Ancient Greeks saw Corona Australis as a wreath rather than a crown and associated it with Sagittarius or Centaurus. Other cultures have likened the pattern to a turtle, ostrich nest, a tent, or even a hut belonging to a rock hyrax.\nAlthough fainter than its northern counterpart, the oval- or horseshoe-shaped pattern of its brighter stars renders it distinctive. Alpha and Beta Coronae Australis are the two brightest stars with an apparent magnitude of around 4.1. Epsilon Coronae Australis is the brightest example of a W Ursae Majoris variable in the southern sky. Lying alongside the Milky Way, Corona Australis contains one of the closest star-forming regions to the Solar System\u2014a dusty dark nebula known as the Corona Australis Molecular Cloud, lying about 430 light years away. Within it are stars at the earliest stages of their lifespan. The variable stars R and TY Coronae Australis light up parts of the nebula, which varies in brightness accordingly.\nName.\nThe name of the constellation was entered as \"Corona Australis\" when the International Astronomical Union (IAU) established the 88 modern constellations in 1922.\nIn 1932, the name was instead recorded as \"Corona Austrina\" when the IAU's commission on notation approved a list of four-letter abbreviations for the constellations.\nThe four-letter abbreviations were repealed in 1955. The IAU presently uses \"Corona Australis\" exclusively.\nCharacteristics.\nCorona Australis is a small constellation bordered by Sagittarius to the north, Scorpius to the west, Telescopium to the south, and Ara to the southwest. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is \"CrA\". The official constellation boundaries, as set by Belgian astronomer Eug\u00e8ne Delporte in 1930, are defined by a polygon of four segments (\"illustrated in infobox\"). In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between \u221236.77\u00b0 and \u221245.52\u00b0. Covering 128 square degrees, Corona Australis culminates at midnight around the 30th of June and ranks 80th in area. Only visible at latitudes south of 53\u00b0 north, Corona Australis cannot be seen from the British Isles as it lies too far south, but it can be seen from southern Europe and readily from the southern United States.\nFeatures.\nWhile not a bright constellation, Corona Australis is nonetheless distinctive due to its easily identifiable pattern of stars, which has been described as horseshoe- or oval-shaped. Though it has no stars brighter than 4th magnitude, it still has 21 stars visible to the unaided eye (brighter than magnitude 5.5). Nicolas Louis de Lacaille used the Greek letters Alpha through to Lambda to label the most prominent eleven stars in the constellation, designating two stars as Eta and omitting Iota altogether. Mu Coronae Australis, a yellow star of spectral type G5.5III and apparent magnitude 5.21, was labelled by Johann Elert Bode and retained by Benjamin Gould, who deemed it bright enough to warrant naming.\nStars.\nThe only star in the constellation to have received a name is Alfecca Meridiana or Alpha CrA. The name combines the Arabic name of the constellation with the Latin for \"southern\". In Arabic, \"Alfecca\" means \"break\", and refers to the shape of both Corona Australis and Corona Borealis. Also called simply \"Meridiana\", it is a white main sequence star located 125 light years away from Earth, with an apparent magnitude of 4.10 and spectral type A2Va. A rapidly rotating star, it spins at almost 200\u00a0km per second at its equator, making a complete revolution in around 14 hours. Like the star Vega, it has excess infrared radiation, which indicates it may be ringed by a disk of dust. It is currently a main-sequence star, but will eventually evolve into a white dwarf; currently, it has a luminosity 31 times greater, and a radius and mass of 2.3 times that of the Sun. Beta Coronae Australis is an orange giant 474 light years from Earth. Its spectral type is K0II, and it is of apparent magnitude 4.11. Since its formation, it has evolved from a B-type star to a K-type star. Its luminosity class places it as a bright giant; its luminosity is 730 times that of the Sun, designating it one of the highest-luminosity K0-type stars visible to the naked eye. 100 million years old, it has a radius of 43 solar radii () and a mass of between 4.5 and 5 solar masses (). Alpha and Beta are so similar as to be indistinguishable in brightness to the naked eye.\nSome of the more prominent double stars include Gamma Coronae Australis\u2014a pair of yellowish white stars 58 light years away from Earth, which orbit each other every 122 years. Widening since 1990, the two stars can be seen as separate with a 100\u00a0mm aperture telescope; they are separated by 1.3 arcseconds at an angle of 61 degrees. They have a combined visual magnitude of 4.2; each component is an F8V dwarf star with a magnitude of 5.01. Epsilon Coronae Australis is an eclipsing binary belonging to a class of stars known as W Ursae Majoris variables. These star systems are known as contact binaries as the component stars are so close together they touch. Varying by a quarter of a magnitude around an average apparent magnitude of 4.83 every seven hours, the star system lies 98 light years away. Its spectral type is F4VFe-0.8+. At the southern end of the crown asterism are the stars Eta\u00b9 and Eta\u00b2 Coronae Australis, which form an optical double. Of magnitude 5.1 and 5.5, they are separable with the naked eye and are both white. Kappa Coronae Australis is an easily resolved optical double\u2014the components are of apparent magnitudes 6.3 and 5.6 and are about 1000 and 150 light years away respectively. They appear at an angle of 359 degrees, separated by 21.6 arcseconds. Kappa\u00b2 is actually the brighter of the pair and is more bluish white, with a spectral type of B9V, while Kappa\u00b9 is of spectral type A0III. Lying 202 light years away, Lambda Coronae Australis is a double splittable in small telescopes. The primary is a white star of spectral type A2Vn and magnitude of 5.1, while the companion star has a magnitude of 9.7. The two components are separated by 29.2 arcseconds at an angle of 214 degrees.\nZeta Coronae Australis is a rapidly rotating main sequence star with an apparent magnitude of 4.8, 221.7 light years from Earth. The star has blurred lines in its hydrogen spectrum due to its rotation. Its spectral type is B9V. Theta Coronae Australis lies further to the west, a yellow giant of spectral type G8III and apparent magnitude 4.62. Corona Australis harbours RX J1856.5-3754, an isolated neutron star that is thought to lie 140 (\u00b140) parsecs, or 460 (\u00b1130) light years, away, with a diameter of 14\u00a0km. It was once suspected to be a strange star, but this has been discounted.\nDeep sky objects.\nIn the north of the constellation is the Corona Australis Molecular Cloud, a dark molecular cloud with many embedded reflection nebulae, including NGC 6729, NGC 6726\u20137, and IC 4812. A star-forming region of around , it contains Herbig\u2013Haro objects (protostars) and some very young stars. About 430 light years (130 parsecs) away, it is one of the closest star-forming regions to the Solar System. The related NGC 6726 and 6727, along with unrelated NGC 6729, were first recorded by Johann Friedrich Julius Schmidt in 1865. The Coronet cluster, about 554 light years (170 parsecs) away at the edge of the Gould Belt, is also used in studying star and protoplanetary disk formation.\nR Coronae Australis is an irregular variable star ranging from magnitudes 9.7 to 13.9. Blue-white, it is of spectral type B5IIIpe. A very young star, it is still accumulating interstellar material. It is obscured by, and illuminates, the surrounding nebula, NGC 6729, which brightens and darkens with it. The nebula is often compared to a comet for its appearance in a telescope, as its length is five times its width. S Coronae Australis is a G-class dwarf in the same field as R and is a T Tauri star. Nearby, another young variable star, TY Coronae Australis, illuminates another nebula: reflection nebula NGC 6726\u20137. TY Coronae Australis ranges irregularly between magnitudes 8.7 and 12.4, and the brightness of the nebula varies with it. Blue-white, it is of spectral type B8e. The largest young stars in the region, R, S, T, TY and VV Coronae Australis, are all ejecting jets of material which cause surrounding dust and gas to coalesce and form Herbig\u2013Haro objects, many of which have been identified nearby. Lying adjacent to the nebulosity is the globular cluster known as NGC 6723, which is actually in the neighbouring constellation of Sagittarius and is much much further away.\nNear Epsilon and Gamma Coronae Australis is Bernes 157, a dark nebula and star forming region. It is a large nebula, 55 by 18 arcminutes, that possesses several stars around magnitude 13. These stars have been dimmed by up to 8 magnitudes by its dust clouds.\nIC 1297 is a planetary nebula of apparent magnitude 10.7, which appears as a green-hued roundish object in higher-powered amateur instruments. The nebula surrounds the variable star RU Coronae Australis, which has an average apparent magnitude of 12.9 and is a WC class Wolf\u2013Rayet star. IC 1297 is small, at only 7 arcseconds in diameter; it has been described as \"a square with rounded edges\" in the eyepiece, elongated in the north\u2013south direction. Descriptions of its color encompass blue, blue-tinged green, and green-tinged blue.\nCorona Australis' location near the Milky Way means that galaxies are uncommonly seen. NGC 6768 is a magnitude 11.2 object 35\u2032 south of IC 1297. It is made up of two galaxies merging, one of which is an elongated elliptical galaxy of classification E4 and the other a lenticular galaxy of classification S0. IC 4808 is a galaxy of apparent magnitude 12.9 located on the border of Corona Australis with the neighbouring constellation of Telescopium and 3.9 degrees west-southwest of Beta Sagittarii. However, amateur telescopes will only show a suggestion of its spiral structure. It is 1.9 arcminutes by 0.8 arcminutes. The central area of the galaxy does appear brighter in an amateur instrument, which shows it to be tilted northeast\u2013southwest.\nSoutheast of Theta and southwest of Eta lies the open cluster ESO 281-SC24, which is composed of the yellow 9th magnitude star GSC 7914 178 1 and five 10th to 11th magnitude stars. Halfway between Theta Coronae Australis and Theta Scorpii is the dense globular cluster NGC 6541. Described as between magnitude 6.3 and magnitude 6.6, it is visible in binoculars and small telescopes. Around 22000 light years away, it is around 100 light years in diameter. It is estimated to be around 14 billion years old. NGC 6541 appears 13.1 arcminutes in diameter and is somewhat resolvable in large amateur instruments; a 12-inch telescope reveals approximately 100 stars but the core remains unresolved.\nMeteor showers.\nThe Corona Australids are a meteor shower that takes place between 14 and 18 March each year, peaking around 16 March. This meteor shower does not have a high peak hourly rate. In 1953 and 1956, observers noted a maximum of 6 meteors per hour and 4 meteors per hour respectively; in 1955 the shower was \"barely resolved\". However, in 1992, astronomers detected a peak rate of 45 meteors per hour. The Corona Australids' rate varies from year to year. At only six days, the shower's duration is particularly short, and its meteoroids are small; the stream is devoid of large meteoroids. The Corona Australids were first seen with the unaided eye in 1935 and first observed with radar in 1955. Corona Australid meteors have an entry velocity of 45 kilometers per second. In 2006, a shower originating near Beta Coronae Australis was designated as the Beta Coronae Australids. They appear in May, the same month as a nearby shower known as the May Microscopids, but the two showers have different trajectories and are unlikely to be related.\nHistory.\nCorona Australis may have been recorded by ancient Mesopotamians in the MUL.APIN, as a constellation called MA.GUR (\"The Bark\"). However, this constellation, adjacent to SUHUR.MASH (\"The Goat-Fish\", modern Capricornus), may instead have been modern Epsilon Sagittarii. As a part of the southern sky, MA.GUR was one of the fifteen \"stars of Ea\".\nIn the 3rd century BC, the Greek didactic poet Aratus wrote of, but did not name the constellation, instead calling the two crowns \u03a3\u03c4\u03b5\u03c6\u03ac\u03bd\u03bf\u03b9 (\"Stephanoi\"). The Greek astronomer Ptolemy described the constellation in the 2nd century AD, though with the inclusion of Alpha Telescopii, since transferred to Telescopium. Ascribing 13 stars to the constellation, he named it \u03a3\u03c4\u03b5\u03c6\u03ac\u03bd\u03bf\u03c2 \u03bd\u03bf\u03c4\u03b9\u03bf\u03c2 (), \"Southern Wreath\", while other authors associated it with either Sagittarius (having fallen off his head) or Centaurus; with the former, it was called \"Corona Sagittarii\". Similarly, the Romans called Corona Australis the \"Golden Crown of Sagittarius\". It was known as \"Parvum Coelum\" (\"Canopy\", \"Little Sky\") in the 5th century. The 18th-century French astronomer J\u00e9r\u00f4me Lalande gave it the names \"Sertum Australe\" (\"Southern Garland\") and \"Orbiculus Capitis\", while German poet and author Philippus Caesius called it \"Corolla\" (\"Little Crown\") or \"Spira Australis\" (\"Southern Coil\"), and linked it with the Crown of Eternal Life from the New Testament. Seventeenth-century celestial cartographer Julius Schiller linked it to the Diadem of Solomon. Sometimes, Corona Australis was not the wreath of Sagittarius but arrows held in his hand.\nCorona Australis has been associated with the myth of Bacchus and Stimula. Jupiter had impregnated Stimula, causing Juno to become jealous. Juno convinced Stimula to ask Jupiter to appear in his full splendor, which the mortal woman could not handle, causing her to burn. After Bacchus, Stimula's unborn child, became an adult and the god of wine, he honored his deceased mother by placing a wreath in the sky.\nIn Chinese astronomy, the stars of Corona Australis are located within the Black Tortoise of the North (\u5317\u65b9\u7384\u6b66, \"B\u011bi F\u0101ng Xu\u00e1n W\u01d4\"). The constellation itself was known as \"ti'en pieh\" (\"Heavenly Turtle\") and during the Western Zhou period, marked the beginning of winter. However, precession over time has meant that the \"Heavenly River\" (Milky Way) became the more accurate marker to the ancient Chinese and hence supplanted the turtle in this role. Arabic names for Corona Australis include \"Al \u0136ubbah\" \"the Tortoise\", \"Al \u0124ib\u0101\" \"the Tent\" or \"Al Ud\u1e25\u0101 al Na'\u0101m\" \"the Ostrich Nest\". It was later given the name \"Al Ikl\u012bl al Jan\u016bbiyyah\", which the European authors Chilmead, Riccioli and Caesius transliterated as Alachil Elgenubi, Elkleil Elgenubi and Aladil Algenubi respectively.\nThe \u01c0Xam speaking San people of South Africa knew the constellation as \"\u2260nabbe ta !nu\" \"house of branches\"\u2014owned originally by the Dassie (rock hyrax), and the star pattern depicting people sitting in a semicircle around a fire.\nThe indigenous Boorong people of northwestern Victoria saw it as \"Won\", a boomerang thrown by \"Totyarguil\" (Altair). The Aranda people of Central Australia saw Corona Australis as a coolamon carrying a baby, which was accidentally dropped to earth by a group of sky-women dancing in the Milky Way. The impact of the coolamon created Gosses Bluff crater, 175\u00a0km west of Alice Springs. The Torres Strait Islanders saw Corona Australis as part of a larger constellation encompassing part of Sagittarius and the tip of Scorpius's tail; the Pleiades and Orion were also associated. This constellation was Tagai's canoe, crewed by the Pleiades, called the \"Usiam\", and Orion, called the \"Seg\". The myth of Tagai says that he was in charge of this canoe, but his crewmen consumed all of the supplies onboard without asking permission. Enraged, Tagai bound the Usiam with a rope and tied them to the side of the boat, then threw them overboard. Scorpius's tail represents a suckerfish, while Eta Sagittarii and Theta Coronae Australis mark the bottom of the canoe. On the island of Futuna, the figure of Corona Australis was called \"Tanuma\" and in the Tuamotus, it was called \"Na Kaua-ki-Tonga\".\nReferences.\nSources.\n\"SIMBAD\""}
{"id": "6426", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=6426", "title": "Corcovado", "text": "Corcovado (), which means \"hunchback\" in Portuguese, is a mountain in central Rio de Janeiro, Brazil. It is a 710-metre (2,329\u00a0ft) granite peak located in the Tijuca Forest, a national park.\nCorcovado hill lies just west of the city center but is wholly within the city limits and visible from great distances. It is known worldwide for the 38-metre (125\u00a0ft) statue of Jesus atop its peak, entitled \"Christ the Redeemer\".\nAccess.\nThe peak and statue can be accessed via a narrow road, by the 3.8 kilometre (2.4\u00a0mi) Corcovado Rack Railway, which was opened in 1884 and refurbished in 1980, or by the walking trail on the south side of the mountain that starts from Parque Lage. The railway uses three electrically powered trains, with a passenger capacity of 540 passengers per hour. The rail trip takes approximately 20 minutes and departs every 20 minutes. Due to its limited passenger capacity, the wait to board at the entry station can take several hours. The year-round schedule is 8:30 to 18:30.\nFrom the train terminus and road, the observation deck at the foot of the statue is reached by 223 steps, or by elevators and escalators. Among the most popular year-round tourist attractions in Rio, the Corcovado railway, access roads, and statue platform are commonly crowded.\nAttractions.\nThe most popular attraction of Corcovado mountain is the statue and viewing platform at its peak, drawing over 300,000 visitors per year. From the peak's platform the panoramic view includes downtown Rio, Sugarloaf Mountain, the Lagoa Rodrigo de Freitas (lake), Copacabana and Ipanema beaches, Est\u00e1dio do Maracan\u00e3 (Maracan\u00e3 Stadium), and several of Rio's favelas. Cloud cover is common in Rio and the view from the platform is often obscured. Sunny days are recommended for optimal viewing.\nNotable past visitors to the mountain peak include Pope Pius XII, Pope John Paul II, Alberto Santos-Dumont, Albert Einstein, Diana, Princess of Wales, and General Sherman, among others. An additional attraction of the mountain is rock climbing. The south face had 54 climbing routes in 1992. The easiest way starts from Park Lage.\nThe Corcovado is also a symbol of the Brazilian culture.\nGeology.\nThe peak of Corcovado is a big granite dome, which describes a generally vertical rocky formation. It is claimed to be the highest such formation in Brazil, the second highest being Pedra Agulha, situated near to the town of Pancas in Esp\u00edrito Santo."}
{"id": "6427", "revid": "9021902", "url": "https://en.wikipedia.org/wiki?curid=6427", "title": "Cheddar, Somerset", "text": "Cheddar is a large village and civil parish in the Sedgemoor district of the English county of Somerset. It is situated on the southern edge of the Mendip Hills, north-west of Wells, south-east of Weston-super-Mare and south-west of Bristol. The civil parish includes the hamlets of Nyland and Bradley Cross. The parish had a population of 5,755 in 2011 and an acreage of as of 1961.\nCheddar Gorge, on the northern edge of the village, is the largest gorge in the United Kingdom and includes several show caves, including Gough's Cave. The gorge has been a centre of human settlement since Neolithic times including a Saxon palace. It has a temperate climate and provides a unique geological and biological environment that has been recognised by the designation of several Sites of Special Scientific Interest. It is also the site of several limestone quarries. The village gave its name to Cheddar cheese and has been a centre for strawberry growing. The crop was formerly transported on the Cheddar Valley rail line, which closed in the late 1960s but is now a cycle path. The village is now a major tourist destination with several cultural and community facilities, including the Cheddar Show Caves Museum.\nThe village supports a variety of community groups including religious, sporting and cultural organisations. Several of these are based on the site of The Kings of Wessex Academy, which is the largest educational establishment.\nHistory.\nThe name Cheddar comes from the Old English word \"ceodor\", meaning deep dark cavity or pouch.\nThere is evidence of occupation from the Neolithic period in Cheddar. Britain's oldest complete human skeleton, Cheddar Man, estimated to be 9,000 years old, was found in Cheddar Gorge in 1903. Older remains from the Upper Late Palaeolithic era (12,000\u201313,000 years ago) have been found. There is some evidence of a Bronze Age field system at the Batts Combe quarry site. There is also evidence of Bronze Age barrows at the mound in the Longwood valley, which if man-made it is likely to be a field system. The remains of a Roman villa have been excavated in the grounds of the current vicarage.\nThe village of Cheddar had been important during the Roman and Saxon eras. There was a royal palace at Cheddar during the Saxon period, which was used on three occasions in the 10th century to host the Witenagemot. The ruins of the palace were excavated in the 1960s. They are located on the grounds of The Kings of Wessex Academy, together with a 14th-century chapel dedicated to St. Columbanus. Roman remains have also been uncovered at the site. Cheddar was listed in the Domesday Book of 1086 as \"Ceder\", meaning \"Shear Water\", from the Old English \"scear\" and Old Welsh \"d\u0175r\". An alternative spelling in earlier documents, common through the 1850s is \"Chedder\". As early as 1130\u00a0AD, the Cheddar Gorge was recognised as one of the \"Four wonders of England\". Historically, Cheddar's source of wealth was farming and cheese making for which it was famous as early as 1170\u00a0AD. The parish was part of the Winterstoke Hundred.\nThe manor of Cheddar was deforested in 1337 and Bishop Ralph was granted a licence by the King to create a hunting forest.\nAs early as 1527 there are records of watermills on the river. In the 17th and 18th centuries, there were several watermills which ground corn and made paper, with 13 mills on the Yeo at the peak, declining to seven by 1791 and just three by 1915.\nIn the Victorian era it also became a centre for the production of clothing. The last mill, used as a shirt factory, closed in the early 1950s. William Wilberforce saw the poor conditions of the locals when he visited Cheddar in 1789. He inspired Hannah More in her work to improve the conditions of the Mendip miners and agricultural workers. In 1801, of common land were enclosed under the Inclosure Acts.\nTourism of the Cheddar gorge and caves began with the opening of the Cheddar Valley Railway in 1869.\nCheddar, its surrounding villages and specifically the gorge has been subject to flooding. In the Chew Stoke flood of 1968 the flow of water washed large boulders down the gorge, washed away cars, and damaged the cafe and the entrance to Gough's Cave.\nGovernment.\nCheddar is recognised as a village. The adjacent settlement of Axbridge, although only about a third the population of Cheddar, is a town. This apparently illogical situation is explained by the relative importance of the two places in historic times. While Axbridge grew in importance as a centre for cloth manufacturing in the Tudor period and gained a charter from King John, Cheddar remained a more dispersed mining and dairy-farming village. Its population grew with the arrival of the railways in the Victorian era and the advent of tourism.\nThe parish council, which has 15 members who are elected for four years, is responsible for local issues, including setting an annual precept (local rate) to cover the council's operating costs and producing annual accounts for public scrutiny. The parish council evaluates local planning applications and works with the police, district council officers, and neighbourhood watch groups on matters of crime, security, and traffic. The parish council's role also includes initiating projects for the maintenance and repair of parish facilities, as well as consulting with the district council on the maintenance, repair, and improvement of highways, drainage, footpaths, public transport, and street cleaning. Conservation matters (including trees and listed buildings) and environmental issues are also the responsibility of the council.\nThe village is in the 'Cheddar and Shipham' electoral ward. After including Shipham the total population of the ward taken at the 2011 census is 6,842.\nThe village falls within the non-metropolitan district of Sedgemoor, which was formed on 1 April 1974 under the Local Government Act 1972. It was previously part of Axbridge Rural District. Sedgemoor is responsible for local planning and building control, local roads, council housing, environmental health, markets and fairs, refuse collection and recycling, cemeteries and crematoria, leisure services, parks, and tourism. Somerset County Council is responsible for running the largest and most expensive local services such as education, social services, the library, roads, public transport, trading standards, waste disposal and strategic planning, although fire, police and ambulance services are provided jointly with other authorities through the Devon and Somerset Fire and Rescue Service, Avon and Somerset Constabulary and the South Western Ambulance Service.\nIt is also part of the Wells county constituency represented in the House of Commons of the Parliament of the United Kingdom. It elects one Member of Parliament (MP) by the first past the post system of election. Prior to Brexit in 2020, it was part of the South West England constituency of the European Parliament.\nInternational relations.\nCheddar is twinned with Felsberg, Germany and Vernouillet, France, and it has an active programme of exchange visits. Initially, Cheddar twinned with Felsberg in 1984. In 2000, Cheddar twinned with Vernouillet, which had also been twinned with Felsberg. Cheddar also has a friendship link with Ocho Rios in Saint Ann Parish, Jamaica.\nGeography.\nThe area is underlain by Black Rock slate, Burrington Oolite and Clifton Down Limestone of the Carboniferous Limestone Series, which contain ooliths and fossil debris on top of Old Red Sandstone, and by Dolomitic Conglomerate of the Keuper. Evidence for Variscan orogeny is seen in the sheared rock and cleaved shales. In many places weathering of these strata has resulted in the formation of immature calcareous soils.\nGorge and caves.\nCheddar Gorge, which is located on the edge of the village, is the largest gorge in the United Kingdom.\nThe gorge is the site of the Cheddar Caves, where Cheddar Man was found in 1903. Older remains from the Upper Late Palaeolithic era (12,000\u201313,000 years ago) have been found. The caves, produced by the activity of an underground river, contain stalactites and stalagmites. Gough's Cave, which was discovered in 1903, leads around into the rock-face, and contains a variety of large rock chambers and formations. Cox's Cave, discovered in 1837, is smaller but contains many intricate formations. A further cave houses a children's entertainment walk known as the \"Crystal Quest\".\nCheddar Gorge, including Cox's Cave, Gough's Cave and other attractions, has become a tourist destination, attracting about 500,000 visitors per year.\nIn a 2005 poll of \"Radio Times\" readers, following its appearance on the 2005 television programme \"Seven Natural Wonders\", Cheddar Gorge was named as the second greatest natural wonder in Britain, surpassed only by the Dan yr Ogof caves.\nSites of Special Scientific Interest.\nThere are several large and unique Sites of Special Scientific Interest (SSSI) around the village.\nCheddar Reservoir is a near-circular artificial reservoir operated by Bristol Water. Dating from the 1930s, it has a capacity of 135\u00a0million\u00a0gallons (614,000\u00a0cubic\u00a0metres). The reservoir is supplied with water taken from the Cheddar Yeo, which rises in Gough's Cave in Cheddar Gorge and is a tributary of the River Axe. The inlet grate for the water pipe that is used to transport the water can be seen next to the sensory garden in Cheddar Gorge. It has been designated as a Site of Special Scientific Interest (SSSI) due to its wintering waterfowl populations.\nCheddar Wood and the smaller Macall's Wood form a biological Site of Special Scientific Interest from what remains of the wood of the Bishops of Bath and Wells in the 13th century and of King Edmund the Magnificent's wood in the 10th. During the 19th century, its lower fringes were grubbed out to make strawberry fields. Most of these have been allowed to revert to woodland. The wood was coppiced until 1917. This site compromises a wide range of habitats which include ancient and secondary semi-natural broadleaved woodland, unimproved neutral grassland, and a complex mosaic of calcareous grassland and acidic dry dwarf-shrub heath. Cheddar Wood is one of only a few English stations for starved wood-sedge (\"Carex depauperata\"). Purple gromwell (\"Lithospermum purpurocaeruleum\"), a nationally rare plant, also grows in the wood. Butterflies include silver-washed fritillary (\"Argynnis paphia\"), dark green fritillary (\"Argynnis aglaja\"), pearl-bordered fritillary (\"Boloria euphrosyne\"), holly blue (\"Celastrina argiolus\") and brown argus (\"Aricia agestis\"). The slug \"Arion fasciatus\", which has a restricted distribution in the south of England, and the soldier beetle \"Cantharis fusca\" also occur.\nBy far the largest of the SSSIs is called Cheddar Complex and covers of the gorge, caves and the surrounding area. It is important because of both biological and geological features. It includes four SSSIs, formerly known as Cheddar Gorge SSSI, August Hole/Longwood Swallet SSSI, GB Cavern Charterhouse SSSI and Charterhouse on-Mendip SSSI. It is partly owned by the National Trust who acquired it in 1910 and partly managed by the Somerset Wildlife Trust.\nQuarries.\nClose to the village and gorge are Batts Combe quarry and Callow Rock quarry, two of the active Quarries of the Mendip Hills where limestone is still extracted. Operating since the early 20th century, Batts Combe is owned and operated by Hanson Aggregates. The output in 2005 was around 4,000 tonnes of limestone per day, one third of which was supplied to an on-site lime kiln, which closed in 2009; the remainder was sold as coated or dusted aggregates. The limestone at this site is close to 99\u00a0percent carbonate of calcium and magnesium (dolomite).\nThe Chelmscombe Quarry finished its work as a limestone quarry in the 1950s and was then used by the Central Electricity Generating Board as a tower testing station. During the 1970s and 1980s it was also used to test the ability of containers of radioactive material to withstand impacts and other accidents.\nClimate.\nAlong with the rest of South West England, Cheddar has a temperate climate which is generally wetter and milder than the rest of the country. The annual mean temperature is approximately . Seasonal temperature variation is less extreme than most of the United Kingdom because of the adjacent sea, which moderates temperature. The summer months of July and August are the warmest with mean daily maxima of approximately . In winter mean minimum temperatures of or are common. In the summer the Azores high-pressure system affects the south-west of England. Convective cloud sometimes forms inland, reducing the number of hours of sunshine; annual sunshine rates are slightly less than the regional average of 1,600\u00a0hours. In December 1998 there were 20 days without sun recorded at Yeovilton. Most of the rainfall in the south-west is caused by Atlantic depressions or by convection. Most of the rainfall in autumn and winter is caused by the Atlantic depressions, which are most active during those seasons. In summer, a large proportion of the rainfall is caused by sun heating the ground leading to convection and to showers and thunderstorms. Average rainfall is around . About 8\u201315 days of snowfall per year is typical. November to March have the highest mean wind speeds, and June to August have the lightest winds. The predominant wind direction is from the south-west.\nDemography.\nThe parish has a population in 2011 of 5,093, with a mean age of 43 years. Residents lived in 2,209 households. The vast majority of households (2,183) gave their ethnic status at the 2001 census as white.\nEconomy.\nThe village gave its name to Cheddar cheese, which is the most popular type of cheese in the United Kingdom. The cheese is now made and consumed worldwide, and only one producer remains in the village.\nSince the 1880s, Cheddar's other main produce has been the strawberry,\nwhich is grown on the south-facing lower slopes of the Mendip hills. As a consequence of its use for transporting strawberries to market, the since-closed Cheddar Valley line became known as \"The Strawberry Line\" after it opened in 1869.\nThe line ran from Yatton to Wells. When the rest of the line was closed and all passenger services ceased, the section of the line between Cheddar and Yatton remained open for goods traffic. It provided a fast link with the main markets for the strawberries in Birmingham and London, but finally closed in 1964, becoming part of the Cheddar Valley Railway Nature Reserve.\nCheddar Ales is a small brewery based in the village, producing beer for local public houses. Tourism is a significant source of employment. Around 15\u00a0percent of employment in Sedgemoor is provided by tourism, but within Cheddar it is estimated to employ as many as 1,000 people.\nThe village also has a youth hostel, and a number of camping and caravan sites.\nCulture and community.\nCheddar has a number of active service clubs including Cheddar Vale Lions Club, Mendip Rotary and Mendip Inner Wheel Club. The clubs raise money for projects in the local community and hold annual events such as a fireworks display, duck races in the Gorge, a dragon boat race on the reservoir and concerts on the grounds of the nearby St Michael's Cheshire Home.\nSeveral notable people have been born or lived in Cheddar. Musician Jack Bessant, the bass guitarist with the band Reef grew up on his parents' strawberry farm, and Matt Goss and Luke Goss, former members of Bros, lived in Cheddar for nine months as children. Trina Gulliver, ten-time World Professional Darts Champion, previously lived in Cheddar until 2017. Forex trader Dan Legg was born in Cheddar. The comedian Richard Herring grew up in Cheddar. His 2008 Edinburgh Festival Fringe show, \"The Headmaster's Son\" is based on his time at The Kings of Wessex School, where his father Keith was the headmaster. The final performance of this show was held at the school in November 2009. He also visited the school in March 2010 to perform his show \"Hitler Moustache\". In May 2013, a community radio station called Pulse was launched.\nLandmarks.\nThe market cross in Bath Street dates from the 15th century, with the shelter having been rebuilt in 1834. It has a central octagonal pier, a socket raised on four steps, a hexagonal shelter with six arched four-centred openings, shallow two-stage buttresses at each angle, and an embattled parapet. The shaft is crowned by an abacus with figures in niches, probably from the late 19th century, although the cross is now missing. It was rebuilt by Thomas, Marquess of Bath. It is a scheduled monument (Somerset County No\u00a021) and Grade\u00a0II* listed building.\nIn January 2000, the cross was seriously damaged in a traffic accident. By 2002, the cross had been rebuilt and the area around it was redesigned to protect and enhance its appearance.\nThe cross was badly damaged again in March 2012, when a taxi crashed into it late at night demolishing two sides.\nRepair work, which included the addition of wooden-clad steel posts to protect against future crashes, was completed in November 2012 at a cost of \u00a360,000.\nHannah More, a philanthropist and educator, founded a school in the village in the late 18th century for the children of miners. Her first school was located in a 17th-century house. Now named \"Hannah More's Cottage\", the Grade II-listed building is used by the local community as a meeting place.\nTransport.\nThe village is situated on the A371 road which runs from Wincanton, to Weston-super-Mare. It is approximately from the route of the M5 motorway with around a drive to junction 22.\nIt was on the Cheddar Valley line, a railway line that was opened in 1869 and closed in 1963. It became known as The Strawberry Line because of the large volume of locally-grown strawberries that it carried. It ran from Yatton railway station through to Wells (Tucker Street) railway station and joined the East Somerset Railway to make a through route via Shepton Mallet (High Street) railway station to Witham. Sections of the now-disused railway have been opened as the Strawberry Line Trail, which currently runs from Yatton to Cheddar. The Cheddar Valley line survived until the \"Beeching Axe\". Towards the end of its life there were so few passengers that diesel railcars were sometimes used. The Cheddar branch closed to passengers on 9 September 1963 and to goods in 1964. The line closed in the 1960s, when it became part of the Cheddar Valley Railway Nature Reserve, and part of the National Cycle Network route\u00a026. The cycle route also intersects with the West Mendip Way and various other footpaths.\nThe principle bus route is hourly service 126 between Weston-super-Mare and Wells operated by First West of England. Other bus routes include the service 668 from Shipham to Street which runs every couple of hours operated by Libra Travel, as well as the college bus service 66 which runs from Axbridge to the Bridgwater Campus of Bridgwater and Taunton College in the mornings and evenings of college term times and is operated by Bakers Dolphin.\nEducation.\nThe first school in Cheddar was set up by Hannah More during the 18th Century, however now Cheddar has three schools belonging to the Cheddar Valley Group of Schools, twelve schools that provide Cheddar Valley's three-tier education system. Cheddar First School has ten classes for children between 4 and 9 years. Fairlands Middle School, a middle school categorised as a middle-deemed-secondary school, has 510 pupils between 9 and 13. Fairlands takes children moving up from Cheddar First School as well as other first schools in the Cheddar Valley. The Kings of Wessex Academy, a coeducational comprehensive school, has been rated as \"good\" by Ofsted. It has 1,176 students aged 13 to 18, including 333 in the sixth form. Kings is a faith school linked to the Church of England. It was awarded the specialist status of Technology College in 2001, enabling it to develop its Information Technology (IT) facilities and improve courses in science, mathematics and design technology. In 2007 it became a foundation school, giving it more control over its own finances. The academy owns and runs a sports centre and swimming pool, Kings Fitness &amp; Leisure, with facilities that are used by students as well as residents. It has since November 2016 been a part of the Wessex Learning Trust which incorporates eight academies from the surrounding area.\nReligious sites.\nThe Church of St Andrew dates from the 14th century. It was restored in 1873 by William Butterfield. It is a Grade\u00a0I listed building and contains some 15th-century stained glass and an altar table of 1631. The chest tomb in the chancel is believed to contain the remains of Sir Thomas Cheddar and is dated 1442. The tower, which rises to , contains a bell dating from 1759 made by Thomas Bilbie of the Bilbie family.\nThere are also churches for Roman Catholic, Methodist and other denominations, including Cheddar Valley Community Church, who not only meet at The Kings of Wessex School on Sunday, but also have their own site on Tweentown for meeting during the week. The Baptist chapel was built in 1831.\nSport.\nKings Fitness &amp; Leisure, situated on the grounds of The Kings of Wessex School, provides a venue for various sports and includes a 20-metre swimming pool, racket sport courts, a sports hall, dance studios and a gym. A youth sports festival was held on Sharpham Road Playing Fields in 2009. In 2010 a skatepark was built in the village, funded by the Cheddar Local Action Team.\nCheddar Football Club, founded in 1892 and nicknamed \"The Cheesemen\", play in the Western Football League Division One. In 2009 plans were revealed to move the club from its present home at Bowdens Park on Draycott Road to a new larger site.\nCheddar Cricket Club was formed in the late 19th century and moved to Sharpham Road Playing Fields in 1964. They now play in the West of England Premier League Somerset Division. Cheddar Rugby Club, who own part of the Sharpham playing fields, was formed in 1836. The club organises an annual Cheddar Rugby Tournament. Cheddar Lawn Tennis Club, was formed in 1924, and play in the North Somerset League and also has social tennis and coaching. Cheddar Running Club organised an annual half marathon until 2009.\nThe village is both on the route of the West Mendip Way and Samaritans Way South West."}
{"id": "6429", "revid": "2927383", "url": "https://en.wikipedia.org/wiki?curid=6429", "title": "Compact disc", "text": "The compact disc (CD) is a digital optical disc data storage format that was co-developed by Philips and Sony to store and play digital audio recordings. It was released in 1982 branded as \"Digital Audio Compact Disc\".\nThe format was later adapted for storage of data (CD-ROM). Several other formats were further derived from these, including write-once audio and data storage (CD-R), rewritable media (CD-RW), Video CD (VCD), Super Video CD (SVCD), Photo CD, Picture CD, Compact Disc-Interactive (CD-i), and Enhanced Music CD.\nStandard CDs have a diameter of and are designed to hold up to 74 minutes of uncompressed stereo digital audio or about 650\u00a0MB of data. Capacity is routinely extended to 80 minutes and 700\u00a0MB by arranging more data closely on the same sized disc. The Mini CD has various diameters ranging from ; they are sometimes used for CD singles, storing up to 24 minutes of audio, or delivering device drivers.\nAt the time of the technology's introduction in 1982, a CD could store much more data than a personal computer hard disk drive, which would typically hold 10 MB. By 2010, hard drives commonly offered as much storage space as a thousand CDs, while their prices had plummeted to commodity level. In 2004, worldwide sales of audio CDs, CD-ROMs, and CD-Rs reached about 30 billion discs. By 2007, 200 billion CDs had been sold worldwide.\nPhysical details.\nA CD is made from thick, polycarbonate plastic and weighs 14\u201333 grams. From the center outward, components are: the center spindle hole (15\u00a0mm), the first-transition area (clamping ring), the clamping area (stacking ring), the second-transition area (mirror band), the program (data) area, and the rim. The inner program area occupies a radius from 25 to 58\u00a0mm.\nA thin layer of aluminum or, more rarely, gold is applied to the surface, making it reflective. The metal is protected by a film of lacquer normally spin coated directly on the reflective layer. The label is printed on the lacquer layer, usually by screen printing or offset printing.\nCD data is represented as tiny indentations known as \"pits\", encoded in a spiral track moulded into the top of the polycarbonate layer. The areas between pits are known as \"lands\". Each pit is approximately 100\u00a0nm deep by 500\u00a0nm wide, and varies from 850\u00a0nm to 3.5\u00a0\u00b5m in length. The distance between the tracks (the \"pitch\") is 1.6\u00a0\u00b5m.\nWhen playing an audio CD, a motor within the CD player spins the disc to a scanning velocity of 1.2\u20131.4\u00a0m/s (constant linear velocity, CLV)\u2014equivalent to approximately 500 RPM at the inside of the disc, and approximately 200 RPM at the outside edge. The track on the CD begins at the inside and spirals outward so a disc played from beginning to end slows its rotation rate during playback.\nThe program area is 86.05\u00a0cm2 and the length of the recordable spiral is With a scanning speed of 1.2\u00a0m/s, the playing time is 74 minutes, or 650\u00a0MB of data on a CD-ROM. A disc with data packed slightly more densely is tolerated by most players (though some old ones fail). Using a linear velocity of 1.2\u00a0m/s and a narrower track pitch of 1.5\u00a0\u00b5m increases the playing time to 80 minutes, and data capacity to 700\u00a0MB.\nA CD is read by focusing a 780\u00a0nm wavelength (near infrared) semiconductor laser through the bottom of the polycarbonate layer. The change in height between pits and lands results in a difference in the way the light is reflected. Because the pits are indented into the top layer of the disc and are read through the transparent polycarbonate base, the pits form bumps when read. The laser hits the disc, casting a circle of light wider than the modulated spiral track reflecting partially from the lands and partially from the top of any bumps where they are present. As the laser passes over a pit (bump), its height means that the part of the light reflected from its peak is 1/2 wavelength out of phase with the light reflected from the land around it. This causes partial cancellation of the laser's reflection from the surface. By measuring the reflected intensity change with a photodiode, a modulated signal is read back from the disc.\nTo accommodate the spiral pattern of data, the laser is placed on a mobile mechanism within the disc tray of any CD player. This mechanism typically takes the form of a sled that moves along a rail. The sled can be driven by a worm gear or linear motor. Where a worm gear is used, a second shorter-throw linear motor, in the form of a coil and magnet, makes fine position adjustments to track eccentricities in the disk at high speed. Some CD drives (particularly those manufactured by Philips during the 1980s and early 1990s) use a swing arm similar to that seen on a gramophone. This mechanism allows the laser to read information from the center to the edge of a disc without having to interrupt the spinning of the disc itself.\nThe pits and lands do \"not\" directly represent the 0's and 1's of binary data. Instead, non-return-to-zero, inverted encoding is used: a change from either pit to land or land to pit indicates a 1, while no change indicates a series of 0's. There must be at least 2, and no more than 10 0's between each 1, which is defined by the length of the pit. This, in turn, is decoded by reversing the eight-to-fourteen modulation used in mastering the disc, and then reversing the cross-interleaved Reed\u2013Solomon coding, finally revealing the raw data stored on the disc. These encoding techniques (defined in the \"Red Book\") were originally designed for CD Digital Audio, but they later became a standard for almost all CD formats (such as CD-ROM).\nIntegrity.\nCDs are susceptible to damage during handling and from environmental exposure. Pits are much closer to the label side of a disc, enabling defects and contaminants on the clear side to be out of focus during playback. Consequently, CDs are more likely to suffer damage on the label side of the disc. Scratches on the clear side can be repaired by refilling them with similar refractive plastic or by careful polishing. The edges of CDs are sometimes incompletely sealed, allowing gases and liquids to enter the CD and corrode the metal reflective layer and/or interfere with the focus of the laser on the pits, a condition known as disc rot. The fungus \"Geotrichum candidum\" has been found\u2014under conditions of high heat and humidity\u2014to consume the polycarbonate plastic and aluminium found in CDs.\nThe data integrity of compact discs can be measured using surface error scanning, which is able to measure the rates of different types of data errors, known as \"C1\", \"C2\", \"CU\" and extended (finer-grain) error measurements known as \"E11\", \"E12\", \"E21\", \"E22\", \"E31\" and \"E32\", of which higher rates indicate a possibly damaged or unclean data surface, low media quality, deteriorating media and recordable media written to by a malfunctioning CD writer.\nError scanning can reliably predict data losses caused by media deteriorating. Support of error scanning varies among vendors and models of optical disc drives, and \"extended\" error scanning (known as \"advanced error scanning\" in Nero DiscSpeed) has only been available on Plextor and some BenQ optical drives so far, as of 2020.\nDisc shapes and diameters.\nThe digital data on a CD begins at the center of the disc and proceeds toward the edge, which allows adaptation to the different size formats available. Standard CDs are available in two sizes. By far, the most common is in diameter, with a 74- or 80-minute audio capacity and a 650 or 700\u00a0MB (737,280,000-byte) data capacity. Discs are 1.2\u00a0mm thick, with a 15\u00a0mm center hole. The official Philips history says this capacity was specified by Sony executive Norio Ohga to be able to contain the entirety of Beethoven's Ninth Symphony on one disc. This is a myth according to Kees Immink, as the EFM code format had not yet been decided in December 1979, when the decision to adopt the 120\u00a0mm was made. The adoption of EFM in June 1980 allowed 30 percent more playing time that would have resulted in 97 minutes for 120\u00a0 mm diameter or 74 minutes for a disc as small as 100\u00a0 mm. Instead, however, the information density was lowered by 30 percent to keep the playing time at 74 minutes. The 120\u00a0 mm diameter has been adopted by subsequent formats, including Super Audio CD, DVD, HD DVD, and Blu-ray Disc. The 80-mm-diameter discs (\"Mini CDs\") can hold up to 24 minutes of music or 210\u00a0MB.\nLogical format.\nAudio CD.\nThe logical format of an audio CD (officially Compact Disc Digital Audio or CD-DA) is described in a document produced in 1980 by the format's joint creators, Sony and Philips. The document is known colloquially as the \"Red Book\" CD-DA after the color of its cover. The format is a two-channel 16-bit PCM encoding at a 44.1\u00a0kHz sampling rate per channel. Four-channel sound was to be an allowable option within the \"Red Book\" format, but has never been implemented. Monaural audio has no existing standard on a \"Red Book\" CD; thus, the mono source material is usually presented as two identical channels in a standard \"Red Book\" stereo track (i.e., mirrored mono); an MP3 CD, however, can have audio file formats with mono sound.\nCD-Text is an extension of the \"Red Book\" specification for an audio CD that allows for the storage of additional text information (e.g., album name, song name, artist) on a standards-compliant audio CD. The information is stored either in the lead-in area of the CD, where there is roughly five kilobytes of space available or in the subcode channels R to W on the disc, which can store about 31 megabytes.\nCompact Disc + Graphics is a special audio compact disc that contains graphics data in addition to the audio data on the disc. The disc can be played on a regular audio CD player, but when played on a special CD+G player, it can output a graphics signal (typically, the CD+G player is hooked up to a television set or a computer monitor); these graphics are almost exclusively used to display lyrics on a television set for karaoke performers to sing along with. The CD+G format takes advantage of the channels R through W. These six bits store the graphics information.\nCD + Extended Graphics (CD+EG, also known as CD+XG) is an improved variant of the Compact Disc + Graphics (CD+G) format. Like CD+G, CD+EG uses basic CD-ROM features to display text and video information in addition to the music being played. This extra data is stored in subcode channels R-W. Very few, if any, CD+EG discs have been published.\nSuper Audio CD.\nSuper Audio CD (SACD) is a high-resolution read-only optical audio disc format that was designed to provide higher-fidelity digital audio reproduction than the \"Red Book\". Introduced in 1999, it was developed by Sony and Philips, the same companies that created the \"Red Book\". SACD was in a format war with DVD-Audio, but neither has replaced audio CDs. The SACD standard is referred to as the \"Scarlet Book\" standard.\nTitles in the SACD format can be issued as hybrid discs; these discs contain the SACD audio stream as well as a standard audio CD layer which is playable in standard CD players, thus making them backward compatible.\nCD-MIDI.\nCD-MIDI is a format used to store music-performance data, which upon playback is performed by electronic instruments that synthesize the audio. Hence, unlike the original \"Red Book\" CD-DA, these recordings are not digitally sampled audio recordings. The CD-MIDI format is defined as an extension of the original \"Red Book\".\nCD-ROM.\nFor the first few years of its existence, the CD was a medium used purely for audio. However, in 1988, the \"Yellow Book\" CD-ROM standard was established by Sony and Philips, which defined a non-volatile optical data computer data storage medium using the same physical format as audio compact discs, readable by a computer with a CD-ROM drive.\nVideo CD (VCD).\nVideo CD (VCD, View CD, and Compact Disc digital video) is a standard digital format for storing video media on a CD. VCDs are playable in dedicated VCD players, most modern DVD-Video players, personal computers, and some video game consoles.\nThe VCD standard was created in 1993 by Sony, Philips, Matsushita, and JVC and is referred to as the \"White Book\" standard.\nOverall picture quality is intended to be comparable to VHS video. Poorly compressed VCD video can sometimes be lower quality than VHS video, but VCD exhibits block artifacts rather than analog noise and does not deteriorate further with each use.\n352\u00d7240 (or SIF) resolution was chosen because it is half the vertical and half the horizontal resolution of the NTSC video. 352\u00d7288 is similarly one-quarter PAL/SECAM resolution. This approximates the (overall) resolution of an analog VHS tape, which, although it has double the number of (vertical) scan lines, has a much lower horizontal resolution.\nSuper Video CD.\nSuper Video CD (Super Video Compact Disc or SVCD) is a format used for storing video media on standard compact discs. SVCD was intended as a successor to VCD and an alternative to DVD-Video and falls somewhere between both in terms of technical capability and picture quality.\nSVCD has two thirds the resolution of DVD, and over 2.7 times the resolution of VCD. One CD-R disc can hold up to 60 minutes of standard-quality SVCD-format video. While no specific limit on SVCD video length is mandated by the specification, one must lower the video bit rate, and therefore quality, to accommodate very long videos. It is usually difficult to fit much more than 100 minutes of video onto one SVCD without incurring significant quality loss, and many hardware players are unable to play video with an instantaneous bit rate lower than 300 to 600 kilobits per second.\nPhoto CD.\nPhoto CD is a system designed by Kodak for digitizing and storing photos on a CD. Launched in 1992, the discs were designed to hold nearly 100 high-quality images, scanned prints and slides using special proprietary encoding. Photo CDs are defined in the \"Beige Book\" and conform to the CD-ROM XA and CD-i Bridge specifications as well. They are intended to play on CD-i players, Photo CD players, and any computer with suitable software (irrespective of operating system). The images can also be printed out on photographic paper with a special Kodak machine. This format is not to be confused with Kodak Picture CD, which is a consumer product in CD-ROM format.\nCD-i.\nThe Philips \"Green Book\" specifies a standard for interactive multimedia compact discs designed for CD-i players (1993). CD-i discs can contain audio tracks that can be played on regular CD players, but CD-i discs are not compatible with most CD-ROM drives and software. The CD-i Ready specification was later created to improve compatibility with audio CD players, and the CD-i Bridge specification was added to create CD-i compatible discs that can be accessed by regular CD-ROM drives.\nCD-i Ready.\nPhilips defined a format similar to CD-i called CD-i Ready, which puts CD-i software and data into the pregap of track 1. This format was supposed to be more compatible with older audio CD players.\nEnhanced Music CD (CD+).\nEnhanced Music CD, also known as CD Extra or CD Plus, is a format which combines audio tracks and data tracks on the same disc by putting audio tracks in a first session and data in a second session. It was developed by Philips and Sony, and it is defined in the \"Blue Book\".\nVinylDisc.\nVinylDisc is the hybrid of a standard audio CD and the vinyl record. The vinyl layer on the disc's label side can hold approximately three minutes of music.\nManufacture.\nIn 1995, material costs were 30 cents for the jewel case and 10 to 15 cents for the CD. Wholesale cost of CDs was $0.75 to $1.15, while the typical retail price of a prerecorded music CD was $16.98. On average, the store received 35 percent of the retail price, the record company 27 percent, the artist 16 percent, the manufacturer 13 percent, and the distributor 9 percent. When 8-track cartridges, compact cassettes and CDs were introduced, each was marketed at a higher price than the format they succeeded, even though the cost to produce the media was reduced. This was done because the apparent value increased. This continued from vinyl to CDs but was broken when Apple marketed MP3s for $0.99, and albums for $9.99. The incremental cost, though, to produce an MP3 is negligible.\nWritable compact discs.\nRecordable CD.\nRecordable Compact Discs, CD-Rs, are injection-molded with a \"blank\" data spiral. A photosensitive dye is then applied, after which the discs are metalized and lacquer-coated. The write laser of the CD recorder changes the color of the dye to allow the read laser of a standard CD player to see the data, just as it would with a standard stamped disc. The resulting discs can be read by most CD-ROM drives and played in most audio CD players. CD-Rs follow the \"Orange Book\" standard.\nCD-R recordings are designed to be permanent. Over time, the dye's physical characteristics may change causing read errors and data loss until the reading device cannot recover with error correction methods. Errors can be predicted using surface error scanning. The design life is from 20 to 100 years, depending on the quality of the discs, the quality of the writing drive, and storage conditions. However, testing has demonstrated such degradation of some discs in as little as 18 months under normal storage conditions. This failure is known as disc rot, for which there are several, mostly environmental, reasons.\nThe recordable audio CD is designed to be used in a consumer audio CD recorder. These consumer audio CD recorders use SCMS (Serial Copy Management System), an early form of digital rights management (DRM), to conform to the AHRA (Audio Home Recording Act). The Recordable Audio CD is typically somewhat more expensive than CD-R due to lower production volume and a 3 percent AHRA royalty used to compensate the music industry for the making of a copy.\nHigh-capacity recordable CD is a higher-density recording format that can hold 20% more data than of conventional discs. The higher capacity is incompatible with some recorders and recording software.\nReWritable CD.\nCD-RW is a re-recordable medium that uses a metallic alloy instead of a dye. The write laser, in this case, is used to heat and alter the properties (amorphous vs. crystalline) of the alloy, and hence change its reflectivity. A CD-RW does not have as great a difference in reflectivity as a pressed CD or a CD-R, and so many earlier CD audio players \"cannot\" read CD-RW discs, although \"most\" later CD audio players and stand-alone DVD players can. CD-RWs follow the \"Orange Book\" standard.\nThe ReWritable Audio CD is designed to be used in a consumer audio CD recorder, which will not (without modification) accept standard CD-RW discs. These consumer audio CD recorders use the Serial Copy Management System (SCMS), an early form of digital rights management (DRM), to conform to the United States' Audio Home Recording Act (AHRA). The ReWritable Audio CD is typically somewhat more expensive than CD-R due to (a) lower volume and (b) a 3 percent AHRA royalty used to compensate the music industry for the making of a copy.\nCopy protection.\nThe \"Red Book\" audio specification, except for a simple \"anti-copy\" statement in the subcode, does not include any copy protection mechanism. Known at least as early as 2001, attempts were made by record companies to market \"copy-protected\" non-standard compact discs, which cannot be ripped, or copied, to hard drives or easily converted to other formats (like FLAC, MP3 or Vorbis). One major drawback to these copy-protected discs is that most will not play on either computer CD-ROM drives or some standalone CD players that use CD-ROM mechanisms. Philips has stated that such discs are not permitted to bear the trademarked \"Compact Disc Digital Audio\" logo because they violate the \"Red Book\" specifications. Numerous copy-protection systems have been countered by readily available, often free, software, or even by simply turning off automatic AutoPlay to prevent the running of the DRM executable program."}
{"id": "6431", "revid": "26801", "url": "https://en.wikipedia.org/wiki?curid=6431", "title": "Charles Farrar Browne", "text": "Charles Farrar Browne (April 26, 1834 \u2013 March 6, 1867) was an American humor writer, better known under his \"nom de plume\", Artemus Ward, which as a character, an illiterate rube with \"Yankee common sense\", Browne also played in public performances. He is considered to be America's first stand-up comedian. His birth name was Brown but he added the \"e\" after he became famous.\nBiography.\nBrowne was born in Waterford, Maine. He began his career as a compositor and occasional contributor to the daily and weekly journals. In 1858, in \"The Plain Dealer\" newspaper (Cleveland, Ohio), he published the first of the \"Artemus Ward\" series, which, in collected form, achieved great popularity in both America and England.\nBrownes' companion at the Plain Dealer, George Hoyt, wrote: \"his desk was a rickety table which had been whittled and gashed until it looked as if it had been the victim of lightning. His chair was a fit companion thereto, a wabbling, unsteady affair, sometimes with four and sometimes with three legs. But Browne saw neither the table, nor the chair, nor any person who might be near, nothing, in fact, but the funny pictures which were tumbling out of his brain. When writing, his gaunt form looked ridiculous enough. One leg hung over the arm of his chair like a great hook, while he would write away, sometimes laughing to himself, and then slapping the table in the excess of his mirth.\"\nIn 1860, he became editor of the first \"Vanity Fair\", a humorous New York weekly that failed in 1863. At about the same time, he began to appear as a lecturer who, by his droll and eccentric humor, attracted large audiences. Browne was also known as a member of the New York bohemian set which included leader Henry Clapp Jr., Walt Whitman, Fitz Hugh Ludlow, and actress Adah Isaacs Menken. \nIn 1863, Browne came to San Francisco to perform as Artemus Ward. An early expert at show business publicity, Browne sent his manager ahead by several weeks to buy advertising in the local papers and promote the show among prominent citizens for endorsements. On November 13, 1863, Browne stood before a packed crowd at Platt's Music Hall, playing the part of Artemus Ward as an illiterate rube but with \"Yankee common sense.\" Writer Brett Harte was in the audience that night and he described it in \"the Golden Era\" as capturing American speech: \"humor that belongs to the country of boundless prairies, limitless rivers, and stupendous cataracts--that fun which overlies the surface of our national life, which is met in the stage, rail-car, canal and flat-boat, which bursts out over camp-fires and around bar-room stoves.\"\n\"Artemus Ward\" was a favorite author of U.S. President Abraham Lincoln. Before presenting \"The Emancipation Proclamation\" to his Cabinet, Lincoln read to them the latest episode, \"Outrage in Utiky\", also known as \"High-Handed Outrage at Utica\".\nWhen Browne performed in Virginia City, Nevada, he met Mark Twain and the two became friends. In his correspondence with Twain, Browne called him \"My Dearest Love.\" Legend has it that, following a stage performance there, Browne, Twain, and Dan De Quille were trekking on a (drunken) rooftop tour of Virginia City until a town constable threatened to blast all three with a shotgun loaded with rock salt. Browne recommended Twain to the editors of the New York Press and urged him to journey to New York.\nIn 1866, Browne visited England and attracted a large following to his playing Artemus Ward, both as lecturer and for his writerly contributions to \"Punch\". But within a year his health gave way and he died of tuberculosis at Southampton on March 6, 1867.\nIn England Browne was buried at Kensal Green Cemetery, but his remains were removed to the United States in 1868 and buried at Elm Vale Cemetery in Waterford, Maine."}
{"id": "6432", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=6432", "title": "Caelum", "text": "Caelum is a faint constellation in the southern sky, introduced in the 1750s by Nicolas Louis de Lacaille and counted among the 88 modern constellations. Its name means \u201c\"chisel\"\u201d in Latin, and it was formerly known as Caelum Sculptorium (\u201c\"the engravers\u2019 chisel\"\u201d); It is a rare word, unrelated to the far more common Latin \"caelum\", meaning \u201csky, heaven, atmosphere\u201d. It is the eighth-smallest constellation, and subtends a solid angle of around 0.038\u00a0steradians, just less than that of Corona Australis.\nDue to its small size and location away from the plane of the Milky Way, Caelum is a rather barren constellation, with few objects of interest. The constellation's brightest star, Alpha Caeli, is only of magnitude\u00a04.45, and only one other star, (Gamma) \u03b3\u00a01\u00a0Caeli, is brighter than magnitude 5\u00a0. Other notable objects in Caelum are RR\u00a0Caeli, a binary star with one known planet approximately away; X\u00a0Caeli, a Delta Scuti variable that forms an optical double with \u03b3\u00a01\u00a0Caeli; and HE0450-2958, a Seyfert galaxy that at first appeared as just a jet, with no host galaxy visible.\nHistory.\nCaelum was incepted as one of fourteen southern constellations in the 18th century by Nicolas Louis de Lacaille, a French astronomer and celebrater of the Age of Enlightenment.\nIt retains its name \"Burin\" among French speakers, latinized in his catalogue of 1763 as \"Caelum Sculptoris\" (\u201c\"Engraver's Chisel\"\u201d).\nFrancis Baily shortened this name to \"Caelum\", as suggested by John Herschel. In Lacaille's original chart, it was shown as a pair of engraver's tools: a standard burin and more specific shape-forming \u00e9choppe tied by a ribbon, but came to be ascribed a simple chisel. Johann Elert Bode stated the name as plural with a singular possessor, \"Caela Scalptoris\" \u2013 in German (\"die\"\u00a0) \"Grabstichel\" (\u201c\"the Engraver\u2019s Chisels\"\u201d) \u2013 but this did not stick.\nCharacteristics.\nCaelum is bordered by Dorado and Pictor to the south, Horologium and Eridanus to the east, Lepus to the north, and Columba to the west. Covering only 125\u00a0square degrees, it ranks 81st of the 88 modern constellations in size.\nIts main asterism consists of four stars, and twenty stars in total are brighter than magnitude\u00a06.5\u00a0.\nThe constellation's boundaries, as set by Belgian astronomer Eug\u00e8ne Delporte in 1930, are a 12-sided polygon. In the equatorial coordinate system, the right ascension coordinates of these borders lie between and and declinations of to . The International Astronomical Union (IAU) adopted the three-letter abbreviation \u201cCae\u201d for the constellation in 1922.\nIts main stars are visible in favourable conditions and with a clear southern horizon, for part of the year as far as about the 41st parallel north\nThese stars avoid being engulfed by daylight for some of every day (when above the horizon) to viewers in mid- and well-inhabited higher latitudes of the Southern Hemisphere. Caelum shares with (to the north) Taurus, Eridanus and Orion midnight culmination in December (high summer), resulting in this fact. In winter (such as June) the constellation can be observed sufficiently inset from the horizons during its rising before dawn and/or setting after dusk as it culminates then at around mid-day, well above the sun. In South Africa, Argentina, their sub-tropical neighbouring areas and some of Australia in high June the key stars may be traced before dawn in the east; near the equator the stars lose night potential in May to June; they ill-compete with the Sun in northern tropics and sub-tropics from late February to mid-September with March being unfavorable as to post-sunset due to the light of the Milky Way.\nNotable features.\nStars.\nCaelum is a faint constellation: It has no star brighter than magnitude\u00a04 and only two stars brighter than magnitude\u00a05.\nLacaille gave six stars Bayer designations, labeling them Alpha (\u03b1\u00a0) to Zeta (\u03b6\u00a0) in 1756, but omitted Epsilon (\u03b5\u00a0) and designated two adjacent stars as Gamma (\u03b3\u00a0). Bode extended the designations to Rho (\u03c1\u00a0) for other stars, but most of these have fallen out of use. Caelum is too far south for any of its stars to bear Flamsteed designations.\nThe brightest star, (Alpha) \u03b1\u00a0Caeli, is a double star, containing an F-type main-sequence star of magnitude\u00a04.45 and a red dwarf of magnitude\u00a012.5\u00a0, from Earth. (Beta) \u03b2\u00a0Caeli, another F-type star of magnitude\u00a05.05\u00a0, is further away, being located from Earth. Unlike \u03b1, \u03b2\u00a0Caeli is a subgiant star, slightly evolved from the main sequence. (Delta) \u03b4\u00a0Caeli, also of magnitude\u00a05.05\u00a0, is a B-type subgiant and is much farther from Earth, at .\n(Gamma) \u03b3\u00a01\u00a0Caeli is a double-star with a red giant primary of magnitude\u00a04.58 and a secondary of magnitude\u00a08.1\u00a0. The primary is from Earth. The two components are difficult to resolve with small amateur telescopes because of their difference in visual magnitude and their close separation. This star system forms an optical double with the unrelated X\u00a0Caeli (previously named \u03b3\u00a02\u00a0Caeli), a Delta Scuti variable located from Earth. These are a class of short-period (six hours at most) pulsating stars that have been used as standard candles and as subjects to study astroseismology. X\u00a0Caeli itself is also a binary star, specifically a contact binary, meaning that the stars are so close that they share envelopes. The only other variable star in Caelum visible to the naked eye is RV\u00a0Caeli, a pulsating red giant of spectral type M1III, which varies between magnitudes\u00a06.44 and 6.56\u00a0.\nThree other stars in Caelum are still occasionally referred to by their Bayer designations, although they are only on the edge of naked-eye visibility. (Nu) \u03bd\u00a0Caeli is another double star, containing a white giant of magnitude\u00a06.07 and a star of magnitude\u00a010.66, with unknown spectral type. The system is approximately away. (Lambda) \u03bb\u00a0Caeli, at magnitude\u00a06.24, is much redder and farther away, being a red giant around from Earth. (Zeta) \u03b6\u00a0Caeli is even fainter, being only of magnitude\u00a06.36\u00a0. This star, located away, is a K-type subgiant of spectral type K1. The other twelve naked-eye stars in Caelum are not referred to by Bode's Bayer designations anymore, including RV\u00a0Caeli.\nOne of the nearest stars in Caelum is the eclipsing binary star RR\u00a0Caeli, at a distance of . This star system consists of a dim red dwarf and a white dwarf. Despite its closeness to the Earth, the system's apparent magnitude is only 14.40 due to the faintness of its components, and thus it cannot be easily seen with amateur equipment. In 2012, the system was found to contain a giant planet, and there is evidence for a second substellar body. The system is a post-common-envelope binary and is losing angular momentum over time, which will eventually cause mass transfer from the red dwarf to the white dwarf. In approximately 9\u201320 billion years, this will cause the system to become a cataclysmic variable.\nDeep-sky objects.\nDue to its small size and location away from the plane of the Milky Way, Caelum is rather devoid of deep-sky objects, and contains no Messier objects. The only deep-sky object in Caelum to receive much attention is HE0450-2958, an unusual Seyfert galaxy. Originally, the jet's host galaxy proved elusive to find, and this jet appeared to be emanating from nothing. Although it has been suggested that the object is an ejected supermassive black hole, the host is now agreed to be a small galaxy that is difficult to see due to light from the jet and a nearby starburst galaxy.\nThe 13th magnitude planetary nebula PN G243-37.1 is also in the eastern regions of the constellation. It is one of only a few planetary nebulae found in the galactic halo, being light-years below the Milky Way's 1000 light-year-thick disk."}
{"id": "6433", "revid": "372290", "url": "https://en.wikipedia.org/wiki?curid=6433", "title": "Clarinet", "text": " \nThe clarinet is a family of woodwind instruments. It has a single-reed mouthpiece, a straight, cylindrical tube with an almost cylindrical bore, and a flared bell. A person who plays a clarinet is called a \"clarinetist\" (sometimes spelled \"clarinettist\").\nWhile the similarity in sound between the earliest clarinets and the trumpet may hold a clue to its name, other factors may have been involved. During the Late Baroque era, composers such as Bach and Handel were making new demands on the skills of their trumpeters, who were often required to play difficult melodic passages in the high, or as it came to be called, \"clarion\" register. Since the trumpets of this time had no valves or pistons, melodic passages would often require the use of the highest part of the trumpet's range, where the harmonics were close enough together to produce scales of adjacent notes as opposed to the gapped scales or arpeggios of the lower register. The trumpet parts that required this specialty were known by the term \"clarino\" and this in turn came to apply to the musicians themselves. It is probable that the term clarinet may stem from the diminutive version of the 'clarion' or 'clarino' and it has been suggested that clarino players may have helped themselves out by playing particularly difficult passages on these newly developed \"mock trumpets\".\nJohann Christoph Denner is generally believed to have invented the clarinet in Germany around the year 1700 by adding a register key to the earlier chalumeau, usually in the key of C. Over time, additional keywork and airtight pads were added to improve the tone and playability.\nIn modern times, the most common clarinet is the B clarinet. However, the clarinet in A, pitched a semitone lower, is regularly used in orchestral, chamber and solo music. An orchestral clarinetist must own both a clarinet in A and B since the repertoire is divided fairly evenly between the two. Since the middle of the 19th century, the bass clarinet (nowadays invariably in B but with extra keys to extend the register down to low written C3) has become an essential addition to the orchestra. The clarinet family ranges from the (extremely rare) BBB octo-contrabass to the A piccolo clarinet. The clarinet has proved to be an exceptionally flexible instrument, used in the classical repertoire as in concert bands, military bands, marching bands, klezmer, jazz, and other styles.\nEtymology.\nThe word \"clarinet\" may have entered the English language via the French \"clarinette\" (the feminine diminutive of Old French \"clarin\" or \"clarion\"), or from Proven\u00e7al \"\", \"oboe\".\nIt would seem, however, that its real roots are to be found among some of the various names for trumpets used around the Renaissance and Baroque eras. \"Clarion\", \"clarin\", and the Italian \"clarino\" are all derived from the medieval term \"claro\", which referred to an early form of trumpet. This is probably the origin of the Italian \"clarinetto\", itself a diminutive of \"clarino\", and consequently of the European equivalents such as \"clarinette\" in French or the German \"Klarinette\". According to Johann Gottfried Walther, writing in 1732, the reason for the name is that \"it sounded from far off not unlike a trumpet\". The English form \"clarinet\" is found as early as 1733, and the now-archaic \"clarionet\" appears from 1784 until the early years of the 20th century.\nCharacteristics.\nSound.\nThe cylindrical bore is primarily responsible for the clarinet's distinctive timbre, which varies between its three main registers, known as the \"chalumeau\", \"clarion\", and \"altissimo\". The tone quality can vary greatly with the clarinetist, music, instrument, mouthpiece, and reed. The differences in instruments and geographical isolation of clarinetists led to the development from the last part of the 18th century onwards of several different schools of playing. The most prominent were the German/Viennese traditions and French school. The latter was centered on the clarinetists of the Conservatoire de Paris. The proliferation of recorded music has made examples of different styles of playing available. The modern clarinetist has a diverse palette of \"acceptable\" tone qualities to choose from.\nThe A and B clarinets have nearly the same bore and use the same mouthpiece. Orchestral clarinetists using the A and B instruments in a concert could use the same mouthpiece (and often the same barrel) (see 'usage' below). The A and B have nearly identical tonal quality, although the A typically has a slightly warmer sound. The tone of the E clarinet is brighter and can be heard even through loud orchestral or concert band textures. The bass clarinet has a characteristically deep, mellow sound, while the alto clarinet is similar in tone to the bass (though not as dark).\nRange.\nClarinets have the largest pitch range of common woodwinds. The intricate key organization that makes this possible can make the playability of some passages awkward. The bottom of the clarinet's written range is defined by the keywork on each instrument, standard keywork schemes allowing a low E on the common B clarinet. The lowest concert pitch depends on the transposition of the instrument in question. The nominal highest note of the B clarinet is a semitone higher than the highest note of the oboe but this depends on the setup and skill of the player. Since the clarinet has a wider range of notes, the lowest note of the B clarinet is significantly deeper (a minor or major sixth) than the lowest note of the oboe.\nNearly all soprano and piccolo clarinets have keywork enabling them to play the E below middle C as their lowest written note (in scientific pitch notation that sounds D3 on a soprano clarinet or C4, i.e. concert middle C, on a piccolo clarinet), though some B clarinets go down to E3 to enable them to match the range of the A clarinet. On the B soprano clarinet, the concert pitch of the lowest note is D3, a whole tone lower than the written pitch. Most alto and bass clarinets have an extra key to allow a (written) E3. Modern professional-quality bass clarinets generally have additional keywork to written C3. Among the less commonly encountered members of the clarinet family, contra-alto and contrabass clarinets may have keywork to written E3, D3, or C3; the basset clarinet and basset horn generally go to low C3.\nDefining the top end of a clarinet's range is difficult, since many advanced players can produce notes well above the highest notes commonly found in method books. G6 is usually the highest note clarinetists encounter in classical repertoire. The C above that (C7 i.e. resting on the fifth ledger line above the treble staff) is attainable by advanced players and is shown on many fingering charts, and fingerings as high as A7 exist.\nThe range of a clarinet can be divided into three distinct registers:\nAll three registers have characteristically different sounds. The chalumeau register is rich and dark. The clarion register is brighter and sweet, like a trumpet (\"clarion\") heard from afar. The altissimo register can be piercing and sometimes shrill.\nAcoustics.\nSound is a wave that propagates through the air as a result of a local variation in air pressure. The production of sound by a clarinet follows these steps:\nThe cycle repeats at a frequency relative to how long it takes a wave to travel to the first open hole and back twice (i.e. four times the length of the pipe). For example: when all the holes bar the very top one are open (i.e. the trill 'B' key is pressed), the note A4 (440 Hz) is produced. This represents a repeat of the cycle 440 times per second.\nIn addition to this primary compression wave, other waves, known as harmonics, are created. Harmonics are caused by factors including the imperfect wobbling and shaking of the reed, the reed sealing the mouthpiece opening for part of the wave cycle (which creates a flattened section of the sound wave), and imperfections (bumps and holes) in the bore. A wide variety of compression waves are created, but only some (primarily the odd harmonics) are reinforced. These extra waves are what gives the clarinet its characteristic tone.\nThe bore is cylindrical for most of the tube with an inner bore diameter between , but there is a subtle hourglass shape, with the thinnest part below the junction between the upper and lower joint. The reduction is depending on the maker. This hourglass shape, although invisible to the naked eye, helps to correct the pitch/scale discrepancy between the chalumeau and clarion registers (perfect twelfth). The diameter of the bore affects characteristics such as available harmonics, timbre, and pitch stability (how far the player can bend a note in the manner required in jazz and other music). The bell at the bottom of the clarinet flares out to improve the tone and tuning of the lowest notes.\nMost modern clarinets have \"undercut\" tone holes that improve intonation and sound. Undercutting means chamfering the bottom edge of tone holes inside the bore. Acoustically, this makes the tone hole function as if it were larger, but its main function is to allow the air column to follow the curve up through the tone hole (surface tension) instead of \"blowing past\" it under the increasingly directional frequencies of the upper registers.\nThe fixed reed and fairly uniform diameter of the clarinet give the instrument an acoustical behavior approximating that of a cylindrical stopped pipe. Recorders use a tapered internal bore to overblow at the octave when the thumb/register hole is pinched open, while the clarinet, with its cylindrical bore, overblows at the twelfth. Adjusting the angle of the bore taper controls the frequencies of the overblown notes (harmonics). Changing the mouthpiece's tip opening and the length of the reed changes aspects of the harmonic timbre or voice of the clarinet because this changes the speed of reed vibrations. Generally, the goal of the clarinetist when producing a sound is to make as much of the reed vibrate as possible, making the sound fuller, warmer, and potentially louder.\nThe lip position and pressure, shaping of the vocal tract, choice of reed and mouthpiece, amount of air pressure created, and evenness of the airflow account for most of the clarinetist's ability to control the tone of a clarinet. A highly skilled clarinetist will provide the ideal lip and air pressure for each frequency (note) being produced. They will have an embouchure which places an even pressure across the reed by carefully controlling their lip muscles. The airflow will also be carefully controlled by using the strong stomach muscles (as opposed to the weaker and erratic chest muscles) and they will use the diaphragm to oppose the stomach muscles to achieve a tone softer than a forte rather than weakening the stomach muscle tension to lower air pressure. Their vocal tract will be shaped to resonate at frequencies associated with the tone being produced.\nCovering or uncovering the tone holes varies the length of the pipe, changing the resonant frequencies of the enclosed air column and hence the pitch. A clarinetist moves between the chalumeau and clarion registers through use of the register key; clarinetists call the change from chalumeau register to clarion register \"the break\". The open register key stops the fundamental frequency from being reinforced, and the reed is forced to vibrate at three times the speed it was originally. This produces a note a twelfth above the original note.\nMost instruments overblow at two times the speed of the fundamental frequency (the octave), but as the clarinet acts as a closed pipe system, the reed cannot vibrate at twice its original speed because it would be creating a 'puff' of air at the time the previous 'puff' is returning as a rarefaction. This means it cannot be reinforced and so would die away. The chalumeau register plays fundamentals, whereas the clarion register, aided by the register key, plays third harmonics (a perfect twelfth higher than the fundamentals). The first several notes of the altissimo range, aided by the register key and venting with the first left-hand hole, play fifth harmonics (a major seventeenth, a perfect twelfth plus a major sixth, above the fundamentals). The clarinet is therefore said to overblow at the twelfth and, when moving to the altissimo register, seventeenth.\nBy contrast, nearly all other woodwind instruments overblow at the octave or (like the ocarina and tonette) do not overblow at all. A clarinet must have holes and keys for nineteen notes, a chromatic octave and a half from bottom E to B, in its lowest register to play the chromatic scale. This overblowing behavior explains the clarinet's great range and complex fingering system. The fifth and seventh harmonics are also available, sounding a further sixth and fourth (a flat, diminished fifth) higher respectively; these are the notes of the altissimo register. This is also why the inner \"waist\" measurement is so critical to these harmonic frequencies.\nThe highest notes can have a shrill, piercing quality and can be difficult to tune accurately. Different instruments often play differently in this respect due to the sensitivity of the bore and reed measurements. Using alternate fingerings and adjusting the embouchure helps correct the pitch of these notes.\nSince approximately 1850, clarinets have been nominally tuned according to twelve-tone equal temperament. Older clarinets were nominally tuned to meantone. Skilled performers can use their embouchures to considerably alter the tuning of individual notes or produce vibrato, a pulsating change of pitch often employed in jazz. Vibrato is rare in classical or concert band literature; however, certain clarinetists, such as Richard Stoltzman, use vibrato in classical music. Special fingerings may be used to play quarter tones and other microtonal intervals.\nAround 1900, Dr. Richard H. Stein, a Berlin musicologist, made a quarter-tone clarinet, which was soon abandoned. Years later, another German, Fritz Sch\u00fcller of Markneukirchen, built a quarter tone clarinet, with two parallel bores of slightly different lengths whose tone holes are operated using the same keywork and a valve to switch from one bore to the other.\nConstruction.\nMaterials.\nClarinet bodies have been made from a variety of materials including wood, plastic, hard rubber, metal, resin, and ivory. The vast majority of clarinets used by professionals are made from African hardwood, mpingo (African Blackwood) or grenadilla, rarely (because of diminishing supplies) Honduran rosewood, and sometimes even cocobolo. Historically other woods, notably boxwood, were used. Most inexpensive clarinets are made of plastic resin, such as ABS. \"Resonite\" is Selmer's trademark name for its type of plastic. Metal soprano clarinets were popular in the early 20th century until plastic instruments supplanted them; metal construction is still used for the bodies of some contra-alto and contrabass clarinets and the necks and bells of nearly all alto and larger clarinets. Ivory was used for a few 18th-century clarinets, but it tends to crack and does not keep its shape well. Buffet Crampon's Greenline clarinets are made from a composite of grenadilla wood powder and carbon fiber. Such clarinets are less affected by humidity and temperature changes than wooden instruments but are heavier. Hard rubber, such as ebonite, has been used for clarinets since the 1860s, although few modern clarinets are made of it. Clarinet designers Alastair Hanson and Tom Ridenour are strong advocates of hard rubber. The Hanson Clarinet Company manufactures clarinets using a grenadilla compound reinforced with ebonite, known as \"BTR\" (bithermal-reinforced) grenadilla. This material is also not affected by humidity, and the weight is the same as that of a wooden clarinet.\nMouthpieces are generally made of hard rubber, although some inexpensive mouthpieces may be made of plastic. Other materials such as crystal/glass, wood, ivory, and metal have also been used. Ligatures are often made of metal and plated in nickel, silver, or gold. Other materials include wire, wire mesh, plastic, naugahyde, string, or leather.\nReed.\nThe clarinet uses a single reed made from the cane of \"Arundo donax\", a type of grass. Reeds may also be manufactured from synthetic materials. The ligature fastens the reed to the mouthpiece. When air is blown through the opening between the reed and the mouthpiece facing, the reed vibrates and produces the clarinet's sound.\nBasic reed measurements are as follows: tip, wide; lay, long (distance from the place where the reed touches the mouthpiece to the tip); gap, (distance between the underside of the reed tip and the mouthpiece). Adjustment to these measurements is one method of affecting tone color.\nMost clarinetists buy manufactured reeds, although many make adjustments to these reeds, and some make their own reeds from cane \"blanks\". Reeds come in varying degrees of hardness, generally indicated on a scale from one (soft) through five (hard). This numbering system is not standardized\u2014reeds with the same number often vary in hardness across manufacturers and models. Reed and mouthpiece characteristics work together to determine ease of playability, pitch stability, and tonal characteristics.\nComponents.\nNote: A B\u00f6hm system soprano clarinet is shown in the photos illustrating this section. However, all modern clarinets have similar components.\nThe \"reed\" is attached to the \"mouthpiece\" by the \"ligature\", and the top half-inch or so of this assembly is held in the player's mouth. In the past, clarinetists used to wrap a string around the mouthpiece and reed instead of using a ligature. The formation of the mouth around the mouthpiece and reed is called the \"embouchure\".\nThe reed is on the underside of the mouthpiece, pressing against the player's lower lip, while the top teeth normally contact the top of the mouthpiece (some players roll the upper lip under the top teeth to form what is called a 'double-lip' embouchure). Adjustments in the strength and shape of the embouchure change the tone and intonation (tuning). It is not uncommon for clarinetists to employ methods to relieve the pressure on the upper teeth and inner lower lip by attaching pads to the top of the mouthpiece or putting (temporary) padding on the front lower teeth, commonly from folded paper.\nNext is the short \"barrel\"; this part of the instrument may be extended to fine-tune the clarinet. As the pitch of the clarinet is fairly temperature-sensitive, some instruments have interchangeable barrels whose lengths vary slightly. Additional compensation for pitch variation and tuning can be made by pulling out the barrel and thus increasing the instrument's length, particularly common in group playing in which clarinets are tuned to other instruments (such as in an orchestra or concert band). Some performers use a plastic barrel with a thumbwheel that adjusts the barrel length. On basset horns and lower clarinets, the barrel is normally replaced by a curved metal neck.\nThe main body of most clarinets is divided into the \"upper joint\", the holes and most keys of which are operated by the left hand, and the \"lower joint\" with holes and most keys operated by the right hand. Some clarinets have a single joint: on some basset horns and larger clarinets the two joints are held together with a screw clamp and are usually not disassembled for storage. The left thumb operates both a \"tone hole\" and the \"register key\". On some models of clarinet, such as many Albert system clarinets and increasingly some higher-end B\u00f6hm system clarinets, the register key is a 'wraparound' key, with the key on the back of the clarinet and the pad on the front. Advocates of the wraparound register key say it improves sound, and it is harder for moisture to accumulate in the tube beneath the pad. Nevertheless, there is a consensus among repair techs that this type of register key is harder to keep in adjustment, i.e., it is hard to have enough spring pressure to close the hole securely.\nThe body of a modern soprano clarinet is equipped with numerous \"tone holes\" of which seven (six front, one back) are covered with the fingertips, and the rest are opened or closed using a set of keys. These tone holes let the player produce every note of the chromatic scale. On alto and larger clarinets, and a few soprano clarinets, key-covered holes replace some or all finger holes. The most common system of keys was named the B\u00f6hm system by its designer Hyacinthe Klos\u00e9 in honour of flute designer Theobald B\u00f6hm, but it is not the same as the B\u00f6hm system used on flutes. The other main system of keys is called the Oehler system and is used mostly in Germany and Austria (see History). The related Albert system is used by some jazz, klezmer, and eastern European folk musicians. The Albert and Oehler systems are both based on the early Mueller system.\nThe cluster of keys at the bottom of the upper joint (protruding slightly beyond the cork of the joint) are known as the \"trill keys\" and are operated by the right hand. These give the player alternative fingerings that make it easy to play ornaments and trills. The entire weight of the smaller clarinets is supported by the right thumb behind the lower joint on what is called the \"thumb-rest\". Basset horns and larger clarinets are supported with a neck strap or a floor peg.\nFinally, the flared end is known as the \"bell\". Contrary to popular belief, the bell does not amplify the sound; rather, it improves the uniformity of the instrument's tone for the lowest notes in each register. For the other notes, the sound is produced almost entirely at the tone holes, and the bell is irrelevant. On basset horns and larger clarinets, the bell curves up and forward and is usually made of metal.\nKeywork.\nTheobald B\u00f6hm did not directly invent the key system of the clarinet. B\u00f6hm was a flautist who created the key system that is now used for the transverse flute. Klos\u00e9 and Buffet applied B\u00f6hm's system to the clarinet. Although the credit goes to those people, B\u00f6hm's name was given to that key system because it was based on that used for the flute.\nThe current B\u00f6hm key system consists of generally 6 rings, on the thumb, 1st, 2nd, 4th, 5th, and 6th holes, and a register key just above the thumb hole, easily accessible with the thumb. Above the 1st hole, there is a key that lifts two covers creating the note A in the throat register (high part of low register) of the clarinet. A key at the side of the instrument at the same height as the A key lifts only one of the two covers, producing G, a semitone lower. The A key can be used in conjunction solely with the register key to produce A/B.\nHistory.\nLineage.\nThe clarinet has its roots in the early single-reed instruments or hornpipes used in Ancient Greece, Ancient Egypt, Middle East, and Europe since the Middle Ages, such as the albogue, alboka, and double clarinet.\nThe modern clarinet developed from a Baroque instrument called the chalumeau. This instrument was similar to a recorder, but with a single-reed mouthpiece and a cylindrical bore. Lacking a register key, it was played mainly in its fundamental register, with a limited range of about one and a half octaves. It had eight finger holes, like a recorder, and two keys for its two highest notes. At this time, contrary to modern practice, the reed was placed in contact with the upper lip.\nAround the turn of the 18th century, the chalumeau was modified by converting one of its keys into a register key to produce the first clarinet. This development is usually attributed to German instrument maker Johann Christoph Denner, though some have suggested his son Jacob Denner was the inventor. This instrument played well in the middle register with a loud, shrill sound, so it was given the name \"clarinetto\" meaning \"little trumpet\" (from \"clarino\" + \"-etto\"). Early clarinets did not play well in the lower register, so players continued to play the chalumeaux for low notes. As clarinets improved, the chalumeau fell into disuse, and these notes became known as the \"chalumeau register\". Original Denner clarinets had two keys, and could play a chromatic scale, but various makers added more keys to get improved tuning, easier fingerings, and a slightly larger range. The classical clarinet of Mozart's day typically had eight finger holes and five keys.\nClarinets were soon accepted into orchestras. Later models had a mellower tone than the originals. Mozart (d. 1791) liked the sound of the clarinet (he considered its tone the closest in quality to the human voice) and wrote numerous pieces for the instrument., and by the time of Beethoven (c. 1800\u20131820), the clarinet was a standard fixture in the orchestra.\nPads.\nThe next major development in the history of clarinet was the invention of the modern pad. Because early clarinets used felt pads to cover the tone holes, they leaked air. This required pad-covered holes to be kept to a minimum, restricting the number of notes the clarinet could play with good tone. In 1812, Iwan M\u00fcller, a Baltic German community-born clarinetist and inventor, developed a new type of pad that was covered in leather or fish bladder. It was airtight and let makers increase the number of pad-covered holes. M\u00fcller designed a new type of clarinet with seven finger holes and thirteen keys. This allowed the instrument to play in any key with near-equal ease. Over the course of the 19th-century, makers made many enhancements to M\u00fcller's clarinet, such as the Albert system and the Baermann system, all keeping the same basic design. Modern instruments may also have cork or synthetic pads.\nArrangement of keys and holes.\nThe final development in the modern design of the clarinet used in most of the world today was introduced by Hyacinthe Klos\u00e9 in 1839. He devised a different arrangement of keys and finger holes, which allow simpler fingering. It was inspired by the Boehm system developed for flutes by Theobald B\u00f6hm. Klos\u00e9 was so impressed by B\u00f6hm's invention that he named his own system for clarinets the Boehm system, although it is different from the one used on flutes. This new system was slow to gain popularity but gradually became the standard, and today the Boehm system is used everywhere in the world except Germany and Austria. These countries still use a direct descendant of the Mueller clarinet known as the Oehler system clarinet. Also, some contemporary Dixieland players continue to use Albert system clarinets.\nOther key systems have been developed, many built around modifications to the basic B\u00f6hm system: Full B\u00f6hm, Mazzeo, McIntyre, Benade NX, and the Reform Boehm system for example. Each of these addressed\u2014and often improved\u2014issues of particular \"weak\" tones, or simplified awkward fingerings, but none has caught on widely among players, and the Boehm system remains the standard, to date.\nUsage and repertoire.\nUse of multiple clarinets.\nThe modern orchestral standard of using soprano clarinets in B and A has to do partly with the history of the instrument and partly with acoustics, aesthetics, and economics. Before about 1800, due to the lack of airtight pads \"(see History)\", practical woodwinds could have only a few keys to control accidentals (notes outside their diatonic home scales). The low (chalumeau) register of the clarinet spans a twelfth (an octave plus a perfect fifth), so the clarinet needs keys/holes to produce all nineteen notes in this range. This involves more keywork than on instruments that \"overblow\" at the octave\u2014oboes, flutes, bassoons, and saxophones, for example, which need only twelve notes before overblowing. Clarinets with few keys cannot therefore easily play chromatically, limiting any such instrument to a few closely related keys. For example, an eighteenth-century clarinet in C could be played in F, C, and G (and their relative minors) with good intonation, but with progressive difficulty and poorer intonation as the key moved away from this range. In contrast, for octave-overblowing instruments, an instrument in C with few keys could much more readily be played in any key. This problem was overcome by using three clarinets\u2014in A, B, and C\u2014so that early 19th-century music, which rarely strayed into the remote keys (five or six sharps or flats), could be played as follows: music in 5 to 2 sharps (B major to D major concert pitch) on A clarinet (D major to F major for the player), music in 1 sharp to 1 flat (G to F) on C clarinet, and music in 2 flats to 4 flats (B to A) on the B clarinet (C to B for the clarinetist). Difficult key signatures and numerous accidentals were thus largely avoided.\nWith the invention of the airtight pad, and as key technology improved and more keys were added to woodwinds, the need for clarinets in multiple keys was reduced. However, the use of multiple instruments in different keys persisted, with the three instruments in C, B, and A all used as specified by the composer.\nThe lower-pitched clarinets sound \"mellower\" (less bright), and the C clarinet\u2014being the highest and therefore brightest of the three\u2014fell out of favour as the other two could cover its range and their sound was considered better. While the clarinet in C began to fall out of general use around 1850, some composers continued to write C parts after this date, e.g., Bizet's Symphony in C (1855), Tchaikovsky's Symphony No. 2 (1872), Smetana's overture to \"The Bartered Bride\" (1866) and \"M\u00e1 Vlast\" (1874), Dvo\u0159\u00e1k's \"Slavonic Dance\" Op. 46, No. 1 (1878), Brahms' Symphony No.\u00a04 (1885), Mahler's Symphony No. 6 (1906), and Richard Strauss deliberately reintroduced it to take advantage of its brighter tone, as in \"Der Rosenkavalier\" (1911).\nWhile technical improvements and an equal-tempered scale reduced the need for two clarinets, the technical difficulty of playing in remote keys persisted, and the A has thus remained a standard orchestral instrument. In addition, by the late 19th century, the orchestral clarinet repertoire contained so much music for clarinet in A that the disuse of this instrument was not practical. Attempts were made to standardise to the B instrument between 1930 and 1950 (e.g., tutors recommended learning routine transposition of orchestral A parts on the B clarinet, including solos written for A clarinet, and some manufacturers provided a low E on the B to match the range of the A), but this failed in the orchestral sphere.\nSimilarly there have been E and D instruments in the upper soprano range, B, A, and C instruments in the bass range, and so forth; but over time the E and B instruments have become predominant. The B instrument remains dominant in concert bands and jazz. B and C instruments are used in some ethnic traditions, such as klezmer.\nClassical music.\nIn classical music, clarinets are part of standard orchestral and concert band instrumentation.\nThe orchestra frequently includes two clarinetists playing individual parts\u2014each player is usually equipped with a pair of standard clarinets in B and A, and clarinet parts commonly alternate between B and A instruments several times over the course of a piece, or less commonly, a movement (e.g., 1st movement Brahms' 3rd symphony). Clarinet sections grew larger during the last few decades of the 19th century, often employing a third clarinetist, an E or a bass clarinet. In the 20th century, composers such as Igor Stravinsky, Richard Strauss, Gustav Mahler, and Olivier Messiaen enlarged the clarinet section on occasion to up to nine players, employing many different clarinets including the E or D soprano clarinets, basset horn, alto clarinet, bass clarinet, and/or contrabass clarinet.\nIn concert bands, clarinets are an important part of the instrumentation. The E clarinet, B clarinet, alto clarinet, bass clarinet, and contra-alto/contrabass clarinet are commonly used in concert bands. Concert bands generally have multiple B clarinets; there are commonly 3 B clarinet parts with 2\u20133 players per part. There is generally only one player per part on the other clarinets. There are not always E clarinet, alto clarinet, and contra-alto clarinets/contrabass clarinet parts in concert band music, but all three are quite common.\nThis practice of using a variety of clarinets to achieve coloristic variety was common in 20th-century classical music and continues today. However, many clarinetists and conductors prefer to play parts originally written for obscure instruments on B or E clarinets, which are often of better quality and more prevalent and accessible.\nThe clarinet is widely used as a solo instrument. The relatively late evolution of the clarinet (when compared to other orchestral woodwinds) has left solo repertoire from the Classical period and later, but few works from the Baroque era. Many clarinet concertos have been written to showcase the instrument, with the concerti by Mozart, Copland, and Weber being well known.\nMany works of chamber music have also been written for the clarinet. Common combinations are:\nJazz.\nThe clarinet was originally a central instrument in jazz, beginning with the New Orleans players in the 1910s. It remained a signature instrument of jazz music through much of the big band era into the 1940s. American players Alphonse Picou, Larry Shields, Jimmie Noone, Johnny Dodds, and Sidney Bechet were all pioneers of the instrument in jazz. The B soprano was the most common instrument, but a few early jazz musicians such as Louis Nelson Delisle and Alcide Nunez preferred the C soprano, and many New Orleans jazz brass bands have used E soprano.\nSwing clarinetists such as Benny Goodman, Artie Shaw, and Woody Herman led successful big bands and smaller groups from the 1930s onward. Duke Ellington, active from the 1920s to the 1970s, used the clarinet as lead instrument in his works, with several players of the instrument (Barney Bigard, Jimmy Hamilton, and Russell Procope) spending a significant portion of their careers in his orchestra. Harry Carney, primarily Ellington's baritone saxophonist, occasionally doubled on bass clarinet. Meanwhile, Pee Wee Russell had a long and successful career in small groups.\nWith the decline of the big bands' popularity in the late 1940s, the clarinet faded from its prominent position in jazz. By that time, an interest in Dixieland or traditional New Orleans jazz had revived; Pete Fountain was one of the best known performers in this genre. Bob Wilber, active since the 1950s, is a more eclectic jazz clarinetist, playing in several classic jazz styles. During the 1950s and 1960s, Britain underwent a surge in the popularity of what was termed 'Trad jazz'. In 1956 the British clarinetist Acker Bilk founded his own ensemble. Several singles recorded by Bilk reached the British pop charts, including the ballad \"Stranger on the Shore\".\nThe clarinet's place in the jazz ensemble was usurped by the saxophone, which projects a more powerful sound and uses a less complicated fingering system. The requirement for an increased speed of execution in modern jazz also did not favour the clarinet, but the clarinet did not entirely disappear. The clarinetist Stan Hasselg\u00e5rd made a transition from swing to bebop in the mid-1940s. A few players such as Buddy DeFranco, Tony Scott, and Jimmy Giuffre emerged during the 1950s playing bebop or other styles. A little later, Eric Dolphy (on bass clarinet), Perry Robinson, John Carter, Theo J\u00f6rgensmann, and others used the clarinet in free jazz. The French composer and clarinetist Jean-Christian Michel initiated a jazz-classical cross-over on the clarinet with the drummer Kenny Clarke.\nIn the U.S., the prominent players on the instrument since the 1980s have included Eddie Daniels, Don Byron, Marty Ehrlich, Ken Peplowski, and others playing the clarinet in more contemporary contexts.\nOther genres.\nThe clarinet is uncommon, but not unheard of, in rock music. Jerry Martini played clarinet on Sly and the Family Stone's 1968 hit, \"Dance to the Music\"; Don Byron, a founder of the Black Rock Coalition who was a member of hard rock guitarist Vernon Reid's band, plays clarinet on the \"Mistaken Identity\" album (1996). The Beatles, Pink Floyd, Radiohead, Aerosmith, Billy Joel, and Tom Waits have also all used clarinet on occasion. A clarinet is prominently featured for two different solos in \"Breakfast in America\", the title song from the Supertramp album of the same name.\nClarinets feature prominently in klezmer music, which entails a distinctive style of playing. The use of quarter-tones requires a different embouchure. Some klezmer musicians prefer Albert system clarinets.\nThe popular Brazilian music styles of choro and samba use the clarinet. Prominent contemporary players include Paulo Moura, Naylor 'Proveta' Azevedo, Paulo S\u00e9rgio dos Santos, and Cuban born Paquito D'Rivera.\nEven though it has been adopted recently in Albanian folklore (around the 18th century), the clarinet, or \"g\u00ebrneta\" as it is called, is one of the most important instruments in Albania, especially in the central and southern areas. The clarinet plays a crucial role in \"saze\" (folk) ensembles that perform in weddings and other celebrations. It is worth mentioning that the \"kaba\" (an instrumental Albanian Isopolyphony included in UNESCO's intangible cultural heritage list) is characteristic of these ensembles. Prominent Albanian clarinet players include Selim Leskoviku, Gaqo Lena, Remzi Lela (\u00c7obani), Laver Bariu (Ustai), and Nevruz Nure (Lulushi i Kor\u00e7\u00ebs).\nThe clarinet is prominent in Bulgarian wedding music as an offshoot of Roma/Romani traditional music. Ivo Papazov is a well-known clarinetist in this genre. In Moravian dulcimer bands, the clarinet is usually the only wind instrument among string instruments.\nIn old-town folk music in North Macedonia (called \u010dalgija (\"\u0447\u0430\u043b\u0433\u0438\u0458\u0430\")), the clarinet has the most important role in wedding music; clarinet solos mark the high point of dancing euphoria. One of the most renowned Macedonian clarinet players is Tale Ognenovski, who gained worldwide fame for his virtuosity.\nIn Greece, the clarinet (usually referred to as \"\u03ba\u03bb\u03b1\u03c1\u03af\u03bd\u03bf\"\u2014\"clarino\") is prominent in traditional music, especially in central, northwest, and northern Greece (Thessaly, Epirus, and Macedonia). The double-reed zurna was the dominant woodwind instrument before the clarinet arrived in the country, although many Greeks regard the clarinet as a native instrument. Traditional dance music, wedding music, and laments include a clarinet soloist and quite often improvisations. Petroloukas Chalkias is a famous clarinetist in this genre.\nThe instrument is equally famous in Turkey, especially the lower-pitched clarinet in G. The western European clarinet crossed via Turkey to Arabic music, where it is widely used in Arabic pop, especially if the intention of the arranger is to imitate the Turkish style.\nAlso in Turkish folk music, a clarinet-like woodwind instrument, the sipsi, is used. However, it is far more rare than the soprano clarinet and is mainly limited to folk music of the Aegean Region.\nGroups of clarinets.\nGroups of clarinets playing together have become increasingly popular among clarinet enthusiasts in recent years. Common forms are:\nClarinet choirs and quartets often play arrangements of both classical and popular music, in addition to a body of literature specially written for a combination of clarinets by composers such as Arnold Cooke, Alfred Uhl, Lucien Caillet, and V\u00e1clav Nelh\u00fdbel.\nExtended family of clarinets.\nThere is a family of many differently pitched clarinet types, some of which are very rare. The following are the most important sizes, from highest to lowest:\nEEE and BBB octocontra-alto and octocontrabass clarinets have also been built. There have also been soprano clarinets in C, A, and B with curved barrels and bells marketed under the names saxonette, claribel, and clariphon."}
{"id": "6434", "revid": "21035594", "url": "https://en.wikipedia.org/wiki?curid=6434", "title": "Chojn\u00f3w", "text": "Chojn\u00f3w (, Silesian German: Hoyn) is a small town in Legnica County, Lower Silesian Voivodeship, in south-western Poland. It is located on the Skora river, a tributary of the Kaczawa at an average altitude of above sea level. Chojn\u00f3w is the administrative seat of the rural gmina called Gmina Chojn\u00f3w, although the town is not part of its territory and forms a separate urban gmina. it had 13,355 inhabitants.\nChojn\u00f3w is located west of Legnica, east from Boles\u0142awiec and north of Z\u0142otoryja, from the A4 motorway. It has railroad connections to Boles\u0142awiec and Legnica.\nHeraldry.\nCoat of arms of the Chojn\u00f3w has is a blue escutcheon. On the dial there is a tower with three bastions of white colour. The central tower has two Windows, and one side. On the towers is located on the right side of the Moon and Sun on the left. In the gate of the Silesian Eagle on a yellow background.\nThe Motto of Chojn\u00f3w is \"Friendly City\".\nGeography.\nChojn\u00f3w is located in the Central-Western part of the Lower Silesia region. The Skora (Leather) River flows through the town in a westerly direction. The city of Chojn\u00f3w is in area, including 41% agricultural land.\nChojn\u00f3w has a connection with the major cities of the country (road and rail) and located south of Chojn\u00f3w has the A4 Autostrada. To the South of the town is the surrounding Chojnowska Plain.\nHistory.\nThe town is first mentioned in a Latin mediaeval document issued in Wroc\u0142aw on February 26, 1253, stating, the Silesian Duke Henry III when the town is mentioned under the name Honowo. Possible the name of nearby Hainau Island. The name is of Polish origin, and in more modern records from the 19th century, the Polish name appears as \"Hajn\u00f3w\", while \"Haynau\" is the Germanized version of the original Polish name.\nThe settlement of \"Haynow\" was mentioned in a 1272 deed. It was already called a \"civitas\" in a 1288 document issued by the Piast duke Henry V of Legnica, and officially received town privileges in 1333 from Duke Boles\u0142aw III the Generous. It was part of the duchies of Wroc\u0142aw, G\u0142og\u00f3w and Legnica of fragmented Poland and remained under the rule of the Piast dynasty until 1675. Its population was predominantly Polish. In 1292 the first castellan of Chojn\u00f3w, Bronis\u0142aw Budziwojowic, was mentioned. In the 14th and early 15th centuries Chojn\u00f3w was granted various privileges, including staple right and gold mining right, thanks to which it flourished.\nThe town survived the Hussites, who burned almost the entire town center and castle, but it quickly helped recover its former glory. The largest boom Chojn\u00f3w experienced was in the 16th century, however by the end of that century began to decline due to fires and epidemic, which claimed many victims in 1613. During the Thirty Years War (1618\u20131648), there was another outbreak in the city, it was occupied by the Austrians and Swedes and in 1642 it was also plundered by the Swedes. It remained part of the Piast-ruled Duchy of Legnica until its dissolution in 1675, when it was incorporated to Habsburg-ruled Bohemia.\nIn the 18th century, cloth production developed and a clothmaking school was established in the town. One of two main routes connecting Warsaw and Dresden ran through the town in the 18th century and Kings Augustus II the Strong and Augustus III of Poland traveled that route numerous times. In 1740 the town was captured by Prussia and subsequently annexed in 1742. In 1804 it suffered a flood. During the Napoleonic wars there were more epidemics. In 1813 in Chojn\u00f3w, Napoleon Bonaparte issued instructions regarding the reorganization of the 8th Polish Corps of Prince J\u00f3zef Poniatowski. The event is commemorated by a plaque in the facade of the Piast Castle. A railway line was opened in the 19th century. Sewer, Gas lighting a Newspaper and a hospital soon followed as the towns economy improved.\nThe city was not spared in World War II, with 30% of the town being destroyed on February 10, 1945 when Soviet Red Army troops took the abandoned town. After World War II and the implementation of the Oder-Neisse line in 1945, the town passed to the Republic of Poland. It was repopulated by Poles, expelled from former eastern Poland annexed by the Soviet Union. In 1946 it was renamed \"Chojn\u00f3w\", a more modern version of the old Polish \"Hajn\u00f3w\". Also Greeks, refugees of the Greek Civil War, settled in Chojn\u00f3w.\nEconomy.\nChojn\u00f3w is an industrial and agricultural town. Among local products are: paper, agricultural machinery, chains, metal furniture for hospitals, equipment for the meat industry, beer, wine, leather clothing, and clothing for infants, children and adults.\nSights and nature.\nAmong the interesting monuments of Chojn\u00f3w are the 13th-century castle of the Dukes of Legnica (currently used as a museum), two old churches, the \"Baszta Tkaczy\" (\"Weavers' Tower\") and preserved fragments of city walls.\nThe biggest green area in Chojn\u00f3w is small forest \"Park Piastowski\" (\"Piast's Park\"), named after Piast dynasty. Wild animals that can be found in the Chojn\u00f3w area are roe deer, foxes, rabbits and wild domestic animals, especially cats.\nCulture and sport.\nEvery year in the first days of June, the \"Days of Chojn\u00f3w\" (\"Dni Chojnowa\") are celebrated. The Whole-Poland bike race \"Masters\" has been organized yearly in Chojn\u00f3w for the past few years.\nChojn\u00f3w has a Municipal sports and recreation center formed in 2008 holding various events, festivals, reviews, exhibitions, and competitions. The regional Museum is housed in the old Piast era castle. The collections include tiles, relics, and the castle garden. Next to the Museum there is a municipal library. In \u015br\u00f3dmiejskim Park, near the Town Hall is the amphitheatre.\nThe local government-run weekly newspaper is Gazeta Chojnowska, which has been published since 1992.\nIt is published biweekly. Editions have a run of 900 copies and it is one of the oldest newspapers in Poland issued without interruption. The \"Chojn\u00f3w\" is the official newspaper of Chojn\u00f3w with copy run of 750 copies.\nEducation.\nIn Chojn\u00f3w, there are two kindergartens, two elementary schools and two middle schools.\nReligion.\nChojn\u00f3w is in the Catholic deanery of Chojn\u00f3w and has two parishes, Immaculate Conception of the Blessed Virgin Mary and also the Holy Apostles Peter and Paul. Both parishes have active congregations.\nThere are also two Congregations of Jehovah's witnesses.\nTwin towns \u2013 sister cities.\nChojn\u00f3w is twinned with:"}
{"id": "6435", "revid": "23646674", "url": "https://en.wikipedia.org/wiki?curid=6435", "title": "Canes Venatici", "text": "Canes Venatici is one of the 88 official modern constellations. It is a small northern constellation that was created by Johannes Hevelius in the 17th century. Its name is Latin for 'hunting dogs', and the constellation is often depicted in illustrations as representing the dogs of Bo\u00f6tes the Herdsman, a neighboring constellation.\nCor Caroli is the constellation's brightest star, with an apparent magnitude of 2.9. La Superba (Y\u00a0CVn) is one of the reddest naked-eye stars and one of the brightest carbon stars. The Whirlpool Galaxy is a spiral galaxy tilted face-on to observers on Earth, and was the first galaxy whose spiral nature was discerned. In addition, TON 618 has one of the most massive black holes with the mass of 66 billion solar masses.\nHistory.\nThe stars of Canes Venatici are not bright. In classical times, they were listed by Ptolemy as unfigured stars below the constellation Ursa Major in his star catalogue. \nIn medieval times, the identification of these stars with the dogs of Bo\u00f6tes arose through a mistranslation: some of Bo\u00f6tes's stars were traditionally described as representing the club (, ) of Bo\u00f6tes. When the Greek astronomer Ptolemy's \"Almagest\" was translated from Greek to Arabic, the translator Hunayn ibn Ishaq did not know the Greek word and rendered it as a similar-sounding compound Arabic word for a kind of weapon, writing , which means 'the staff having a hook'.\nWhen the Arabic text was later translated into Latin, the translator, Gerard of Cremona, mistook ('hook') for ('dogs'). Both written words look the same in Arabic text without diacritics, leading Gerard to write it as ('spearshaft-having dogs').\nIn 1533, the German astronomer Peter Apian depicted Bo\u00f6tes as having two dogs with him.\nThese spurious dogs floated about the astronomical literature until Hevelius decided to make them a separate constellation in 1687. Hevelius chose the name \"Asterion\" for the northern dog and \"Chara\" for the southern dog, as , 'the hunting dogs', in his star atlas.\nIn his star catalogue, the Czech astronomer Anton\u00edn Be\u010dv\u00e1\u0159 assigned the names \"Asterion\" to \u03b2\u00a0CVn and \"Chara\" to \u03b1\u00a0CVn.\nAlthough the International Astronomical Union dropped several constellations in 1930 that were medieval and Renaissance innovations, Canes Venatici survived to become one of the 88 IAU designated constellations.\nNeighbors and borders.\nCanes Venatici is bordered by Ursa Major to the north and west, Coma Berenices to the south, and Bo\u00f6tes to the east. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is \"CVn\". The official constellation boundaries, as set by Belgian astronomer Eug\u00e8ne Delporte in 1930, are defined by a polygon of 14 sides.\nIn the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between +27.84\u00b0 and +52.36\u00b0. Covering 465 square degrees, it ranks 38th of the 88 constellations in size.\nProminent stars and deep-sky objects.\nStars.\nCanes Venatici contains no bright stars, Alpha and Beta Canum Venaticorum being only of 3rd and 4th magnitude respectively. Flamsteed catalogued 25 stars in the constellation, labelling them 1 to 25 Canum Venaticorum (CVn); however, 1CVn turned out to be in Ursa Major, 13CVn was in Coma Berenices, and 22CVn did not exist.\nSupervoid.\nThe Giant Void, an extremely large void (part of the universe containing very few galaxies), is within the vicinity of this constellation. It may be possibly the largest void ever discovered, slightly larger than the Eridanus Supervoid and 1,200 times the volume of expected typical voids. It was discovered in 1988 in a deep-sky survey.\nDeep-sky objects.\nCanes Venatici contains five Messier objects, including four galaxies. One of the more significant galaxies in Canes Venatici is the Whirlpool Galaxy (M51, NGC\u00a05194) and NGC 5195, a small barred spiral galaxy that is seen face-on. This was the first galaxy recognised as having a spiral structure, this structure being first observed by Lord Rosse in 1845. It is a face-on spiral galaxy 37\u00a0million light-years from Earth. Widely considered to be one of the most beautiful galaxies visible, M51 has many star-forming regions and nebulae in its arms, coloring them pink and blue in contrast to the older yellow core. M\u00a051 has a smaller companion, NGC\u00a05195, that has very few star-forming regions and thus appears yellow. It is passing behind M\u00a051 and may be the cause of the larger galaxy's prodigious star formation.\nOther notable spiral galaxies in Canes Venatici are the Sunflower Galaxy (M63, NGC\u00a05055), M94 (NGC\u00a04736), and M106 (NGC\u00a04258).\nTON 618 is a hyperluminous quasar and blazar in this constellation, near its border with the neighboring Coma Berenices. It possesses a black hole with a mass 66 billion times that of our Sun, making it one of the most massive black holes ever measured."}
{"id": "6436", "revid": "8240947", "url": "https://en.wikipedia.org/wiki?curid=6436", "title": "Chamaeleon", "text": "Chamaeleon () is a small constellation in the southern sky. It is named after the chameleon, a kind of lizard. It was first defined in the 16th century.\nHistory.\nChamaeleon was one of twelve constellations created by Petrus Plancius from the observations of Pieter Dirkszoon Keyser and Frederick de Houtman. It first appeared on a 35-cm diameter celestial globe published in 1597 (or 1598) in Amsterdam by Plancius and Jodocus Hondius. Johann Bayer was the first uranographer to put Chamaeleon in a celestial atlas. It was one of many constellations created by European explorers in the 15th and 16th centuries out of unfamiliar Southern Hemisphere stars.\nFeatures.\nStars.\nThere are four bright stars in Chamaeleon that form a compact diamond-shape approximately 10 degrees from the South Celestial Pole and about 15 degrees south of Acrux, along the axis formed by Acrux and Gamma Crucis. Alpha Chamaeleontis is a white-hued star of magnitude 4.1, 63 light-years from Earth. Beta Chamaeleontis is a blue-white hued star of magnitude 4.2, 271 light-years from Earth. Gamma Chamaeleontis is a red-hued giant star of magnitude 4.1, 413 light-years from Earth. The other bright star in Chamaeleon is Delta Chamaeleontis, a wide double star. The brighter star is Delta2 Chamaeleontis, a blue-hued star of magnitude 4.4. Delta1 Chamaeleontis, the dimmer component, is an orange-hued giant star of magnitude 5.5. They both lie about 350 light years away.\nChamaeleon is also the location of Cha 110913, a unique dwarf star or proto solar system.\nDeep-sky objects.\nIn 1999, a nearby open cluster was discovered centered on the star \u03b7 Chamaeleontis. The cluster, known as either\nthe Eta Chamaeleontis cluster or Mamajek 1, is 8 million years old, and lies 316 light years from Earth.\nThe constellation contains a number of molecular clouds (the Chamaeleon dark clouds) that are forming low-mass T Tauri stars. The cloud complex lies some 400 to 600 light years from Earth, and contains tens of thousands of solar masses of gas and dust. The most prominent cluster of T Tauri stars and young B-type stars are in the Chamaeleon I cloud, and are associated with the reflection nebula IC 2631.\nChamaeleon contains one planetary nebula, NGC 3195, which is fairly faint. It appears in a telescope at about the same apparent size as Jupiter.\nEquivalents.\nIn Chinese astronomy, the stars that form Chamaeleon were classified as the Little Dipper (\u5c0f\u6597, \"Xi\u01ceod\u01d2u\") among the Southern Asterisms (\u8fd1\u5357\u6975\u661f\u5340, \"J\u00ecnn\u00e1nj\u00edx\u012bng\u014du\") by Xu Guangqi. Chamaeleon is sometimes also called the Frying Pan in Australia."}
{"id": "6437", "revid": "35608441", "url": "https://en.wikipedia.org/wiki?curid=6437", "title": "Cholesterol", "text": "Cholesterol (from the Ancient Greek \"chole-\" (bile) and \"stereos\" (solid), followed by the chemical suffix \"-ol\" for an alcohol) is an organic molecule. It is a sterol (or modified steroid), a type of lipid. Cholesterol is biosynthesized by all animal cells and is an essential structural component of animal cell membranes. It is a yellowish crystalline solid.\nCholesterol also serves as a precursor for the biosynthesis of steroid hormones, bile acid and vitamin D. Cholesterol is the principal sterol synthesized by all animals. In vertebrates, hepatic cells typically produce the greatest amounts. It is absent among prokaryotes (bacteria and archaea), although there are some exceptions, such as \"Mycoplasma\", which require cholesterol for growth.\nFran\u00e7ois Poulletier de la Salle first identified cholesterol in solid form in gallstones in 1769. However, it was not until 1815 that chemist Michel Eug\u00e8ne Chevreul named the compound \"cholesterine\".\nPhysiology.\nCholesterol is essential for all animal life, with each cell capable of synthesizing it by way of a complex 37-step process. This begins with the mevalonate or HMG-CoA reductase pathway, the target of statin drugs, which encompasses the first 18 steps. This is followed by 19 additional steps to convert the resulting lanosterol into cholesterol.\nA human male weighing 68\u00a0kg (150\u00a0lb) normally synthesizes about 1 gram (1,000\u00a0mg) of cholesterol per day, and his body contains about 35 g, mostly contained within the cell membranes. Typical daily cholesterol dietary intake for a man in the United States is 307\u00a0mg.\nMost ingested cholesterol is esterified, which causes it to be poorly absorbed by the gut. The body also compensates for absorption of ingested cholesterol by reducing its own cholesterol synthesis. For these reasons, cholesterol in food, seven to ten hours after ingestion, has little, if any effect on concentrations of cholesterol in the blood. However, during the first seven hours after ingestion of cholesterol, as absorbed fats are being distributed around the body within extracellular water by the various lipoproteins (which transport all fats in the water outside cells), the concentrations increase.\nPlants make cholesterol in very small amounts. In larger quantities they produce phytosterols, chemically similar substances which can compete with cholesterol for reabsorption in the intestinal tract, thus potentially reducing cholesterol reabsorption. When intestinal lining cells absorb phytosterols, in place of cholesterol, they usually excrete the phytosterol molecules back into the GI tract, an important protective mechanism. The intake of naturally occurring phytosterols, which encompass plant sterols and stanols, ranges between \u2248200\u2013300\u00a0mg/day depending on eating habits. Specially designed vegetarian experimental diets have been produced yielding upwards of 700\u00a0mg/day.\nFunction in cells.\nMembranes.\nCholesterol composes about 30% of all animal cell membranes. It is required to build and maintain membranes and modulates membrane fluidity over the range of physiological temperatures. The hydroxyl group of each cholesterol molecule interacts with water molecules surrounding the membrane, as do the polar heads of the membrane phospholipids and sphingolipids, while the bulky steroid and the hydrocarbon chain are embedded in the membrane, alongside the nonpolar fatty-acid chain of the other lipids. Through the interaction with the phospholipid fatty-acid chains, cholesterol increases membrane packing, which both alters membrane fluidity and maintains membrane integrity so that animal cells do not need to build cell walls (like plants and most bacteria). The membrane remains stable and durable without being rigid, allowing animal cells to change shape and animals to move.\nThe structure of the tetracyclic ring of cholesterol contributes to the fluidity of the cell membrane, as the molecule is in a \"trans\" conformation making all but the side chain of cholesterol rigid and planar. In this structural role, cholesterol also reduces the permeability of the plasma membrane to neutral solutes, hydrogen ions, and sodium ions.\nGates.\nWithin the cell membrane, cholesterol also functions in intracellular transport, cell signaling and nerve conduction. Cholesterol is essential for the structure and function of invaginated caveolae and clathrin-coated pits, including caveola-dependent and clathrin-dependent endocytosis. The role of cholesterol in endocytosis of these types can be investigated by using methyl beta cyclodextrin (M\u03b2CD) to remove cholesterol from the plasma membrane.\nSubstrate presentation.\nCholesterol regulates the biological process of substrate presentation and the enzymes that use substrate presentation as a mechanism of their activation. (PLD2) is a well-defined example of an enzyme activated by substrate presentation. The enzyme is palmitoylated causing the enzyme to traffic to cholesterol dependent lipid domains sometimes called \"lipid rafts\". The substrate of phospholipase D is phosphatidylcholine (PC) which is unsaturated and is of low abundance in lipid rafts. PC localizes to the disordered region of the cell along with the polyunsaturated lipid phosphatidylinositol 4,5-bisphosphate (PIP2). PLD2 has a PIP2 binding domain. When PIP2 concentration in the membrane increases, PLD2 leaves the cholesterol dependent domains and binds to PIP2 where it then gains access to its substrate PC and commences catalysis based on substrate presentation.\nSignaling.\nCholesterol is also implicated in cell signaling processes, assisting in the formation of lipid rafts in the plasma membrane, which brings receptor proteins in close proximity with high concentrations of second messenger molecules. In multiple layers, cholesterol and phospholipids, both electrical insulators, can facilitate speed of transmission of electrical impulses along nerve tissue. For many neuron fibers, a myelin sheath, rich in cholesterol since it is derived from compacted layers of Schwann cell membrane, provides insulation for more efficient conduction of impulses. Demyelination (loss of some of these Schwann cells) is believed to be part of the basis for multiple sclerosis.\nCholesterol binds to and affects the gating of a number of ion channels such as the nicotinic acetylcholine receptor, GABAA receptor, and the inward-rectifier potassium channel. Cholesterol also activates the estrogen-related receptor alpha (ERR\u03b1), and may be the endogenous ligand for the receptor. The constitutively active nature of the receptor may be explained by the fact that cholesterol is ubiquitous in the body. Inhibition of ERR\u03b1 signaling by reduction of cholesterol production has been identified as a key mediator of the effects of statins and bisphosphonates on bone, muscle, and macrophages. On the basis of these findings, it has been suggested that the ERR\u03b1 should be de-orphanized and classified as a receptor for cholesterol.\nChemical precursor.\nWithin cells, cholesterol is also a precursor molecule for several biochemical pathways. For example, it is the precursor molecule for the synthesis of vitamin D in the calcium metabolism and all steroid hormones, including the adrenal gland hormones cortisol and aldosterone, as well as the sex hormones progesterone, estrogens, and testosterone, and their derivatives.\nMetabolism.\nCholesterol is recycled in the body. The liver excretes cholesterol into biliary fluids, which are then stored in the gallbladder, which then excretes them in a non-esterified form (via bile) into the digestive tract. Typically, about 50% of the excreted cholesterol is reabsorbed by the small intestine back into the bloodstream.\nBiosynthesis and regulation.\nBiosynthesis.\nAll animal cells manufacture cholesterol, for both membrane structure and other uses, with relative production rates varying by cell type and organ function. About 80% of total daily cholesterol production occurs in the liver and the intestines; other sites of higher synthesis rates include the brain, the adrenal glands, and the reproductive organs.\nSynthesis within the body starts with the mevalonate pathway where two molecules of acetyl CoA condense to form acetoacetyl-CoA. This is followed by a second condensation between acetyl CoA and acetoacetyl-CoA to form 3-hydroxy-3-methylglutaryl CoA (HMG-CoA). \nThis molecule is then reduced to mevalonate by the enzyme HMG-CoA reductase. Production of mevalonate is the rate-limiting and irreversible step in cholesterol synthesis and is the site of action for statins (a class of cholesterol-lowering drugs).\nMevalonate is finally converted to isopentenyl pyrophosphate (IPP) through two phosphorylation steps and one decarboxylation step that requires ATP.\nThree molecules of isopentenyl pyrophosphate condense to form farnesyl pyrophosphate through the action of geranyl transferase.\nTwo molecules of farnesyl pyrophosphate then condense to form squalene by the action of squalene synthase in the endoplasmic reticulum. \nOxidosqualene cyclase then cyclizes squalene to form lanosterol.\nFinally, lanosterol is converted to cholesterol via either of two pathways, the Block pathway, or the Kandutsch-Russell pathway.\nThe final 19 steps to cholesterol contain NADPH and oxygen to help oxidize methyl groups for removal of carbons, mutases to move alkene groups, and NADH to help reduce ketones.\nKonrad Bloch and Feodor Lynen shared the Nobel Prize in Physiology or Medicine in 1964 for their discoveries concerning some of the mechanisms and methods of regulation of cholesterol and fatty acid metabolism.\nRegulation of cholesterol synthesis.\nBiosynthesis of cholesterol is directly regulated by the cholesterol levels present, though the homeostatic mechanisms involved are only partly understood. A higher intake from food leads to a net decrease in endogenous production, whereas lower intake from food has the opposite effect. The main regulatory mechanism is the sensing of intracellular cholesterol in the endoplasmic reticulum by the protein SREBP (sterol regulatory element-binding protein 1 and 2). In the presence of cholesterol, SREBP is bound to two other proteins: SCAP (SREBP cleavage-activating protein) and INSIG-1. When cholesterol levels fall, INSIG-1 dissociates from the SREBP-SCAP complex, which allows the complex to migrate to the Golgi apparatus. Here SREBP is cleaved by S1P and S2P (site-1 protease and site-2 protease), two enzymes that are activated by SCAP when cholesterol levels are low.\nThe cleaved SREBP then migrates to the nucleus, and acts as a transcription factor to bind to the sterol regulatory element (SRE), which stimulates the transcription of many genes. Among these are the low-density lipoprotein (LDL) receptor and HMG-CoA reductase. The LDL receptor scavenges circulating LDL from the bloodstream, whereas HMG-CoA reductase leads to an increase of endogenous production of cholesterol. A large part of this signaling pathway was clarified by Dr. Michael S. Brown and Dr. Joseph L. Goldstein in the 1970s. In 1985, they received the Nobel Prize in Physiology or Medicine for their work. Their subsequent work shows how the SREBP pathway regulates expression of many genes that control lipid formation and metabolism and body fuel allocation.\nCholesterol synthesis can also be turned off when cholesterol levels are high. HMG-CoA reductase contains both a cytosolic domain (responsible for its catalytic function) and a membrane domain. The membrane domain senses signals for its degradation. Increasing concentrations of cholesterol (and other sterols) cause a change in this domain's oligomerization state, which makes it more susceptible to destruction by the proteasome. This enzyme's activity can also be reduced by phosphorylation by an AMP-activated protein kinase. Because this kinase is activated by AMP, which is produced when ATP is hydrolyzed, it follows that cholesterol synthesis is halted when ATP levels are low.\nPlasma transport and regulation of absorption.\nAs an isolated molecule, cholesterol is only minimally soluble in water, or hydrophilic. Because of this, it dissolves in blood at exceedingly small concentrations. To be transported effectively, cholesterol is instead packaged within lipoproteins, complex discoidal particles with exterior amphiphilic proteins and lipids, whose outward-facing surfaces are water-soluble and inward-facing surfaces are lipid-soluble. This allows it to travel through the blood via emulsification. Unbound cholesterol, being amphipathic, is transported in the monolayer surface of the lipoprotein particle along with phospholipids and proteins. Cholesterol esters bound to fatty acid, on the other hand, are transported within the fatty hydrophilic core of the lipoprotein, along with triglyceride.\nThere are several types of lipoproteins in the blood. In order of increasing density, they are chylomicrons, very-low-density lipoprotein (VLDL), intermediate-density lipoprotein (IDL), low-density lipoprotein (LDL), and high-density lipoprotein (HDL). Lower protein/lipid ratios make for less dense lipoproteins. Cholesterol within different lipoproteins is identical, although some is carried as its native \"free\" alcohol form (the cholesterol-OH group facing the water surrounding the particles), while others as fatty acyl esters, known also as cholesterol esters, within the particles.\nLipoprotein particles are organized by complex apolipoproteins, typically 80\u2013100 different proteins per particle, which can be recognized and bound by specific receptors on cell membranes, directing their lipid payload into specific cells and tissues currently ingesting these fat transport particles. These surface receptors serve as unique molecular signatures, which then help determine fat distribution delivery throughout the body.\nChylomicrons, the least dense cholesterol transport molecules, contain apolipoprotein B-48, apolipoprotein C, and apolipoprotein E (the principal cholesterol carrier in the brain) in their shells. Chylomicrons carry fats from the intestine to muscle and other tissues in need of fatty acids for energy or fat production. Unused cholesterol remains in more cholesterol-rich chylomicron remnants, and taken up from here to the bloodstream by the liver.\nVLDL molecules are produced by the liver from triacylglycerol and cholesterol which was not used in the synthesis of bile acids. These molecules contain apolipoprotein B100 and apolipoprotein E in their shells, and can be degraded by lipoprotein lipase on the artery wall to IDL. This arterial wall cleavage allows absorption of triacylglycerol and increases concentration of circulating cholesterol. IDL molecules are then consumed in two processes: half is metabolized by HTGL and taken up by the LDL receptor on the liver cell surfaces, while the other half continues to lose triacylglycerols in the bloodstream until they become cholesterol laden LDL particles.\nLDL particles are the major blood cholesterol carriers. Each one contains approximately 1,500 molecules of cholesterol ester. LDL molecule shells contain just one molecule of apolipoprotein B100, recognized by LDL receptors in peripheral tissues. Upon binding of apolipoprotein B100, many LDL receptors concentrate in clathrin-coated pits. Both LDL and its receptor form vesicles within a cell via endocytosis. These vesicles then fuse with a lysosome, where the lysosomal acid lipase enzyme hydrolyzes the cholesterol esters. The cholesterol can then be used for membrane biosynthesis or esterified and stored within the cell, so as to not interfere with the cell membranes.\nLDL receptors are used up during cholesterol absorption, and its synthesis is regulated by SREBP, the same protein that controls the synthesis of cholesterol \"de novo\", according to its presence inside the cell. A cell with abundant cholesterol will have its LDL receptor synthesis blocked, to prevent new cholesterol in LDL molecules from being taken up. Conversely, LDL receptor synthesis proceeds when a cell is deficient in cholesterol.\nWhen this process becomes unregulated, LDL molecules without receptors begin to appear in the blood. These LDL molecules are oxidized and taken up by macrophages, which become engorged and form foam cells. These foam cells often become trapped in the walls of blood vessels and contribute to atherosclerotic plaque formation. Differences in cholesterol homeostasis affect the development of early atherosclerosis (carotid intima-media thickness). These plaques are the main causes of heart attacks, strokes, and other serious medical problems, leading to the association of so-called LDL cholesterol (actually a lipoprotein) with \"bad\" cholesterol.\nHDL particles are thought to transport cholesterol back to the liver, either for excretion or for other tissues that synthesize hormones, in a process known as reverse cholesterol transport (RCT). Large numbers of HDL particles correlates with better health outcomes, whereas low numbers of HDL particles is associated with atheromatous disease progression in the arteries.\nMetabolism, recycling and excretion.\nCholesterol is susceptible to oxidation and easily forms oxygenated derivatives called oxysterols. Three different mechanisms can form these: autoxidation, secondary oxidation to lipid peroxidation, and cholesterol-metabolizing enzyme oxidation. A great interest in oxysterols arose when they were shown to exert inhibitory actions on cholesterol biosynthesis. This finding became known as the \u201coxysterol hypothesis\u201d. Additional roles for oxysterols in human physiology include their participation in bile acid biosynthesis, function as transport forms of cholesterol, and regulation of gene transcription.\nIn biochemical experiments radiolabelled forms of cholesterol, such as tritiated-cholesterol are used. These derivatives undergo degradation upon storage and it is essential to purify cholesterol prior to use. Cholesterol can be purified using small Sephadex LH-20 columns.\nCholesterol is oxidized by the liver into a variety of bile acids. These, in turn, are conjugated with glycine, taurine, glucuronic acid, or sulfate. A mixture of conjugated and nonconjugated bile acids, along with cholesterol itself, is excreted from the liver into the bile. Approximately 95% of the bile acids are reabsorbed from the intestines, and the remainder are lost in the feces. The excretion and reabsorption of bile acids forms the basis of the enterohepatic circulation, which is essential for the digestion and absorption of dietary fats. Under certain circumstances, when more concentrated, as in the gallbladder, cholesterol crystallises and is the major constituent of most gallstones (lecithin and bilirubin gallstones also occur, but less frequently). Every day, up to 1 g of cholesterol enters the colon. This cholesterol originates from the diet, bile, and desquamated intestinal cells, and can be metabolized by the colonic bacteria. Cholesterol is converted mainly into coprostanol, a nonabsorbable sterol that is excreted in the feces.\nAlthough cholesterol is a steroid generally associated with mammals, the human pathogen \"Mycobacterium tuberculosis\" is able to completely degrade this molecule and contains a large number of genes that are regulated by its presence. Many of these cholesterol-regulated genes are homologues of fatty acid \u03b2-oxidation genes, but have evolved in such a way as to bind large steroid substrates like cholesterol.\nDietary sources.\nAnimal fats are complex mixtures of triglycerides, with lesser amounts of both the phospholipids and cholesterol molecules from which all animal (and human) cell membranes are constructed. Since all animal cells manufacture cholesterol, all animal-based foods contain cholesterol in varying amounts. Major dietary sources of cholesterol include red meat, egg yolks and whole eggs, liver, kidney, giblets, fish oil, and butter. Human breast milk also contains significant quantities of cholesterol.\nPlant cells synthesize cholesterol as a precursor for other compounds, such as phytosterols and steroidal glycoalkaloids, with cholesterol remaining in plant foods only in minor amounts or absent. Some plant foods, such as avocado, flax seeds and peanuts, contain phytosterols, which compete with cholesterol for absorption in the intestines, reduce the absorption of both dietary and bile cholesterol. A typical diet contributes on the order of 0.2 gram of phytosterols, which is not enough to have a significant impact on blocking cholesterol absorption. Phytosterols intake can be supplemented through the use of phytosterol-containing functional foods or dietary supplements that are recognized as having potential to reduce levels of LDL-cholesterol.\nMedical guidelines and recommendations.\nIn 2016, the United States Department of Agriculture Dietary Guidelines Advisory Committee recommended that Americans eat as little dietary cholesterol as possible. Most foods that are rich in cholesterol are also high in saturated fat and thereby may increase the risk of cardiovascular disease.\nSome supplemental guidelines have recommended doses of phytosterols in the 1.6\u20133.0\u00a0grams per day range (Health Canada, EFSA, ATP III, FDA). A recent meta-analysis demonstrating a 12% reduction in LDL-cholesterol at a mean dose of 2.1\u00a0grams per day. However, the benefits of a diet supplemented with phytosterols have also been questioned.\nClinical significance.\nHypercholesterolemia.\nAccording to the lipid hypothesis, elevated levels of cholesterol in the blood lead to atherosclerosis which may increase the risk of heart attack, stroke, and peripheral artery disease. Since higher blood LDL \u2013 especially higher LDL concentrations and smaller LDL particle size \u2013 contributes to this process more than the cholesterol content of the HDL particles, LDL particles are often termed \"bad cholesterol\". High concentrations of functional HDL, which can remove cholesterol from cells and atheromas, offer protection and are commonly referred to as \"good cholesterol\". These balances are mostly genetically determined, but can be changed by body composition, medications, diet, and other factors. A 2007 study demonstrated that blood total cholesterol levels have an exponential effect on cardiovascular and total mortality, with the association more pronounced in younger subjects. Because cardiovascular disease is relatively rare in the younger population, the impact of high cholesterol on health is larger in older people.\nElevated levels of the lipoprotein fractions, LDL, IDL and VLDL, rather than the total cholesterol level, correlate with the extent and progress of atherosclerosis. Conversely, the total cholesterol can be within normal limits, yet be made up primarily of small LDL and small HDL particles, under which conditions atheroma growth rates are high. A \"post hoc\" analysis of the IDEAL and the EPIC prospective studies found an association between high levels of HDL cholesterol (adjusted for apolipoprotein A-I and apolipoprotein B) and increased risk of cardiovascular disease, casting doubt on the cardioprotective role of \"good cholesterol\".\nAbout one in 250 individuals can have a genetic mutation for the LDL cholesterol receptor that causes them to have familial hypercholerolemia. Inherited high cholesterol can also include genetic mutations in the PCSK9 gene and the gene for apolipoprotein B.\nElevated cholesterol levels are treated with a strict diet consisting of low saturated fat, trans fat-free, low cholesterol foods, often followed by one of various hypolipidemic agents, such as statins, fibrates, cholesterol absorption inhibitors, nicotinic acid derivatives or bile acid sequestrants. There are several international guidelines on the treatment of hypercholesterolaemia.\nHuman trials using HMG-CoA reductase inhibitors, known as statins, have repeatedly confirmed that changing lipoprotein transport patterns from unhealthy to healthier patterns significantly lowers cardiovascular disease event rates, even for people with cholesterol values currently considered low for adults. Studies have shown that reducing LDL cholesterol levels by about 38.7\u00a0mg/dL with the use of statins can reduce cardiovascular disease and stroke risk by about 21%. Studies have also found that statins reduce atheroma progression. As a result, people with a history of cardiovascular disease may derive benefit from statins irrespective of their cholesterol levels (total cholesterol below 5.0\u00a0mmol/L [193\u00a0mg/dL]), and in men without cardiovascular disease, there is benefit from lowering abnormally high cholesterol levels (\"primary prevention\"). Primary prevention in women was originally practiced only by extension of the findings in studies on men, since, in women, none of the large statin trials conducted prior to 2007 demonstrated a significant reduction in overall mortality or in cardiovascular endpoints. Meta-analyses have demonstrated significant reductions in all-cause and cardiovascular mortality, without significant heterogeneity by sex.\nThe 1987 report of National Cholesterol Education Program, Adult Treatment Panels suggests the total blood cholesterol level should be: &lt; 200\u00a0mg/dL normal blood cholesterol, 200\u2013239\u00a0mg/dL borderline-high, &gt; 240\u00a0mg/dL high cholesterol. The American Heart Association provides a similar set of guidelines for total (fasting) blood cholesterol levels and risk for heart disease: Statins are effective in lowering LDL cholesterol and widely used for primary prevention in people at high risk of cardiovascular disease, as well as in secondary prevention for those who have developed cardiovascular disease.\nMore current testing methods determine LDL (\"bad\") and HDL (\"good\") cholesterol separately, allowing cholesterol analysis to be more nuanced. The desirable LDL level is considered to be less than 130\u00a0mg/dL (2.6 mmol/L), although a newer upper limit of 70\u00a0mg/dL (1.8\u00a0mmol/L) can be considered in higher-risk individuals based on some of the above-mentioned trials. A ratio of total cholesterol to HDL\u2014another useful measure\u2014of far less than 5:1 is thought to be healthier.\nTotal cholesterol is defined as the sum of HDL, LDL, and VLDL. Usually, only the total, HDL, and triglycerides are measured. For cost reasons, the VLDL is usually estimated as one-fifth of the triglycerides and the LDL is estimated using the Friedewald formula (or a variant): estimated LDL = [total cholesterol] \u2212 [total HDL] \u2212 [estimated VLDL]. VLDL can be calculated by dividing total triglycerides by five. Direct LDL measures are used when triglycerides exceed 400\u00a0mg/dL. The estimated VLDL and LDL have more error when triglycerides are above 400\u00a0mg/dL.\nIn the Framingham Heart Study, in subjects over 50 years of age, they found an 11% increase overall and 14% increase in cardiovascular disease mortality per 1\u00a0mg/dL per year drop in total cholesterol levels. The researchers attributed this phenomenon to the fact that people with severe chronic diseases or cancer tend to have below-normal cholesterol levels. This explanation is not supported by the Vorarlberg Health Monitoring and Promotion Programme, in which men of all ages and women over 50 with very low cholesterol were likely to die of cancer, liver diseases, and mental diseases. This result indicates the low-cholesterol effect occurs even among younger respondents, contradicting the previous assessment among cohorts of older people that this is a proxy or marker for frailty occurring with age.\nAlthough there is a link between cholesterol and atherosclerosis as discussed above, a 2014 review concluded there is insufficient evidence to support the recommendation of high consumption of polyunsaturated fatty acids and low consumption of total saturated fats for cardiovascular health. A 2016 review concluded that HDL cholesterol was inversely linked to mortality in people over age 60 years, and there was either no link between LDL and mortality, or that lower LDL was linked to a higher mortality risk, especially in older adults.\nHypocholesterolemia.\nAbnormally low levels of cholesterol are termed \"hypocholesterolemia\". Research into the causes of this state is relatively limited, but some studies suggest a link with depression, cancer, and cerebral hemorrhage. In general, the low cholesterol levels seem to be a consequence, rather than a cause, of an underlying illness. A genetic defect in cholesterol synthesis causes Smith\u2013Lemli\u2013Opitz syndrome, which is often associated with low plasma cholesterol levels. Hyperthyroidism, or any other endocrine disturbance which causes upregulation of the LDL receptor, may result in hypocholesterolemia.\nCholesterol testing.\nThe American Heart Association recommends testing cholesterol every 4\u20136 years for people aged 20 years or older. A separate set of American Heart Association guidelines issued in 2013 indicates that patients taking statin medications should have their cholesterol tested 4\u201312 weeks after their first dose and then every 3\u201312 months thereafter.\nA blood sample after 12-hour fasting is taken by a doctor, or a home cholesterol-monitoring device is used to measure a lipid profile, an approach used to estimate a person's lipoproteins, the vastly more important issue because lipoproteins have always been concordant with outcomes though the lipid profile is commonly discordant LDL Particle Number and Risk of Future Cardiovascular Disease in the Framingham Offspring Study.\nThe lipid profile measures: (a) total cholesterol, (b) cholesterol associated with HDL (i.e. Higher Density {than water} Lipids-transported-within-proteins) particles (\"which can regress arterial disease\"), (c) triglycerides and (d) (by a calculation and assumptions) cholesterol carried by LDL (i.e. Lower Density {than water} Lipids-transported-within-proteins) particles (\"which drive arterial disease\").\nIt is recommended to test cholesterol at least every five years if a person has total cholesterol of 5.2\u00a0mmol/L or more (200+\u00a0mg/dL), or if a man over age 45 or a woman over age 50 has HDL-C values less than 1\u00a0mmol/L (40\u00a0mg/dL), or there are other drivers heart disease and stroke. Additional drivers of heart disease include diabetes mellitus, hypertension (or use of anti-hypertensive medication), low HDL level, family history of coronary artery disease (CAD) and hypercholesterolemia, and cigarette smoking.\nCholesteric liquid crystals.\nSome cholesterol derivatives (among other simple cholesteric lipids) are known to generate the liquid crystalline \"cholesteric phase\". The cholesteric phase is, in fact, a chiral nematic phase, and it changes colour when its temperature changes. This makes cholesterol derivatives useful for indicating temperature in liquid-crystal display thermometers and in temperature-sensitive paints.\nStereoisomers.\nCholesterol has 256 stereoisomers that arise from its 8 stereocenters, although only two of the stereoisomers are of biochemical significance (\"nat\"-cholesterol and \"ent\"-cholesterol, for \"natural\" and \"enantiomer\", respectively), and only one occurs naturally (\"nat\"-cholesterol)."}
{"id": "6438", "revid": "21302502", "url": "https://en.wikipedia.org/wiki?curid=6438", "title": "Chromosome", "text": "A chromosome is a long DNA molecule with part or all of the genetic material of an organism. Most eukaryotic chromosomes include packaging proteins called histones which, aided by chaperone proteins, bind to and condense the DNA molecule to maintain its integrity. These chromosomes display a complex three-dimensional structure, which plays a significant role in transcriptional regulation.\nChromosomes are normally visible under a light microscope only during the metaphase of cell division (where all chromosomes are aligned in the center of the cell in their condensed form). Before this happens, each chromosome is duplicated (S phase), and both copies are joined by a centromere, resulting either in an X-shaped structure (pictured above), if the centromere is located equatorially, or a two-arm structure, if the centromere is located distally. The joined copies are now called sister chromatids. During metaphase the X-shaped structure is called a metaphase chromosome, which is highly condensed and thus easiest to distinguish and study. In animal cells, chromosomes reach their highest compaction level in anaphase during chromosome segregation.\nChromosomal recombination during meiosis and subsequent sexual reproduction play a significant role in genetic diversity. If these structures are manipulated incorrectly, through processes known as chromosomal instability and translocation, the cell may undergo mitotic catastrophe. Usually, this will make the cell initiate apoptosis leading to its own death, but sometimes mutations in the cell hamper this process and thus cause progression of cancer.\nSome use the term chromosome in a wider sense, to refer to the individualized portions of chromatin in cells, either visible or not under light microscopy. Others use the concept in a narrower sense, to refer to the individualized portions of chromatin during cell division, visible under light microscopy due to high condensation.\nEtymology.\nThe word \"chromosome\" () comes from the Greek (\"chroma\", \"colour\") and (\"soma\", \"body\"), describing their strong staining by particular dyes. The term was coined by the German anatomist Heinrich Wilhelm Waldeyer, referring to the term chromatin, which was introduced by Walther Flemming, the discoverer of cell division.\nSome of the early karyological terms have become outdated. For example, Chromatin (Flemming 1880) and Chromosom (Waldeyer 1888), both ascribe color to a non-colored state.\nHistory of discovery.\nThe German scientists Schleiden, Virchow and B\u00fctschli were among the first scientists who recognized the structures now familiar as chromosomes.\nIn a series of experiments beginning in the mid-1880s, Theodor Boveri gave definitive contributions to elucidating that chromosomes are the vectors of heredity, with two notions that became known as \u2018chromosome continuity\u2019 and \u2018chromosome individuality\u2019. \nWilhelm Roux suggested that each chromosome carries a different genetic configuration, and Boveri was able to test and confirm this hypothesis. Aided by the rediscovery at the start of the 1900s of Gregor Mendel's earlier work, Boveri was able to point out the connection between the rules of inheritance and the behaviour of the chromosomes. Boveri influenced two generations of American cytologists: Edmund Beecher Wilson, Nettie Stevens, Walter Sutton and Theophilus Painter were all influenced by Boveri (Wilson, Stevens, and Painter actually worked with him).\nIn his famous textbook \"The Cell in Development and Heredity\", Wilson linked together the independent work of Boveri and Sutton (both around 1902) by naming the chromosome theory of inheritance the Boveri\u2013Sutton chromosome theory (the names are sometimes reversed). Ernst Mayr remarks that the theory was hotly contested by some famous geneticists: William Bateson, Wilhelm Johannsen, Richard Goldschmidt and T.H. Morgan, all of a rather dogmatic turn of mind. Eventually, complete proof came from chromosome maps in Morgan's own lab.\nThe number of human chromosomes was published in 1923 by Theophilus Painter. By inspection through the microscope, he counted 24 pairs, which would mean 48 chromosomes. His error was copied by others and it was not until 1956 that the true number, 46, was determined by Indonesia-born cytogeneticist Joe Hin Tjio.\nProkaryotes.\nThe prokaryotes\u00a0\u2013 bacteria and archaea\u00a0\u2013 typically have a single circular chromosome, but many variations exist. The chromosomes of most bacteria, which some authors prefer to call genophores, can range in size from only 130,000 base pairs in the endosymbiotic bacteria \"Candidatus Hodgkinia cicadicola\" and \"Candidatus Tremblaya princeps\", to more than 14,000,000 base pairs in the soil-dwelling bacterium \"Sorangium cellulosum\". Spirochaetes of the genus \"Borrelia\" are a notable exception to this arrangement, with bacteria such as \"Borrelia burgdorferi\", the cause of Lyme disease, containing a single \"linear\" chromosome.\nStructure in sequences.\nProkaryotic chromosomes have less sequence-based structure than eukaryotes. Bacteria typically have a one-point (the origin of replication) from which replication starts, whereas some archaea contain multiple replication origins. The genes in prokaryotes are often organized in operons, and do not usually contain introns, unlike eukaryotes.\nDNA packaging.\nProkaryotes do not possess nuclei. Instead, their DNA is organized into a structure called the nucleoid. The nucleoid is a distinct structure and occupies a defined region of the bacterial cell. This structure is, however, dynamic and is maintained and remodeled by the actions of a range of histone-like proteins, which associate with the bacterial chromosome. In archaea, the DNA in chromosomes is even more organized, with the DNA packaged within structures similar to eukaryotic nucleosomes.\nCertain bacteria also contain plasmids or other extrachromosomal DNA. These are circular structures in the cytoplasm that contain cellular DNA and play a role in horizontal gene transfer. In prokaryotes (see nucleoids) and viruses, the DNA is often densely packed and organized; in the case of archaea, by homology to eukaryotic histones, and in the case of bacteria, by histone-like proteins.\nBacterial chromosomes tend to be tethered to the plasma membrane of the bacteria. In molecular biology application, this allows for its isolation from plasmid DNA by centrifugation of lysed bacteria and pelleting of the membranes (and the attached DNA).\nProkaryotic chromosomes and plasmids are, like eukaryotic DNA, generally supercoiled. The DNA must first be released into its relaxed state for access for transcription, regulation, and replication.\nEukaryotes.\nEach eukaryotic chromosome consists of a long linear DNA molecule associated with proteins, forming a compact complex of proteins and DNA called \"chromatin.\" Chromatin contains the vast majority of the DNA of an organism, but a small amount inherited maternally, can be found in the mitochondria. It is present in most cells, with a few exceptions, for example, red blood cells.\nHistones are responsible for the first and most basic unit of chromosome organization, the nucleosome.\nEukaryotes (cells with nuclei such as those found in plants, fungi, and animals) possess multiple large linear chromosomes contained in the cell's nucleus. Each chromosome has one centromere, with one or two arms projecting from the centromere, although, under most circumstances, these arms are not visible as such. In addition, most eukaryotes have a small circular mitochondrial genome, and some eukaryotes may have additional small circular or linear cytoplasmic chromosomes.\nIn the nuclear chromosomes of eukaryotes, the uncondensed DNA exists in a semi-ordered structure, where it is wrapped around histones (structural proteins), forming a composite material called chromatin.\nInterphase chromatin.\nThe packaging of DNA into nucleosomes causes a 10 nanometer fibre which may further condense up to 30\u00a0nm fibres Most of the euchromatin in interphase nuclei appears to be in the form of 30-nm fibers. Chromatin structure is the more decondensed state, i.e. the 10-nm conformation allows transcription.\nDuring interphase (the period of the cell cycle where the cell is not dividing), two types of chromatin can be distinguished:\nMetaphase chromatin and division.\nIn the early stages of mitosis or meiosis (cell division), the chromatin double helix become more and more condensed. They cease to function as accessible genetic material (transcription stops) and become a compact transportable form. The loops of 30-nm chromatin fibers are thought to fold upon themselves further to form the compact metaphase chromosomes of mitotic cells. The DNA is thus condensed about 10,000 fold. \nThe chromosome scaffold, which is made of proteins such as condensin, TOP2A and KIF4, plays an important role in holding the chromatin into compact chromosomes. Loops of 30\u00a0nm structure further condense with scaffold into higher order structures. \nThis highly compact form makes the individual chromosomes visible, and they form the classic four arm structure, a pair of sister chromatids attached to each other at the centromere. The shorter arms are called \"p arms\" (from the French \"petit\", small) and the longer arms are called \"q arms\" (\"q\" follows \"p\" in the Latin alphabet; q-g \"grande\"; alternatively it is sometimes said q is short for \"queue\" meaning tail in French). This is the only natural context in which individual chromosomes are visible with an optical microscope.\nMitotic metaphase chromosomes are best described by a linearly organized longitudinally compressed array of consecutive chromatin loops.\nDuring mitosis, microtubules grow from centrosomes located at opposite ends of the cell and also attach to the centromere at specialized structures called kinetochores, one of which is present on each sister chromatid. A special DNA base sequence in the region of the kinetochores provides, along with special proteins, longer-lasting attachment in this region. The microtubules then pull the chromatids apart toward the centrosomes, so that each daughter cell inherits one set of chromatids. Once the cells have divided, the chromatids are uncoiled and DNA can again be transcribed. In spite of their appearance, chromosomes are structurally highly condensed, which enables these giant DNA structures to be contained within a cell nucleus.\nHuman chromosomes.\nChromosomes in humans can be divided into two types: autosomes (body chromosome(s)) and allosome (sex chromosome(s)). Certain genetic traits are linked to a person's sex and are passed on through the sex chromosomes. The autosomes contain the rest of the genetic hereditary information. All act in the same way during cell division. Human cells have 23 pairs of chromosomes (22 pairs of autosomes and one pair of sex chromosomes), giving a total of 46 per cell. In addition to these, human cells have many hundreds of copies of the mitochondrial genome. Sequencing of the human genome has provided a great deal of information about each of the chromosomes. Below is a table compiling statistics for the chromosomes, based on the Sanger Institute's human genome information in the Vertebrate Genome Annotation (VEGA) database. Number of genes is an estimate, as it is in part based on gene predictions. Total chromosome length is an estimate as well, based on the estimated size of unsequenced heterochromatin regions.\nNumber in various organisms.\nIn eukaryotes.\nThese tables give the total number of chromosomes (including sex chromosomes) in a cell nucleus. For example, most eukaryotes are diploid, like humans who have 22 different types of autosomes, each present as two homologous pairs, and two sex chromosomes. This gives 46 chromosomes in total. Other organisms have more than two copies of their chromosome types, such as bread wheat, which is \"hexaploid\" and has six copies of seven different chromosome types\u00a0\u2013 42 chromosomes in total.\nNormal members of a particular eukaryotic species all have the same number of nuclear chromosomes (see the table). Other eukaryotic chromosomes, i.e., mitochondrial and plasmid-like small chromosomes, are much more variable in number, and there may be thousands of copies per cell.\nAsexually reproducing species have one set of chromosomes that are the same in all body cells. However, asexual species can be either haploid or diploid.\nSexually reproducing species have somatic cells (body cells), which are diploid [2n] having two sets of chromosomes (23 pairs in humans), one set from the mother and one from the father. Gametes, reproductive cells, are haploid [n]: They have one set of chromosomes. Gametes are produced by meiosis of a diploid germ line cell. During meiosis, the matching chromosomes of father and mother can exchange small parts of themselves (crossover), and thus create new chromosomes that are not inherited solely from either parent. When a male and a female gamete merge (fertilization), a new diploid organism is formed.\nSome animal and plant species are polyploid [Xn]: They have more than two sets of homologous chromosomes. Plants important in agriculture such as tobacco or wheat are often polyploid, compared to their ancestral species. Wheat has a haploid number of seven chromosomes, still seen in some cultivars as well as the wild progenitors. The more-common pasta and bread wheat types are polyploid, having 28 (tetraploid) and 42 (hexaploid) chromosomes, compared to the 14 (diploid) chromosomes in the wild wheat.\nIn prokaryotes.\nProkaryote species generally have one copy of each major chromosome, but most cells can easily survive with multiple copies. For example, \"Buchnera\", a symbiont of aphids has multiple copies of its chromosome, ranging from 10\u2013400 copies per cell. However, in some large bacteria, such as \"Epulopiscium fishelsoni\" up to 100,000 copies of the chromosome can be present. Plasmids and plasmid-like small chromosomes are, as in eukaryotes, highly variable in copy number. The number of plasmids in the cell is almost entirely determined by the rate of division of the plasmid\u00a0\u2013 fast division causes high copy number.\nKaryotype.\nIn general, the karyotype is the characteristic chromosome complement of a eukaryote species. The preparation and study of karyotypes is part of cytogenetics.\nAlthough the replication and transcription of DNA is highly standardized in eukaryotes, the same cannot be said for their karyotypes, which are often highly variable. There may be variation between species in chromosome number and in detailed organization.\nIn some cases, there is significant variation within species. Often there is:\nAlso, variation in karyotype may occur during development from the fertilized egg.\nThe technique of determining the karyotype is usually called \"karyotyping\". Cells can be locked part-way through division (in metaphase) in vitro (in a reaction vial) with colchicine. These cells are then stained, photographed, and arranged into a \"karyogram\", with the set of chromosomes arranged, autosomes in order of length, and sex chromosomes (here X/Y) at the end.\nLike many sexually reproducing species, humans have special gonosomes (sex chromosomes, in contrast to autosomes). These are XX in females and XY in males. \nHistory and analysis techniques.\nInvestigation into the human karyotype took many years to settle the most basic question: \"How many chromosomes does a normal diploid human cell contain?\" In 1912, Hans von Winiwarter reported 47 chromosomes in spermatogonia and 48 in oogonia, concluding an XX/XO sex determination mechanism. Painter in 1922 was not certain whether the diploid number of man is 46 or 48, at first favouring 46. He revised his opinion later from 46 to 48, and he correctly insisted on humans having an XX/XY system.\nNew techniques were needed to definitively solve the problem:\nIt took until 1954 before the human diploid number was confirmed as 46. Considering the techniques of Winiwarter and Painter, their results were quite remarkable. Chimpanzees, the closest living relatives to modern humans, have 48 chromosomes as do the other great apes: in humans two chromosomes fused to form chromosome 2.\nAberrations.\nChromosomal aberrations are disruptions in the normal chromosomal content of a cell and are a major cause of genetic conditions in humans, such as Down syndrome, although most aberrations have little to no effect. Some chromosome abnormalities do not cause disease in carriers, such as translocations, or chromosomal inversions, although they may lead to a higher chance of bearing a child with a chromosome disorder. Abnormal numbers of chromosomes or chromosome sets, called aneuploidy, may be lethal or may give rise to genetic disorders. Genetic counseling is offered for families that may carry a chromosome rearrangement.\nThe gain or loss of DNA from chromosomes can lead to a variety of genetic disorders. Human examples include:\nSperm aneuploidy.\nExposure of males to certain lifestyle, environmental and/or occupational hazards may increase the risk of aneuploid spermatozoa. In particular, risk of aneuploidy is increased by tobacco smoking, and occupational exposure to benzene, insecticides, and perfluorinated compounds. Increased aneuploidy is often associated with increased DNA damage in spermatozoa."}
{"id": "6439", "revid": "4082870", "url": "https://en.wikipedia.org/wiki?curid=6439", "title": "Charge", "text": "Charge or charged may refer to:"}
{"id": "6440", "revid": "36043475", "url": "https://en.wikipedia.org/wiki?curid=6440", "title": "Colonna family", "text": "Colonna, also known as \"Sciarrillo\" or \"Sciarra\", is an Italian noble family, forming part of the papal nobility. It was powerful in medieval and Renaissance Rome, supplying one Pope (Martin V) and many other church and political leaders. The family is notable for its bitter feud with the Orsini family over influence in Rome, until it was stopped by Papal Bull in 1511. In 1571, the heads of both families married nieces of Pope Sixtus V. Thereafter, historians recorded that \"no peace had been concluded between the princes of Christendom, in which they had not been included by name\".\nHistory.\nOrigins.\nAccording to tradition, the Colonna family is a branch of the Counts of Tusculum \u2014 by Peter (1099\u20131151) son of Gregory III, called Peter \"de Columna\" from his property the Columna Castle in Colonna, in the Alban Hills. Further back, they trace their lineage past the Counts of Tusculum via Lombard and Italo-Roman nobles, merchants, and clergy through the Early Middle Ages \u2014 ultimately claiming origins from the Julio-Claudian dynasty.\nThe first cardinal from the family was appointed in 1206, when Giovanni Colonna di Carbognano was made Cardinal Deacon of SS. Cosma e Damiano. For many years, Cardinal Giovanni di San Paolo (elevated in 1193) was identified as a member of the Colonna family and therefore its first representative in the College of Cardinals, but modern scholars have established that this was based on false information from the beginning of the 16th century.\nGiovanni Colonna (born c. 1206) nephew of Cardinal Giovanni Colonna di Carbognano, made his solemn vows as a Dominican around 1228 and received his theological and philosophical training at the Roman \"studium\" of Santa Sabina, the forerunner of the Pontifical University of Saint Thomas Aquinas, \"Angelicum\". He served as the Provincial of the Roman province of the Dominican Order and led the provincial chapter of 1248 at Anagni. Colonna was appointed as Archbishop of Messina in 1255.\nMargherita Colonna (died 1248) was a member of the Franciscan Order. She was beatified by Pope Pius IX in 1848.\nAt this time, a rivalry began with the pro-papal Orsini family, leaders of the Guelph faction. This reinforced the pro-Emperor Ghibelline course that the Colonna family followed throughout the period of conflict between the Papacy and the Holy Roman Empire.\nColonna versus Papacy.\nIn 1297, Cardinal Jacopo (Giacomo Colonna) disinherited his brothers Ottone, Matteo, and Landolfo of their lands. The latter three appealed to Pope Boniface VIII, who ordered Jacopo to return the land, and furthermore hand over the family's strongholds of Colonna, Palestrina, and other towns to the Papacy. Jacopo refused; in May, Boniface removed him from the College of Cardinals and excommunicated him and his followers.\nThe Colonna family (aside from the three brothers allied with the Pope) declared that Boniface had been elected illegally following the unprecedented abdication of Pope Celestine V. The dispute led to open warfare, and in September, Boniface appointed Landolfo to the command of his army, to put down the revolt of Landolfo's own Colonna relatives. By the end of 1298, Landolfo had captured Colonna, Palestrina and other towns, and razed them to the ground. The family's lands were distributed among Landolfo and his loyal brothers; the rest of the family fled Italy.\nThe exiled Colonnas allied with the Pope's other great enemy, Philip IV of France, who in his youth had been tutored by Cardinal Egidio Colonna. In September 1303, Sciarra and Philipp's advisor, Guillaume de Nogaret, led a small force into Anagni to arrest Boniface VIII and bring him to France, where he was to stand trial. The two managed to apprehend the pope, and Sciarra reportedly slapped the pope in the face in the process, which was accordingly dubbed the \"Outrage of Anagni\". The attempt eventually failed after a few days, when locals freed the pope. However, Boniface VIII died on 11 October, allowing France to dominate his weaker successors during the Avignon papacy.\nLate Middle Ages.\nThe family remained at the centre of civic and religious life throughout the late Middle Ages. Cardinal Egidio Colonna died at the papal court in Avignon in 1314. An Augustinian, he had studied theology in Paris under St. Thomas of Aquinas to become one of the most authoritative thinkers of his time.\nIn the 14th century, the family sponsored the decoration of the Church of San Giovanni, most notably the floor mosaics.\nIn 1328, Louis IV of Germany marched into Italy for his coronation as Holy Roman Emperor. As Pope John XXII was residing in Avignon and had publicly declared that he would not crown Louis, the King decided to be crowned by a member of the Roman aristocracy, who proposed Sciarra Colonna. In honor of this event, the Colonna family was granted the privilege of using the imperial pointed crown on top of their coat of arms.\nThe celebrated poet Petrarch, was a great friend of the family, in particular of Giovanni Colonna and often lived in Rome as a guest of the family. He composed a number of sonnets for special occasions within the Colonna family, including \"Colonna the Glorious, the great Latin name upon which all our hopes rest\". In this period, the Colonna started claiming they were descendants of the Julio-Claudian dynasty.\nAt the Council of Constance, the Colonna finally succeeded in their papal ambitions when Oddone Colonna was elected on 14 November 1417. As Martin V, he reigned until his death on 20 February 1431.\nEarly modern period.\nVittoria Colonna became famous in the sixteenth century as a poet and a figure in literate circles.\nIn 1627 Anna Colonna, daughter of Filippo I Colonna, married Taddeo Barberini of the family Barberini; nephew of Pope Urban VIII.\nIn 1728, the Carbognano branch (Colonna di Sciarra) of the Colonna family added the name Barberini to its family name when Giulio Cesare Colonna di Sciarra married Cornelia Barberini, daughter of the last male Barberini to hold the name and granddaughter of Maffeo Barberini (son of Taddeo Barberini).\nCurrent status.\nThe Colonna family have been Prince Assistants to the Papal Throne since 1710, though their papal princely title only dates from 1854.\nThe family residence in Rome, the Palazzo Colonna, is open to the public every Saturday morning.\nThe main 'Colonna di Paliano' line is represented today by Prince Marcantonio Colonna di Paliano, Prince and Duke of Paliano (b. 1948), whose heir is Don Giovanni Andrea Colonna di Paliano (b. 1975), and by Don Prospero Colonna di Paliano, Prince of Avella (b. 1956), whose heir is Don Filippo Colonna di Paliano (b. 1995).\nThe 'Colonna di Stigliano' line is represented by Don Prospero Colonna di Stigliano, Prince of Stigliano (b. 1938), whose heir is his nephew Don Stefano Colonna di Stigliano (b. 1975)."}
{"id": "6443", "revid": "1024072076", "url": "https://en.wikipedia.org/wiki?curid=6443", "title": "Ceuta", "text": "Ceuta (, , ; ; ) is a Spanish autonomous city on the north coast of Africa.\nBordered by Morocco, it lies along the boundary between the Mediterranean Sea and the Atlantic Ocean. It is one of nine populated Spanish territories in Africa and, along with Melilla, one of two populated Spanish territories on mainland Africa. It was part of the province of C\u00e1diz until 14 March 1995. On that date, Statutes of Autonomy were passed for both Ceuta and Melilla.\nCeuta, like Melilla and the Canary Islands, was classified as a free port before Spain joined the European Union. Its population consists of Christians, Muslims, and small minorities of Sephardic Jews and ethnic Sindhi Hindus from modern-day Pakistan.\nSpanish is the official language. Darija Arabic is also spoken by the 40\u201350% of the population who are of Moroccan origin.\nNames.\nThe name Abyla has been said to have been a Punic name (\"Lofty Mountain\" or \"Mountain of God\") for Jebel Musa, the southern Pillar of Hercules. The name of the mountain was in fact \"Habenna\" (, , \"Stone\" or \"Stele\") or \"\u02beAbin-\u1e25\u012bq\" (, , \"Rock of the Bay\"), in reference to the nearby Bay of Benz\u00fa. The name was hellenized variously as \"\u00c1pini\" (), \"Ab\u00fdla\" (), \"Ab\u00fdl\u0113\" (), \"Abl\u00fdx\" (), and \"Ab\u00edl\u0113 St\u1e17l\u0113\" (, \"Pillar of Abyla\") and in Latin as Mount Abyla (') or the Pillar of Abyla (').\nThe settlement below Jebel Musa was later renamed for the seven hills around the site, collectively referred to as the \"Seven Brothers\" (; ). In particular, the Roman stronghold at the site took the name \"Fort at the Seven Brothers\" (). This was gradually shortened to Septem ( \"S\u00e9pton\") or, occasionally, Septum or Septa. These clipped forms continued as Berber \"Sebta\" and Arabic \"Sabtan\" or \"Sabtah\" (), which themselves became in Portuguese () and Spanish ().\nHistory.\nAncient.\nControlling access between the Atlantic Ocean and the Mediterranean Sea, the Strait of Gibraltar is an important military and commercial chokepoint. The Phoenicians realized the extremely narrow isthmus joining the Peninsula of Almina to the African mainland makes Ceuta eminently defensible and established an outpost there early in the 1st millenniumBC. The Greek geographers record it by variations of \"Abyla\", the ancient name of nearby Jebel Musa. Beside Calpe, the other Pillar of Hercules now known as the Rock of Gibraltar, the Phoenicians established Kart at what is now San Roque, Spain. Other good anchorages nearby became Phoenician and then Carthaginian ports at what are now Tangiers and C\u00e1diz.\nAfter Carthage's destruction in the Punic Wars, most of northwest Africa was left to the Roman client states of Numidia andaround AbylaMauretania. Punic culture continued to thrive in what the Romans knew as \"Septem\". After the Battle of Thapsus in 46\u00a0BC, Caesar and his heirs began annexing north Africa directly as Roman provinces but, as late as Augustus, most of Septem's Berber residents continued to speak and write in Punic.\nCaligula assassinated the Mauretanian king Ptolemy in AD40 and seized his kingdom, which Claudius organized in AD\u00a042, placing Septem in the province of Tingitana and raising it to the level of a colony. It subsequently romanized and thrived into the late 3rd century, trading heavily with Roman Spain and becoming well known for its salted fish. Roads connected it overland with Tingis (Tangiers) and Volubilis. Under in the late 4th century, Septem still had 10,000 inhabitants, nearly all Christian citizens speaking Latin and African Romance.\nMedieval.\nVandals, probably invited by Count Boniface as protection against the empress dowager, crossed the strait near Tingis around 425 and swiftly overran Roman North Africa. Their king Gaiseric focused his attention on the rich lands around Carthage; although the Romans eventually accepted his conquests and he continued to raid them anyway, he soon lost control of Tingis and Septem in a series of Berber revolts. When Justinian decided to reconquer the Vandal lands, his victorious general Belisarius continued along the coast, making Septem an outpost of the Byzantine Empire around 533. Unlike the Roman administration, however, the Byzantines did not push far into hinterland and made the more defensible Septem their regional capital in place of Tingis.\nEpidemics, less capable successors, and overstretched supply lines forced a retrenchment and left Septem isolated. It is likely that its count (\"\") was obliged to pay homage to the Visigoth Kingdom in Spain in the early 7th century. There are no reliable contemporary accounts of the end of the Islamic conquest of the Maghreb around 710. Instead, the rapid Muslim conquest of Spain produced romances concerning Count Julian of Septem and his betrayal of Christendom in revenge for the dishonor that befell his daughter at King Roderick's court. Allegedly with Julian's encouragement and instructions, the Berber convert and freedman Tariq ibn Ziyad took his garrison from Tangiers across the strait and overran the Spanish so swiftly that both he and his master Musa bin Nusayr fell afoul of a jealous caliph, who stripped them of their wealth and titles.\nAfter the death of Julian, sometimes also described as a king of the Ghomara Berbers, Berber converts to Islam took direct control of what they called Sebta. It was then destroyed during their great revolt against the Umayyad Caliphate around 740. Sebta subsequently remained a small village of Muslims and Christians surrounded by ruins until its resettlement in the 9th century by M\u00e2jakas, chief of the Majkasa Berber tribe, who started the short-lived Banu Isam dynasty. His great-grandson briefly allied his tribe with the Idrisids, but Banu Isam rule ended in 931 when he abdicated in favor of Abd ar-Rahman III, the Umayyad caliph of Cordoba. Ceuta reverted to Moorish Andalusian rule in 927 along with Melilla, and later Tangier, in 951.\nChaos ensued with the fall of the Caliphate of C\u00f3rdoba in 1031. Following this, Ceuta and Muslim Iberia were controlled by successive North African dynasties. Starting in 1084, the Almoravid Berbers ruled the region until 1147, when the Almohads conquered the land. Apart from Ibn Hud's rebellion in 1232, they ruled until the Tunisian Hafsids established control. The Hafsids' influence in the west rapidly waned, and Ceuta's inhabitants eventually expelled them in 1249. After this, a period of political instability persisted, under competing interests from the kingdoms of Fez and Granada as well as autonomous rule under the native Banu al-Azafi. The Fez finally conquered the region in 1387, with assistance from Aragon.\nPortuguese.\nOn the morning of 21 August 1415, King John I of Portugal led his sons and their assembled forces in a surprise assault that would come to be known as the Conquest of Ceuta. The battle was almost anti-climactic, because the 45,000 men who traveled on 200 Portuguese ships caught the defenders of Ceuta off guard and only suffered eight casualties. By nightfall the town was captured. On the morning of 22 August, Ceuta was in Portuguese hands. \u00c1lvaro Vaz de Almada, 1st Count of Avranches was asked to hoist what was to become the flag of Ceuta, which is identical to the flag of Lisbon, but in which the coat of arms derived from that of the Kingdom of Portugal was added to the center; the original Portuguese flag and coat of arms of Ceuta remained unchanged, and the modern-day Ceuta flag features the configuration of the Portuguese shield.\nJohn's son Henry the Navigator distinguished himself in the battle, being wounded during the conquest. The looting of the city proved to be less profitable than expected for John I; he decided to keep the city to pursue further enterprises in the area.\nFrom 1415 to 1437, Pedro de Meneses became the first governor of Ceuta.\nThe Benemerine sultan started the 1418 siege but was defeated by the first governor of Ceuta before reinforcements arrived in the form of John, Constable of Portugal and his brother Henry the Navigator who were sent with troops to defend Ceuta.\nUnder King John I's son, Duarte, the colony at Ceuta rapidly became a drain on the Portuguese treasury. Trans-Saharan trade journeyed instead to Tangier. It was soon realized that without the city of Tangier, possession of Ceuta was worthless. In 1437, Duarte's brothers Henry the Navigator and Fernando, the Saint Prince persuaded him to launch an attack on the Marinid sultanate. The resulting Battle of Tangier (1437), led by Henry, was a debacle. In the resulting treaty, Henry promised to deliver Ceuta back to the Marinids in return for allowing the Portuguese army to depart unmolested, which he reneged on.\nPossession of Ceuta would indirectly lead to further Portuguese expansion. The main area of Portuguese expansion, at this time, was the coast of the Maghreb, where there was grain, cattle, sugar, and textiles, as well as fish, hides, wax, and honey.\nCeuta had to endure alone for 43 years, until the position of the city was consolidated with the taking of Ksar es-Seghir (1458), Arzila and Tangier (1471) by the Portuguese.\nThe city was recognized as a Portuguese possession by the Treaty of Alc\u00e1\u00e7ovas (1479) and by the Treaty of Tordesillas (1494).\nIn the 1540s the Portuguese began building the Royal Walls of Ceuta as they are today including bastions, a navigable moat and a drawbridge. Some of these bastions are still standing, like the bastions of Coraza Alta, Bandera and Mallorquines.\nLu\u00eds de Cam\u00f5es lived in Ceuta between 1549 and 1551, losing his right eye in battle, which influenced his work of poetry \"Os Lus\u00edadas\".\nIberian Union.\nIn 1578 King Sebastian of Portugal died at the Battle of Alc\u00e1cer Quibir (known as the Battle of Three Kings) in what is today northern Morocco, without descendants, triggering the 1580 Portuguese succession crisis. His granduncle, the elderly Cardinal Henry, succeeded him as King, but Henry also had no descendants, having taken holy orders. When the cardinal-king died two years after Sebastian's disappearance, three grandchildren of King Manuel I of Portugal claimed the throne: Infanta Catarina, Duchess of Braganza, Ant\u00f3nio, Prior of Crato, and Philip II of Spain (Uncle of former King Sebastian of Portugal), who would go on to be crowned King Philip I of Portugal in 1581, uniting the two crowns and overseas empires known as the Iberian Union, which allowed the two kingdoms to continue without being merged.\nDuring the Iberian Union 1580 to 1640, Ceuta attracted many residents of Spanish origin. Ceuta became the only city of the Portuguese Empire that sided with Spain when Portugal regained its independence in the Portuguese Restoration War of 1640.\nSpanish.\nOn 1 January 1668, King Afonso VI of Portugal recognized the formal allegiance of Ceuta to Spain and formally ceded Ceuta to King Carlos II of Spain by the Treaty of Lisbon.\nThe city was attacked by Moroccan forces under Moulay Ismail during the Siege of Ceuta (1694\u20131727). During the longest siege in history, the city underwent changes leading to the loss of its Portuguese character. While most of the military operations took place around the Royal Walls of Ceuta, there were also small-scale penetrations by Spanish forces at various points on the Moroccan coast, and seizure of shipping in the Strait of Gibraltar.\nDisagreements regarding the border of Ceuta resulted in the Hispano-Moroccan War (1859\u201360), which ended at the Battle of Tetu\u00e1n.\nIn July 1936, General Francisco Franco took command of the Spanish Army of Africa and rebelled against the Spanish republican government; his military uprising led to the Spanish Civil War of 1936\u20131939. Franco transported troops to mainland Spain in an airlift using transport aircraft supplied by Germany and Italy. Ceuta became one of the first casualties of the uprising: General Franco's rebel nationalist forces seized Ceuta, while at the same time the city came under fire from the air and sea forces of the official republican government.\nThe Llano Amarillo monument was erected to honor Francisco Franco, it was inaugurated on 13 July 1940. The tall obelisk has since been abandoned, but the shield symbols of the Falange and Imperial Eagle remain visible.\nWhen Spain recognized the independence of Spanish Morocco in 1956, Ceuta and the other remained under Spanish rule. Spain considered them integral parts of the Spanish state, but Morocco has disputed this point.\nCulturally, modern Ceuta is part of the Spanish region of Andalusia. It was attached to the province of C\u00e1diz until 1925, the Spanish coast being only 20\u00a0km (12.5 miles) away. It is a cosmopolitan city, with a large ethnic Arab-Berber Muslim minority as well as Sephardic Jewish and Hindu minorities.\nOn 5 November 2007, King Juan Carlos I visited the city, sparking great enthusiasm from the local population and protests from the Moroccan government. It was the first time a Spanish head of state had visited Ceuta in 80 years.\nSince 2010, Ceuta (and Melilla) have declared the Muslim holiday of Eid al-Adha, or Feast of the Sacrifice, an official public holiday. It is the first time a non-Christian religious festival has been officially celebrated in Spain since the Reconquista.\nGeography.\nCeuta is separated by from the province of C\u00e1diz on the Spanish mainland by the Strait of Gibraltar and it shares a land border with M'diq-Fnideq Prefecture in the Kingdom of Morocco. It has an area of . It is dominated by Monte Anyera, a hill along its western frontier with Morocco, which is guarded by a Spanish military fort. Monte Hacho on the Peninsula of Almina overlooking the port is one of the possible locations of the southern pillar of the Pillars of Hercules of Greek legend (the other possibility being Jebel Musa).\nImportant Bird Area.\nThe Ceuta Peninsula has been recognised as an Important Bird Area (IBA) by BirdLife International because the site is part of a migratory bottleneck, or choke point, at the western end of the Mediterranean for large numbers of raptors, storks and other birds flying between Europe and Africa. These include European honey buzzards, black kites, short-toed snake eagles, Egyptian vultures, griffon vultures, black storks, white storks and Audouin's gulls.\nClimate.\nCeuta has a maritime-influenced Subtropical/Mediterranean climate, similar to nearby Spanish and Moroccan cities such as Tarifa, Algeciras or Tangiers. The average diurnal temperature variation is relatively low; the average annual temperature is with average yearly highs of and lows of though the Ceuta weather station has only been in operation since 2003. Ceuta has relatively mild winters for the latitude, while summers are warm yet milder than in the interior of Southern Spain, due to the moderating effect of the Straits of Gibraltar. Summers are very dry, but yearly precipitation is still at , which could be considered a humid climate if the summers were not so arid.\nPolitics.\nSince 1995, Ceuta is, along with Melilla, one of the two autonomous cities of Spain.\nCeuta is known officially in Spanish as (English: \"Autonomous City of Ceuta\"), with a rank between a standard Spanish city and an autonomous community. Ceuta is part of the territory of the European Union. The city was a free port before Spain joined the European Union in 1986. Now it has a low-tax system within the Economic and Monetary Union of the European Union. As of 2018, its population was 85,144.\nSince 1979, Ceuta has held elections to its 25-seat assembly every four years. The leader of its government was the Mayor until the Autonomy Statute provided for the new title of Mayor-President. , the People's Party (PP) won 18 seats, keeping Juan Jes\u00fas Vivas as Mayor-President, which he has been since 2001. The remaining seats are held by the regionalist Caballas Coalition (4) and the Socialist Workers' Party (PSOE, 3).\nOwing to its small population, Ceuta elects only one member of the Congress of Deputies, the lower house of the Spanish legislature. election, this post is held by Mar\u00eda Teresa L\u00f3pez of Vox.\nSubdivisions.\nCeuta is subdivided into 63 (\"neighborhoods\"), such as Barriada de Berizu, Barriada de P. Alfonso, Barriada del Sarchal, and El Hacho.\nDispute with Morocco.\nThe government of Morocco has repeatedly called for Spain to transfer the sovereignty of Ceuta and Melilla, along with uninhabited islets such as the islands of Alhucemas, Velez and the Perejil island, drawing comparisons with Spain's territorial claim to Gibraltar. In both cases, the national governments and local populations of the disputed territories reject these claims by a large majority. The Spanish position is that both Ceuta and Melilla are integral parts of Spain, and have been since the 16th century, centuries prior to Morocco's independence from France in 1956, whereas Gibraltar, being a British Overseas Territory, is not and never has been part of the United Kingdom. Morocco has claimed the territories are colonies. One of the chief arguments used by Morocco to reclaim Ceuta comes from geography, as this exclave, which is surrounded by Morocco and the Mediterranean Sea, has no territorial continuity with the rest of Spanish territory. This argument was originally developed by one of the founders of the Moroccan Istiqlal Party, Alal-El Faasi, who openly advocated the Moroccan conquest of Ceuta and other territories under Spanish rule.\nOn 21st December 2020, following the affirmations of the Moroccan Prime Minister, Saadeddine Othmani stating that Ceuta and Melilla \"are Moroccan as the Sahara [is]\", Spain urgently summoned the Moroccan ambassador to convey that Spain expects all its partners to respect the sovereignty and territorial integrity of its territory in Africa and asked for explanations of Othmani's words.\nEconomy.\nThe official currency of Ceuta is the euro. It is part of a special low tax zone in Spain. Ceuta is one of two Spanish port cities on the northern shore of Africa, along with Melilla. They are historically military strongholds, free ports, oil ports, and also fishing ports. Today the economy of the city depends heavily on its port (now in expansion) and its industrial and retail centres. Ceuta Heliport is now used to connect the city to mainland Spain by air. Lidl, Decathlon and El Corte Ingl\u00e9s (hardware) have branches in Ceuta. There is also a casino.\nBorder trade between Ceuta and Morocco is active because of advantage of tax-free status. Thousands of Moroccan women are involved in the cross-border porter trade daily, as porteadoras. The Moroccan dirham is actually used in such trade, despite the fact that prices are marked in euros.\nTransport.\nThe city's Port of Ceuta receives high numbers of ferries each day from Algeciras in Andalusia in the south of Spain. The closest airport is Sania Ramel Airport in Morocco.\nA single road border checkpoint to the south of Ceuta near Fnideq allows for cars and pedestrians to travel between Morocco and Ceuta. An additional border crossing for pedestrians also exists between Benz\u00fa and Belyounech on the northern coast. The rest of the border is closed and inaccessible.\nThere is a bus service throughout the city, and while it does not pass into neighboring Morocco, it services both frontier crossings.\nHospitals.\nThe following hospitals are located within Ceuta:\nDemographics.\nDue to its location, Ceuta is home to a mixed ethnic and religious population. The two main religious groups are Christians and Muslims. As of 2006 approximately 50% of the population was Christian and approximately 48% Muslim. However, by 2012, the portion of Ceuta's population that identify as Roman Catholic was 68.0%, while the portion of Ceuta's population that identify as Muslim was 28.3%.\nSpanish is the primary and official language of the enclave. Moroccan Arabic is widely spoken, as are Berber and French.\nReligion.\nChristianity has been present in Ceuta continuously from late antiquity, as evidenced by the ruins of a basilica in downtown Ceuta and accounts of the martyrdom of St. Daniel Fasanella and his Franciscans in 1227 during the Almohad Caliphate.\nThe town's Grand Mosque had been built over a Byzantine-era church. In 1415, the year of the city's conquest, the Portuguese converted the Grand Mosque into Ceuta Cathedral. The present form of the cathedral dates to refurbishments undertaken in the late 17th century, combining baroque and neoclassical elements. It was dedicated to StMary of the Assumption in 1726.\nThe Roman Catholic Diocese of Ceuta was established in 1417. It incorporated the suppressed Diocese of Tanger in 1570. The Diocese of Ceuta was a suffragan of Lisbon until 1675, when it became a suffragan of Seville. In 1851, Ceuta's administration was notionally merged into the Diocese of C\u00e1diz and Ceuta as part of a concordat between Spain and the Holy See; the union was not actually accomplished, however, until 1879.\nSmall Jewish and Hindu minorities are also present in the city.\nRoman Catholicism is, by far, the largest religion in Ceuta. In 2019, the proportion of Ceutans that identify themselves as Roman Catholic was 60.0%, The next largest religion was Islam (36.7%).\nEducation.\nThe University of Granada offers undergraduate programs at their campus in Ceuta. Like all areas of Spain, Ceuta is also served by the National University of Distance Education (UNED).\nPrimary and secondary education is possible only in Spanish however a growing number of schools are entering the Bilingual Education Program.\nMigrants.\nLike Melilla, Ceuta attracts African migrants who try to use it as an entry to Europe. As a result, the enclave is surrounded by double fences that are high and hundreds of migrants congregate near the fences waiting for a chance to cross them. The fences are regularly stormed by migrants trying to claim asylum once they enter Ceuta.\nTwin towns and sister cities.\nCeuta is twinned with:"}
{"id": "6444", "revid": "25131418", "url": "https://en.wikipedia.org/wiki?curid=6444", "title": "Cleopatra (disambiguation)", "text": "Cleopatra (69-30 BC) was the last active Ptolemaic ruler of Egypt before it became a Roman province.\nCleopatra may also refer to:"}
{"id": "6445", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=6445", "title": "Carcinogen", "text": "A carcinogen is any substance, radionuclide, or radiation that promotes carcinogenesis, the formation of cancer. This may be due to the ability to damage the genome or to the disruption of cellular metabolic processes. Several radioactive substances are considered carcinogens, but their carcinogenic activity is attributed to the radiation, for example gamma rays and alpha particles, which they emit. Common examples of non-radioactive carcinogens are inhaled asbestos, certain dioxins, and tobacco smoke. Although the public generally associates carcinogenicity with synthetic chemicals, it is equally likely to arise in both natural and synthetic substances. Carcinogens are not necessarily immediately toxic; thus, their effect can be insidious.\nCancer is any disease in which normal cells are damaged and do not undergo programmed cell death as fast as they divide via mitosis. Carcinogens may increase the risk of cancer by altering cellular metabolism or damaging DNA directly in cells, which interferes with biological processes, and induces the uncontrolled, malignant division, ultimately leading to the formation of tumors. Usually, severe DNA damage leads to programmed cell death, but if the programmed cell death pathway is damaged, then the cell cannot prevent itself from becoming a cancer cell.\nThere are many natural carcinogens. Aflatoxin B1, which is produced by the fungus \"Aspergillus flavus\" growing on stored grains, nuts and peanut butter, is an example of a potent, naturally occurring microbial carcinogen. Certain viruses such as hepatitis B and human papilloma virus have been found to cause cancer in humans. The first one shown to cause cancer in animals is Rous sarcoma virus, discovered in 1910 by Peyton Rous. Other infectious organisms which cause cancer in humans include some bacteria (e.g. \"Helicobacter pylori\" ) and helminths (e.g. \"Opisthorchis viverrini\" and \"Clonorchis sinensis\").\nDioxins and dioxin-like compounds, benzene, kepone, EDB, and asbestos have all been classified as carcinogenic. As far back as the 1930s, Industrial smoke and tobacco smoke were identified as sources of dozens of carcinogens, including benzo[\"a\"]pyrene, tobacco-specific nitrosamines such as nitrosonornicotine, and reactive aldehydes such as formaldehyde, which is also a hazard in embalming and making plastics. Vinyl chloride, from which PVC is manufactured, is a carcinogen and thus a hazard in PVC production.\nCo-carcinogens are chemicals that do not necessarily cause cancer on their own, but promote the activity of other carcinogens in causing cancer.\nAfter the carcinogen enters the body, the body makes an attempt to eliminate it through a process called biotransformation. The purpose of these reactions is to make the carcinogen more water-soluble so that it can be removed from the body. However, in some cases, these reactions can also convert a less toxic carcinogen into a more toxic carcinogen.\nDNA is nucleophilic; therefore, soluble carbon electrophiles are carcinogenic, because DNA attacks them. For example, some alkenes are toxicated by human enzymes to produce an electrophilic epoxide. DNA attacks the epoxide, and is bound permanently to it. This is the mechanism behind the carcinogenicity of benzo[\"a\"]pyrene in tobacco smoke, other aromatics, aflatoxin and mustard gas.\nRadiation.\nCERCLA identifies all radionuclides as carcinogens, although the nature of the emitted radiation (alpha, beta, gamma, or neutron and the radioactive strength), its consequent capacity to cause ionization in tissues, and the magnitude of radiation exposure, determine the potential hazard. Carcinogenicity of radiation depends on the type of radiation, type of exposure, and penetration. For example, alpha radiation has low penetration and is not a hazard outside the body, but emitters are carcinogenic when inhaled or ingested. For example, Thorotrast, a (incidentally radioactive) suspension previously used as a contrast medium in x-ray diagnostics, is a potent human carcinogen known because of its retention within various organs and persistent emission of alpha particles. Low-level ionizing radiation may induce irreparable DNA damage (leading to replicational and transcriptional errors needed for neoplasia or may trigger viral interactions) leading to pre-mature aging and cancer.\nNot all types of electromagnetic radiation are carcinogenic. Low-energy waves on the electromagnetic spectrum including radio waves, microwaves, infrared radiation and visible light are thought not to be, because they have insufficient energy to break chemical bonds. Evidence for carcinogenic effects of non-ionizing radiation is generally inconclusive, though there are some documented cases of radar technicians with prolonged high exposure experiencing significantly higher cancer incidence.\nHigher-energy radiation, including ultraviolet radiation (present in sunlight), x-rays, and gamma radiation, generally \"is\" carcinogenic, if received in sufficient doses. For most people, ultraviolet radiations from sunlight is the most common cause of skin cancer. In Australia, where people with pale skin are often exposed to strong sunlight, melanoma is the most common cancer diagnosed in people aged 15\u201344 years.\nSubstances or foods irradiated with electrons or electromagnetic radiation (such as microwave, X-ray or gamma) are not carcinogenic. In contrast, non-electromagnetic neutron radiation produced inside nuclear reactors can produce secondary radiation through nuclear transmutation.\nIn prepared food.\nChemicals used in processed and cured meat such as some brands of bacon, sausages and ham may produce carcinogens. For example, nitrites used as food preservatives in cured meat such as bacon have also been noted as being carcinogenic with demographic links, but not causation, to colon cancer. Cooking food at high temperatures, for example grilling or barbecuing meats, may also lead to the formation of minute quantities of many potent carcinogens that are comparable to those found in cigarette smoke (i.e., benzo[\"a\"]pyrene). Charring of food looks like coking and tobacco pyrolysis, and produces carcinogens. There are several carcinogenic pyrolysis products, such as polynuclear aromatic hydrocarbons, which are converted by human enzymes into epoxides, which attach permanently to DNA. Pre-cooking meats in a microwave oven for 2\u20133 minutes before grilling shortens the time on the hot pan, and removes heterocyclic amine (HCA) precursors, which can help minimize the formation of these carcinogens.\nReports from the Food Standards Agency have found that the known animal carcinogen acrylamide is generated in fried or overheated carbohydrate foods (such as french fries and potato chips). Studies are underway at the FDA and Europe regulatory agencies to assess its potential risk to humans.\nIn cigarettes.\nThere is a strong association of smoking with lung cancer; the risk of developing lung cancer increases significantly in smokers. A large number of known carcinogens are found in cigarette smoke. Potent carcinogens found in cigarette smoke include polycyclic aromatic hydrocarbons (PAH, such as benzo(a)pyrene), benzene, and nitrosamine.\nMechanisms of carcinogenicity.\nCarcinogens can be classified as genotoxic or nongenotoxic. Genotoxins cause irreversible genetic damage or mutations by binding to DNA. Genotoxins include chemical agents like N-nitroso-N-methylurea (NMU) or non-chemical agents such as ultraviolet light and ionizing radiation. Certain viruses can also act as carcinogens by interacting with DNA.\nNongenotoxins do not directly affect DNA but act in other ways to promote growth. These include hormones and some organic compounds.\nClassification.\nInternational Agency for Research on Cancer.\nThe International Agency for Research on Cancer (IARC) is an intergovernmental agency established in 1965, which forms part of the World Health Organization of the United Nations. It is based in Lyon, France. Since 1971 it has published a series of \"Monographs on the Evaluation of Carcinogenic Risks to Humans\" that have been highly influential in the classification of possible carcinogens.\nGlobally Harmonized System.\nThe Globally Harmonized System of Classification and Labelling of Chemicals (GHS) is a United Nations initiative to attempt to harmonize the different systems of assessing chemical risk which currently exist (as of March 2009) around the world. It classifies carcinogens into two categories, of which the first may be divided again into subcategories if so desired by the competent regulatory authority:\nU.S. National Toxicology Program.\nThe National Toxicology Program of the U.S. Department of Health and Human Services is mandated to produce a biennial \"Report on Carcinogens\". As of June 2011, the latest edition was the 12th report (2011). It classifies carcinogens into two groups:\nAmerican Conference of Governmental Industrial Hygienists.\nThe American Conference of Governmental Industrial Hygienists (ACGIH) is a private organization best known for its publication of threshold limit values (TLVs) for occupational exposure and monographs on workplace chemical hazards. It assesses carcinogenicity as part of a wider assessment of the occupational hazards of chemicals.\nEuropean Union.\nThe European Union classification of carcinogens is contained in the Regulation (EC) No 1272/2008. It consists of three categories:\nThe former European Union classification of carcinogens was contained in the Dangerous Substances Directive and the Dangerous Preparations Directive. It also consisted of three categories:\nThis assessment scheme is being phased out in favor of the GHS scheme (see above), to which it is very close in category definitions.\nSafe Work Australia.\nUnder a previous name, the NOHSC, in 1999 Safe Work Australia published the Approved Criteria for Classifying Hazardous Substances [NOHSC:1008(1999)].\nSection 4.76 of this document outlines the criteria for classifying carcinogens as approved by the Australian government. This classification consists of three categories:\nCommon carcinogens.\nOccupational carcinogens.\nOccupational carcinogens are agents that pose a risk of cancer in several specific work-locations:\nMajor carcinogens implicated in the four most common cancers worldwide.\nIn this section, the carcinogens implicated as the main causative agents of the four most common cancers worldwide are briefly described. These four cancers are lung, breast, colon, and stomach cancers. Together they account for about 41% of worldwide cancer incidence and 42% of cancer deaths (for more detailed information on the carcinogens implicated in these and other cancers, see references).\nLung cancer.\nLung cancer (pulmonary carcinoma) is the most common cancer in the world, both in terms of cases (1.6 million cases; 12.7% of total cancer cases) and deaths (1.4 million deaths; 18.2% of total cancer deaths). Lung cancer is largely caused by tobacco smoke. Risk estimates for lung cancer in the United States indicate that tobacco smoke is responsible for 90% of lung cancers. Other factors are implicated in lung cancer, and these factors can interact synergistically with smoking so that total attributable risk adds up to more than 100%. These factors include occupational exposure to carcinogens (about 9-15%), radon (10%) and outdoor air pollution (1-2%). Tobacco smoke is a complex mixture of more than 5,300 identified chemicals. The most important carcinogens in tobacco smoke have been determined by a \u201cMargin of Exposure\u201d approach. Using this approach, the most important tumorigenic compounds in tobacco smoke were, in order of importance, acrolein, formaldehyde, acrylonitrile, 1,3-butadiene, cadmium, acetaldehyde, ethylene oxide, and isoprene. Most of these compounds cause DNA damage by forming DNA adducts or by inducing other alterations in DNA. DNA damages are subject to error-prone DNA repair or can cause replication errors. Such errors in repair or replication can result in mutations in tumor suppressor genes or oncogenes leading to cancer.\nBreast cancer.\nBreast cancer is the second most common cancer [(1.4 million cases, 10.9%), but ranks 5th as cause of death (458,000, 6.1%)]. Increased risk of breast cancer is associated with persistently elevated blood levels of estrogen. Estrogen appears to contribute to breast carcinogenesis by three processes; (1) the metabolism of estrogen to genotoxic, mutagenic carcinogens, (2) the stimulation of tissue growth, and (3) the repression of phase II detoxification enzymes that metabolize ROS leading to increased oxidative DNA damage. The major estrogen in humans, estradiol, can be metabolized to quinone derivatives that form adducts with DNA. These derivatives can cause dupurination, the removal of bases from the phosphodiester backbone of DNA, followed by inaccurate repair or replication of the apurinic site leading to mutation and eventually cancer. This genotoxic mechanism may interact in synergy with estrogen receptor-mediated, persistent cell proliferation to ultimately cause breast cancer. Genetic background, dietary practices and environmental factors also likely contribute to the incidence of DNA damage and breast cancer risk.\nColon cancer.\nColorectal cancer is the third most common cancer [1.2 million cases (9.4%), 608,000 deaths (8.0%)]. Tobacco smoke may be responsible for up to 20% of colorectal cancers in the United States. In addition, substantial evidence implicates bile acids as an important factor in colon cancer. Twelve studies (summarized in Bernstein et al.) indicate that the bile acids deoxycholic acid (DCA) or lithocholic acid (LCA) induce production of DNA-damaging reactive oxygen species or reactive nitrogen species in human or animal colon cells. Furthermore, 14 studies showed that DCA and LCA induce DNA damage in colon cells. Also 27 studies reported that bile acids cause programmed cell death (apoptosis). Increased apoptosis can result in selective survival of cells that are resistant to induction of apoptosis. Colon cells with reduced ability to undergo apoptosis in response to DNA damage would tend to accumulate mutations, and such cells may give rise to colon cancer. Epidemiologic studies have found that fecal bile acid concentrations are increased in populations with a high incidence of colon cancer. Dietary increases in total fat or saturated fat result in elevated DCA and LCA in feces and elevated exposure of the colon epithelium to these bile acids. When the bile acid DCA was added to the standard diet of wild-type mice invasive colon cancer was induced in 56% of the mice after 8 to 10 months. Overall, the available evidence indicates that DCA and LCA are centrally important DNA-damaging carcinogens in colon cancer.\nStomach cancer.\nStomach cancer is the fourth most common cancer [990,000 cases (7.8%), 738,000 deaths (9.7%)]. \"Helicobacter pylori\" infection is the main causative factor in stomach cancer. Chronic gastritis (inflammation) caused by \"H. pylori\" is often long-standing if not treated. Infection of gastric epithelial cells with \"H. pylori\" results in increased production of reactive oxygen species (ROS). ROS cause oxidative DNA damage including the major base alteration 8-hydroxydeoxyguanosine (8-OHdG). 8-OHdG resulting from ROS is increased in chronic gastritis. The altered DNA base can cause errors during DNA replication that have mutagenic and carcinogenic potential. Thus \"H. pylori\"-induced ROS appear to be the major carcinogens in stomach cancer because they cause oxidative DNA damage leading to carcinogenic mutations. Diet is thought to be a contributing factor in stomach cancer - in Japan where very salty pickled foods are popular, the incidence of stomach cancer is high. Preserved meat such as bacon, sausages, and ham increases the risk while a diet high in fresh fruit and vegetables may reduce the risk. The risk also increases with age."}
{"id": "6446", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=6446", "title": "Camouflage", "text": "Camouflage is the use of any combination of materials, coloration, or illumination for concealment, either by making animals or objects hard to see, or by disguising them as something else. Examples include the leopard's spotted coat, the battledress of a modern soldier, and the leaf-mimic katydid's wings. A third approach, motion dazzle, confuses the observer with a conspicuous pattern, making the object visible but momentarily harder to locate. The majority of camouflage methods aim for crypsis, often through a general resemblance to the background, high contrast disruptive coloration, eliminating shadow, and countershading. In the open ocean, where there is no background, the principal methods of camouflage are transparency, silvering, and countershading, while the ability to produce light is among other things used for counter-illumination on the undersides of cephalopods such as squid. Some animals, such as chameleons and octopuses, are capable of actively changing their skin pattern and colours, whether for camouflage or for signalling. It is possible that some plants use camouflage to evade being eaten by herbivores.\nMilitary camouflage was spurred by the increasing range and accuracy of firearms in the 19th century. In particular the replacement of the inaccurate musket with the rifle made personal concealment in battle a survival skill. In the 20th century, military camouflage developed rapidly, especially during the First World War. On land, artists such as Andr\u00e9 Mare designed camouflage schemes and observation posts disguised as trees. At sea, merchant ships and troop carriers were painted in dazzle patterns that were highly visible, but designed to confuse enemy submarines as to the target's speed, range, and heading. During and after the Second World War, a variety of camouflage schemes were used for aircraft and for ground vehicles in different theatres of war. The use of radar since the mid-20th century has largely made camouflage for fixed-wing military aircraft obsolete.\nNon-military use of camouflage includes making cell telephone towers less obtrusive and helping hunters to approach wary game animals. Patterns derived from military camouflage are frequently used in fashion clothing, exploiting their strong designs and sometimes their symbolism. Camouflage themes recur in modern art, and both figuratively and literally in science fiction and works of literature.\nHistory.\nIn ancient Greece, Aristotle (384\u2013322 BC) commented on the colour-changing abilities, both for camouflage and for signalling, of cephalopods including the octopus, in his \"Historia animalium\":\nCamouflage has been a topic of interest and research in zoology for well over a century. According to Charles Darwin's 1859 theory of natural selection, features such as camouflage evolved by providing individual animals with a reproductive advantage, enabling them to leave more offspring, on average, than other members of the same species. In his \"Origin of Species\", Darwin wrote:\nThe English zoologist Edward Bagnall Poulton studied animal coloration, especially camouflage. In his 1890 book \"The Colours of Animals\", he classified different types such as \"special protective resemblance\" (where an animal looks like another object), or \"general aggressive resemblance\" (where a predator blends in with the background, enabling it to approach prey). His experiments showed that swallow-tailed moth pupae were camouflaged to match the backgrounds on which they were reared as larvae. Poulton's \"general protective resemblance\" was at that time considered to be the main method of camouflage, as when Frank Evers Beddard wrote in 1892 that \"tree-frequenting animals are often green in colour. Among vertebrates numerous species of parrots, iguanas, tree-frogs, and the green tree-snake are examples\". Beddard did however briefly mention other methods, including the \"alluring coloration\" of the flower mantis and the possibility of a different mechanism in the orange tip butterfly. He wrote that \"the scattered green spots upon the under surface of the wings might have been intended for a rough sketch of the small flowerets of the plant [an umbellifer], so close is their mutual resemblance.\" He also explained the coloration of sea fish such as the mackerel: \"Among pelagic fish it is common to find the upper surface dark-coloured and the lower surface white, so that the animal is inconspicuous when seen either from above or below.\"\nThe artist Abbott Handerson Thayer formulated what is sometimes called Thayer's Law, the principle of countershading. However, he overstated the case in the 1909 book \"Concealing-Coloration in the Animal Kingdom\", arguing that \"All patterns and colors whatsoever of all animals that ever preyed or are preyed on are under certain normal circumstances obliterative\" (that is, cryptic camouflage), and that \"Not one 'mimicry' mark, not one 'warning color'...\u00a0nor any 'sexually selected' color, exists anywhere in the world where there is not every reason to believe it the very best conceivable device for the concealment of its wearer\", and using paintings such as \"Peacock in the Woods\" (1907) to reinforce his argument. Thayer was roundly mocked for these views by critics including Teddy Roosevelt.\nThe English zoologist Hugh Cott's 1940 book \"Adaptive Coloration in Animals\" corrected Thayer's errors, sometimes sharply: \"Thus we find Thayer straining the theory to a fantastic extreme in an endeavour to make it cover almost every type of coloration in the animal kingdom.\" Cott built on Thayer's discoveries, developing a comprehensive view of camouflage based on \"maximum disruptive contrast\", countershading and hundreds of examples. The book explained how disruptive camouflage worked, using streaks of boldly contrasting colour, paradoxically making objects less visible by breaking up their outlines. While Cott was more systematic and balanced in his view than Thayer, and did include some experimental evidence on the effectiveness of camouflage, his 500-page textbook was, like Thayer's, mainly a natural history narrative which illustrated theories with examples.\nExperimental evidence that camouflage helps prey avoid being detected by predators was first provided in 2016, when ground-nesting birds (plovers and coursers) were shown to survive according to how well their egg contrast matched the local environment.\nFossil history.\nCamouflage is a soft-tissue feature that is rarely preserved in the fossil record, but rare fossilised skin samples from the Cretaceous period show that some marine reptiles were countershaded. The skins, pigmented with dark-coloured eumelanin, reveal that both leatherback turtles and mosasaurs had dark backs and light bellies. There is fossil evidence of camouflaged insects going back over 100\u00a0million years, for example lacewings larvae that stick debris all over their bodies much as their modern descendants do, hiding them from their prey. Dinosaurs appear to have been camouflaged, as a 120\u00a0million year old fossil of a \"Psittacosaurus\" has been preserved with countershading.\nPrinciples.\nCamouflage can be achieved by different methods, described below. Most of the methods help to hide against a background; but mimesis and motion dazzle protect without hiding. Methods may be applied on their own or in combination. Many mechanisms are visual, but some research has explored the use of techniques against olfactory (scent) and acoustic (sound) detection. Methods may also apply to military equipment.\nResemblance to surroundings.\nSome animals' colours and patterns resemble a particular natural background. This is an important component of camouflage in all environments. For instance, tree-dwelling parakeets are mainly green; woodcocks of the forest floor are brown and speckled; reedbed bitterns are streaked brown and buff; in each case the animal's coloration matches the hues of its habitat. Similarly, desert animals are almost all desert coloured in tones of sand, buff, ochre, and brownish grey, whether they are mammals like the gerbil or fennec fox, birds such as the desert lark or sandgrouse, or reptiles like the skink or horned viper. Military uniforms, too, generally resemble their backgrounds; for example khaki uniforms are a muddy or dusty colour, originally chosen for service in South Asia. Many moths show industrial melanism, including the peppered moth which has coloration that blends in with tree bark. The coloration of these insects evolved between 1860 and 1940 to match the changing colour of the tree trunks on which they rest, from pale and mottled to almost black in polluted areas. This is taken by zoologists as evidence that camouflage is influenced by natural selection, as well as demonstrating that it changes where necessary to resemble the local background.\nDisruptive coloration.\nDisruptive patterns use strongly contrasting, non-repeating markings such as spots or stripes to break up the outlines of an animal or military vehicle, or to conceal telltale features, especially by masking the eyes, as in the common frog. Disruptive patterns may use more than one method to defeat visual systems such as edge detection. Predators like the leopard use disruptive camouflage to help them approach prey, while potential prey use it to avoid detection by predators. Disruptive patterning is common in military usage, both for uniforms and for military vehicles. Disruptive patterning, however, does not always achieve crypsis on its own, as an animal or a military target may be given away by factors like shape, shine, and shadow.\nThe presence of bold skin markings does not in itself prove that an animal relies on camouflage, as that depends on its behaviour. For example, although giraffes have a high contrast pattern that could be disruptive coloration, the adults are very conspicuous when in the open. Some authors have argued that adult giraffes are cryptic, since when standing among trees and bushes they are hard to see at even a few metres distance. However, adult giraffes move about to gain the best view of an approaching predator, relying on their size and ability to defend themselves, even from lions, rather than on camouflage. A different explanation is implied by young giraffes being far more vulnerable to predation than adults. More than half of all giraffe calves die within a year, and giraffe mothers hide their newly born calves, which spend much of the time lying down in cover while their mothers are away feeding. The mothers return once a day to feed their calves with milk. Since the presence of a mother nearby does not affect survival, it is argued that these juvenile giraffes must be very well camouflaged; this is supported by coat markings being strongly inherited.\nThe possibility of camouflage in plants has been little studied until the late 20th century. Leaf variegation with white spots may serve as camouflage in forest understory plants, where there is a dappled background; leaf mottling is correlated with closed habitats. Disruptive camouflage would have a clear evolutionary advantage in plants: they would tend to escape from being eaten by herbivores. Another possibility is that some plants have leaves differently coloured on upper and lower surfaces or on parts such as veins and stalks to make green-camouflaged insects conspicuous, and thus benefit the plants by favouring the removal of herbivores by carnivores. These hypotheses are testable.\nEliminating shadow.\nSome animals, such as the horned lizards of North America, have evolved elaborate measures to eliminate shadow. Their bodies are flattened, with the sides thinning to an edge; the animals habitually press their bodies to the ground; and their sides are fringed with white scales which effectively hide and disrupt any remaining areas of shadow there may be under the edge of the body. The theory that the body shape of the horned lizards which live in open desert is adapted to minimise shadow is supported by the one species which lacks fringe scales, the roundtail horned lizard, which lives in rocky areas and resembles a rock. When this species is threatened, it makes itself look as much like a rock as possible by curving its back, emphasizing its three-dimensional shape. Some species of butterflies, such as the speckled wood, \"Pararge aegeria\", minimise their shadows when perched by closing the wings over their backs, aligning their bodies with the sun, and tilting to one side towards the sun, so that the shadow becomes a thin inconspicuous line rather than a broad patch. Similarly, some ground-nesting birds, including the European nightjar, select a resting position facing the sun. Eliminating shadow was identified as a principle of military camouflage during the Second World War.\nDistraction.\nMany prey animals have conspicuous high-contrast markings which paradoxically attract the predator's gaze. These distractive markings may serve as camouflage by distracting the predator's attention from recognising the prey as a whole, for example by keeping the predator from identifying the prey's outline. Experimentally, search times for blue tits increased when artificial prey had distractive markings.\nSelf-decoration.\nSome animals actively seek to hide by decorating themselves with materials such as twigs, sand, or pieces of shell from their environment, to break up their outlines, to conceal the features of their bodies, and to match their backgrounds. For example, a caddisfly larva builds a decorated case and lives almost entirely inside it; a decorator crab covers its back with seaweed, sponges, and stones. The nymph of the predatory masked bug uses its hind legs and a 'tarsal fan' to decorate its body with sand or dust. There are two layers of bristles (trichomes) over the body. On these, the nymph spreads an inner layer of fine particles and an outer layer of coarser particles. The camouflage may conceal the bug from both predators and prey.\nSimilar principles can be applied for military purposes, for instance when a sniper wears a ghillie suit designed to be further camouflaged by decoration with materials such as tufts of grass from the sniper's immediate environment. Such suits were used as early as 1916, the British army having adopted \"coats of motley hue and stripes of paint\" for snipers. Cott takes the example of the larva of the blotched emerald moth, which fixes a screen of fragments of leaves to its specially hooked bristles, to argue that military camouflage uses the same method, pointing out that the \"device is ... essentially the same as one widely practised during the Great War for the concealment, not of caterpillars, but of caterpillar-tractors, [gun] battery positions, observation posts and so forth.\"\nCryptic behaviour.\nMovement catches the eye of prey animals on the lookout for predators, and of predators hunting for prey. Most methods of crypsis therefore also require suitable cryptic behaviour, such as lying down and keeping still to avoid being detected, or in the case of stalking predators such as the tiger, moving with extreme stealth, both slowly and quietly, watching its prey for any sign they are aware of its presence. As an example of the combination of behaviours and other methods of crypsis involved, young giraffes seek cover, lie down, and keep still, often for hours until their mothers return; their skin pattern blends with the pattern of the vegetation, while the chosen cover and lying position together hide the animals' shadows. The flat-tail horned lizard similarly relies on a combination of methods: it is adapted to lie flat in the open desert, relying on stillness, its cryptic coloration, and concealment of its shadow to avoid being noticed by predators. In the ocean, the leafy sea dragon sways mimetically, like the seaweeds amongst which it rests, as if rippled by wind or water currents. Swaying is seen also in some insects, like Macleay's spectre stick insect, \"Extatosoma tiaratum\". The behaviour may be motion crypsis, preventing detection, or motion masquerade, promoting misclassification (as something other than prey), or a combination of the two.\nMotion camouflage.\nMost forms of camouflage are ineffective when the camouflaged animal or object moves, because the motion is easily seen by the observing predator, prey or enemy. However, insects such as hoverflies and dragonflies use motion camouflage: the hoverflies to approach possible mates, and the dragonflies to approach rivals when defending territories. Motion camouflage is achieved by moving so as to stay on a straight line between the target and a fixed point in the landscape; the pursuer thus appears not to move, but only to loom larger in the target's field of vision. The same method can be used for military purposes, for example by missiles to minimise their risk of detection by an enemy. However, missile engineers, and animals such as bats, use the method mainly for its efficiency rather than camouflage.\nChangeable skin coloration.\nAnimals such as chameleon, frog, flatfish such as the peacock flounder, squid and octopus actively change their skin patterns and colours using special chromatophore cells to resemble their current background, or, as in most chameleons, for signalling. However, Smith's dwarf chameleon does use active colour change for camouflage.\nEach chromatophore contains pigment of only one colour. In fish and frogs, colour change is mediated by a type of chromatophore known as melanophores that contain dark pigment. A melanophore is star-shaped; it contains many small pigmented organelles which can be dispersed throughout the cell, or aggregated near its centre. When the pigmented organelles are dispersed, the cell makes a patch of the animal's skin appear dark; when they are aggregated, most of the cell, and the animal's skin, appears light. In frogs, the change is controlled relatively slowly, mainly by hormones. In fish, the change is controlled by the brain, which sends signals directly to the chromatophores, as well as producing hormones.\nThe skins of cephalopods such as the octopus contain complex units, each consisting of a chromatophore with surrounding muscle and nerve cells. The cephalopod chromatophore has all its pigment grains in a small elastic sac, which can be stretched or allowed to relax under the control of the brain to vary its opacity. By controlling chromatophores of different colours, cephalopods can rapidly change their skin patterns and colours.\nOn a longer timescale, animals like the Arctic hare, Arctic fox, stoat, and rock ptarmigan have snow camouflage, changing their coat colour (by moulting and growing new fur or feathers) from brown or grey in the summer to white in the winter; the Arctic fox is the only species in the dog family to do so. However, Arctic hares which live in the far north of Canada, where summer is very short, remain white year-round.\nThe principle of varying coloration either rapidly or with the changing seasons has military applications. \"Active camouflage\" could in theory make use of both dynamic colour change and counterillumination. Simple methods such as changing uniforms and repainting vehicles for winter have been in use since World War II. In 2011, BAE Systems announced their Adaptiv infrared camouflage technology. It uses about 1,000 hexagonal panels to cover the sides of a tank. The Peltier plate panels are heated and cooled to match either the vehicle's surroundings (crypsis), or an object such as a car (mimesis), when viewed in infrared.\nCountershading.\nCountershading uses graded colour to counteract the effect of self-shadowing, creating an illusion of flatness. Self-shadowing makes an animal appear darker below than on top, grading from light to dark; countershading 'paints in' tones which are darkest on top, lightest below, making the countershaded animal nearly invisible against a suitable background. Thayer observed that \"Animals are painted by Nature, darkest on those parts which tend to be most lighted by the sky's light, and \"vice versa\"\". Accordingly, the principle of countershading is sometimes called \"Thayer's Law\". Countershading is widely used by terrestrial animals, such as gazelles and grasshoppers; marine animals, such as sharks and dolphins; and birds, such as snipe and dunlin.\nCountershading is less often used for military camouflage, despite Second World War experiments that showed its effectiveness. English zoologist Hugh Cott encouraged the use of methods including countershading, but despite his authority on the subject, failed to persuade the British authorities. Soldiers often wrongly viewed camouflage netting as a kind of invisibility cloak, and they had to be taught to look at camouflage practically, from an enemy observer's viewpoint. At the same time in Australia, zoologist William John Dakin advised soldiers to copy animals' methods, using their instincts for wartime camouflage.\nThe term countershading has a second meaning unrelated to \"Thayer's Law\". It is that the upper and undersides of animals such as sharks, and of some military aircraft, are different colours to match the different backgrounds when seen from above or from below. Here the camouflage consists of two surfaces, each with the simple function of providing concealment against a specific background, such as a bright water surface or the sky. The body of a shark or the fuselage of an aircraft is not gradated from light to dark to appear flat when seen from the side. The camouflage methods used are the matching of background colour and pattern, and disruption of outlines.\nCounter-illumination.\nCounter-illumination means producing light to match a background that is brighter than an animal's body or military vehicle; it is a form of active camouflage. It is notably used by some species of squid, such as the firefly squid and the midwater squid. The latter has light-producing organs (photophores) scattered all over its underside; these create a sparkling glow that prevents the animal from appearing as a dark shape when seen from below. Counterillumination camouflage is the likely function of the bioluminescence of many marine organisms, though light is also produced to attract or to detect prey and for signalling.\nCounterillumination has rarely been used for military purposes. \"Diffused lighting camouflage\" was trialled by Canada's National Research Council during the Second World War. It involved projecting light on to the sides of ships to match the faint glow of the night sky, requiring awkward external platforms to support the lamps. The Canadian concept was refined in the American Yehudi lights project, and trialled in aircraft including B-24 Liberators and naval Avengers. The planes were fitted with forward-pointing lamps automatically adjusted to match the brightness of the night sky. This enabled them to approach much closer to a target \u2013 within \u2013 before being seen. Counterillumination was made obsolete by radar, and neither diffused lighting camouflage nor Yehudi lights entered active service.\nTransparency.\nMany marine animals that float near the surface are highly transparent, giving them almost perfect camouflage. However, transparency is difficult for bodies made of materials that have different refractive indices from seawater. Some marine animals such as jellyfish have gelatinous bodies, composed mainly of water; their thick mesogloea is acellular and highly transparent. This conveniently makes them buoyant, but it also makes them large for their muscle mass, so they cannot swim fast, making this form of camouflage a costly trade-off with mobility. Gelatinous planktonic animals are between 50 and 90 percent transparent. A transparency of 50 percent is enough to make an animal invisible to a predator such as cod at a depth of ; better transparency is required for invisibility in shallower water, where the light is brighter and predators can see better. For example, a cod can see prey that are 98 percent transparent in optimal lighting in shallow water. Therefore, sufficient transparency for camouflage is more easily achieved in deeper waters.\nSome tissues such as muscles can be made transparent, provided either they are very thin or organised as regular layers or fibrils that are small compared to the wavelength of visible light. A familiar example is the transparency of the lens of the vertebrate eye, which is made of the protein crystallin, and the vertebrate cornea which is made of the protein collagen. Other structures cannot be made transparent, notably the retinas or equivalent light-absorbing structures of eyes \u2013 they must absorb light to be able to function. The camera-type eye of vertebrates and cephalopods must be completely opaque. Finally, some structures are visible for a reason, such as to lure prey. For example, the nematocysts (stinging cells) of the transparent siphonophore \"Agalma okenii\" resemble small copepods. Examples of transparent marine animals include a wide variety of larvae, including radiata (coelenterates), siphonophores, salps (floating tunicates), gastropod molluscs, polychaete worms, many shrimplike crustaceans, and fish; whereas the adults of most of these are opaque and pigmented, resembling the seabed or shores where they live. Adult comb jellies and jellyfish obey the rule, often being mainly transparent. Cott suggests this follows the more general rule that animals resemble their background: in a transparent medium like seawater, that means being transparent. The small Amazon river fish \"Microphilypnus amazonicus\" and the shrimps it associates with, \"Pseudopalaemon gouldingi\", are so transparent as to be \"almost invisible\"; further, these species appear to select whether to be transparent or more conventionally mottled (disruptively patterned) according to the local background in the environment.\nSilvering.\nWhere transparency cannot be achieved, it can be imitated effectively by silvering to make an animal's body highly reflective. At medium depths at sea, light comes from above, so a mirror oriented vertically makes animals such as fish invisible from the side. Most fish in the upper ocean such as sardine and herring are camouflaged by silvering.\nThe marine hatchetfish is extremely flattened laterally, leaving the body just millimetres thick, and the body is so silvery as to resemble aluminium foil. The mirrors consist of microscopic structures similar to those used to provide structural coloration: stacks of between 5 and 10 crystals of guanine spaced about of a wavelength apart to interfere constructively and achieve nearly 100 per cent reflection. In the deep waters that the hatchetfish lives in, only blue light with a wavelength of 500 nanometres percolates down and needs to be reflected, so mirrors 125 nanometres apart provide good camouflage.\nIn fish such as the herring which live in shallower water, the mirrors must reflect a mixture of wavelengths, and the fish accordingly has crystal stacks with a range of different spacings. A further complication for fish with bodies that are rounded in cross-section is that the mirrors would be ineffective if laid flat on the skin, as they would fail to reflect horizontally. The overall mirror effect is achieved with many small reflectors, all oriented vertically. Silvering is found in other marine animals as well as fish. The cephalopods, including squid, octopus and cuttlefish, have multilayer mirrors made of protein rather than guanine.\nUltra-blackness.\nSome deep sea fishes have very black skin, reflecting under 0.5% of ambient light. This can prevent detection by predators or prey fish which use bioluminescence for illumination. \"Oneirodes\" had a particularly black skin which reflected only 0.044% of 480\u00a0nm wavelength light. The ultra-blackness is achieved with a thin but continuous layer of particles in the dermis, melanosomes. These particles both absorb most of the light, and are sized and shaped so as to scatter rather than reflect most of the rest. Modelling suggests that this camouflage should reduce the distance at which such a fish can be seen by a factor of 6 compared to a fish with a nominal 2% reflectance. Species with this adaptation are widely dispersed in various orders of the phylogenetic tree of bony fishes (Actinopterygii), implying that natural selection has driven the convergent evolution of ultra-blackness camouflage independently many times.\nMimesis.\nIn mimesis (also called \"masquerade\"), the camouflaged object looks like something else which is of no special interest to the observer. Mimesis is common in prey animals, for example when a peppered moth caterpillar mimics a twig, or a grasshopper mimics a dry leaf. It is also found in nest structures; some eusocial wasps, such as \"Leipomeles dorsata\", build a nest envelope in patterns that mimic the leaves surrounding the nest.\nMimesis is also employed by some predators and parasites to lure their prey. For example, a flower mantis mimics a particular kind of flower, such as an orchid. This tactic has occasionally been used in warfare, for example with heavily armed Q-ships disguised as merchant ships.\nThe common cuckoo, a brood parasite, provides examples of mimesis both in the adult and in the egg. The female lays her eggs in nests of other, smaller species of bird, one per nest. The female mimics a sparrowhawk. The resemblance is sufficient to make small birds take action to avoid the apparent predator. The female cuckoo then has time to lay her egg in their nest without being seen to do so. The cuckoo's egg itself mimics the eggs of the host species, reducing its chance of being rejected.\nMotion dazzle.\nMost forms of camouflage are made ineffective by movement: a deer or grasshopper may be highly cryptic when motionless, but instantly seen when it moves. But one method, motion dazzle, requires rapidly moving bold patterns of contrasting stripes. Motion dazzle may degrade predators' ability to estimate the prey's speed and direction accurately, giving the prey an improved chance of escape. Motion dazzle distorts speed perception and is most effective at high speeds; stripes can also distort perception of size (and so, perceived range to the target). As of 2011, motion dazzle had been proposed for military vehicles, but never applied. Since motion dazzle patterns would make animals more difficult to locate accurately when moving, but easier to see when stationary, there would be an evolutionary trade-off between motion dazzle and crypsis.\nAn animal that is commonly thought to be dazzle-patterned is the zebra. The bold stripes of the zebra have been claimed to be disruptive camouflage, background-blending and countershading. After many years in which the purpose of the coloration was disputed, an experimental study by Tim Caro suggested in 2012 that the pattern reduces the attractiveness of stationary models to biting flies such as horseflies and tsetse flies. However, a simulation study by Martin How and Johannes Zanker in 2014 suggests that when moving, the stripes may confuse observers, such as mammalian predators and biting insects, by two visual illusions: the wagon-wheel effect, where the perceived motion is inverted, and the barberpole illusion, where the perceived motion is in a wrong direction.\nApplications.\nMilitary.\nBefore 1800.\nShip camouflage was occasionally used in ancient times. Philostratus () wrote in his \"Imagines\" that Mediterranean pirate ships could be painted blue-gray for concealment. Vegetius () says that \"Venetian blue\" (sea green) was used in the Gallic Wars, when Julius Caesar sent his \"speculatoria navigia\" (reconnaissance boats) to gather intelligence along the coast of Britain; the ships were painted entirely in bluish-green wax, with sails, ropes and crew the same colour. There is little evidence of military use of camouflage on land before 1800, but two unusual ceramics show men in Peru's Mochica culture from before 500\u00a0AD, hunting birds with blowpipes which are fitted with a kind of shield near the mouth, perhaps to conceal the hunters' hands and faces. Another early source is a 15th-century French manuscript, \"The Hunting Book of Gaston Phebus\", showing a horse pulling a cart which contains a hunter armed with a crossbow under a cover of branches, perhaps serving as a hide for shooting game. Jamaican Maroons are said to have used plant materials as camouflage in the First Maroon War ().\n19th-century origins.\nThe development of military camouflage was driven by the increasing range and accuracy of infantry firearms in the 19th century. In particular the replacement of the inaccurate musket with weapons such as the Baker rifle made personal concealment in battle essential. Two Napoleonic War skirmishing units of the British Army, the 95th Rifle Regiment and the 60th Rifle Regiment, were the first to adopt camouflage in the form of a rifle green jacket, while the Line regiments continued to wear scarlet tunics. A contemporary study in 1800 by the English artist and soldier Charles Hamilton Smith provided evidence that grey uniforms were less visible than green ones at a range of 150 yards.\nIn the American Civil War, rifle units such as the 1st United States Sharp Shooters (in the Federal army) similarly wore green jackets while other units wore more conspicuous colours. The first British Army unit to adopt khaki uniforms was the Corps of Guides at Peshawar, when Sir Harry Lumsden and his second in command, William Hodson introduced a \"drab\" uniform in 1848. Hodson wrote that it would be more appropriate for the hot climate, and help make his troops \"invisible in a land of dust\". Later they improvised by dyeing cloth locally. Other regiments in India soon adopted the khaki uniform, and by 1896 khaki drill uniform was used everywhere outside Europe; by the Second Boer War six years later it was used throughout the British Army.\nDuring the late 19th century camouflage was applied to British coastal fortifications. The fortifications around Plymouth, England were painted in the late 1880s in \"irregular patches of red, brown, yellow and green.\" From 1891 onwards British coastal artillery was permitted to be painted in suitable colours \"to harmonise with the surroundings\" and by 1904 it was standard practice that artillery and mountings should be painted with \"large irregular patches of different colours selected to suit local conditions.\"\nFirst World War.\nIn the First World War, the French army formed a camouflage corps, led by Lucien-Victor Guirand de Sc\u00e9vola, employing artists known as \"camoufleurs\" to create schemes such as tree observation posts and covers for guns. Other armies soon followed them. The term \"camouflage\" probably comes from \"camoufler\", a Parisian slang term meaning \"to disguise\", and may have been influenced by \"camouflet\", a French term meaning \"smoke blown in someone's face\". The English zoologist John Graham Kerr, artist Solomon J. Solomon and the American artist Abbott Thayer led attempts to introduce scientific principles of countershading and disruptive patterning into military camouflage, with limited success. In early 1916 the Royal Naval Air Service began to create dummy air fields to draw the attention of enemy planes to empty land. They created decoy homes and lined fake runways with flares, which were meant to help protect real towns from night raids. This strategy was not common practice and did not succeed at first, but in 1918 it caught the Germans off guard multiple times.\nShip camouflage was introduced in the early 20th century as the range of naval guns increased, with ships painted grey all over. In April 1917, when German U-boats were sinking many British ships with torpedoes, the marine artist Norman Wilkinson devised dazzle camouflage, which paradoxically made ships more visible but harder to target. In Wilkinson's own words, dazzle was designed \"not for low visibility, but in such a way as to break up her form and thus confuse a submarine officer as to the course on which she was heading\".\nSecond World War.\nIn the Second World War, the zoologist Hugh Cott, a prot\u00e9g\u00e9 of Kerr, worked to persuade the British army to use more effective camouflage methods, including countershading, but, like Kerr and Thayer in the First World War, with limited success. For example, he painted two rail-mounted coastal guns, one in conventional style, one countershaded. In aerial photographs, the countershaded gun was essentially invisible. The power of aerial observation and attack led every warring nation to camouflage targets of all types. The Soviet Union's Red Army created the comprehensive doctrine of \"Maskirovka\" for military deception, including the use of camouflage. For example, during the Battle of Kursk, General Katukov, the commander of the Soviet 1st Tank Army, remarked that the enemy \"did not suspect that our well-camouflaged tanks were waiting for him. As we later learned from prisoners, we had managed to move our tanks forward unnoticed\". The tanks were concealed in previously prepared defensive emplacements, with only their turrets above ground level. In the air, Second World War fighters were often painted in ground colours above and sky colours below, attempting two different camouflage schemes for observers above and below. Bombers and night fighters were often black, while maritime reconnaissance planes were usually white, to avoid appearing as dark shapes against the sky. For ships, dazzle camouflage was mainly replaced with plain grey in the Second World War, though experimentation with colour schemes continued.\nAs in the First World War, artists were pressed into service; for example, the surrealist painter Roland Penrose became a lecturer at the newly founded Camouflage Development and Training Centre at Farnham Castle, writing the practical \"Home Guard Manual of Camouflage\". The film-maker Geoffrey Barkas ran the Middle East Command Camouflage Directorate during the 1941\u20131942 war in the Western Desert, including the successful deception of Operation Bertram. Hugh Cott was chief instructor; the artist camouflage officers, who called themselves \"camoufleurs\", included Steven Sykes and Tony Ayrton. In Australia, artists were also prominent in the Sydney Camouflage Group, formed under the chairmanship of Professor William John Dakin, a zoologist from Sydney University. Max Dupain, Sydney Ure Smith, and William Dobell were among the members of the group, which worked at Bankstown Airport, RAAF Base Richmond and Garden Island Dockyard. In the United States, artists like John Vassos took a certificate course in military and industrial camouflage at the American School of Design with Baron Nicholas Cerkasoff, and went on to create camouflage for the Air Force.\nAfter 1945.\nCamouflage has been used to protect military equipment such as vehicles, guns, ships, aircraft and buildings as well as individual soldiers and their positions.\nVehicle camouflage methods begin with paint, which offers at best only limited effectiveness. Other methods for stationary land vehicles include covering with improvised materials such as blankets and vegetation, and erecting nets, screens and soft covers which may suitably reflect, scatter or absorb near infrared and radar waves. Some military textiles and vehicle camouflage paints also reflect infrared to help provide concealment from night vision devices.\nAfter the Second World War, radar made camouflage generally less effective, though coastal boats are sometimes painted like land vehicles. Aircraft camouflage too came to be seen as less important because of radar, and aircraft of different air forces, such as the Royal Air Force's Lightning, were often uncamouflaged.\nMany camouflaged textile patterns have been developed to suit the need to match combat clothing to different kinds of terrain (such as woodland, snow, and desert). The design of a pattern effective in all terrains has proved elusive. The American Universal Camouflage Pattern of 2004 attempted to suit all environments, but was withdrawn after a few years of service. Terrain-specific patterns have sometimes been developed but are ineffective in other terrains. The problem of making a pattern that works at different ranges has been solved with multiscale designs, often with a pixellated appearance and designed digitally, that provide a fractal-like range of patch sizes so they appear disruptively coloured both at close range and at a distance. The first genuinely digital camouflage pattern was the Canadian Disruptive Pattern (CADPAT), issued to the army in 2002, soon followed by the American Marine pattern (MARPAT). A pixellated appearance is not essential for this effect, though it is simpler to design and to print.\nHunting.\nHunters of game have long made use of camouflage in the form of materials such as animal skins, mud, foliage, and green or brown clothing to enable them to approach wary game animals. Field sports such as driven grouse shooting conceal hunters in hides (also called blinds or shooting butts). Modern hunting clothing makes use of fabrics that provide a disruptive camouflage pattern; for example, in 1986 the hunter Bill Jordan created cryptic clothing for hunters, printed with images of specific kinds of vegetation such as grass and branches.\nCivil structures.\nCamouflage is occasionally used to make built structures less conspicuous: for example, in South Africa, towers carrying cell telephone antennae are sometimes camouflaged as tall trees with plastic branches, in response to \"resistance from the community\". Since this method is costly (a figure of three times the normal cost is mentioned), alternative forms of camouflage can include using neutral colours or familiar shapes such as cylinders and flagpoles. Conspicuousness can also be reduced by siting masts near, or on, other structures.\nAutomotive manufacturers often use patterns to disguise upcoming products. This camouflage is designed to obfuscate the vehicle's visual lines, and is used along with padding, covers, and decals. The patterns' purpose is to prevent visual observation (and to a lesser degree photography), that would subsequently enable reproduction of the vehicle's form factors.\nFashion, art and society.\nMilitary camouflage patterns influenced fashion and art from the time of the First World War onwards. Gertrude Stein recalled the cubist artist Pablo Picasso's reaction in around 1915:\nIn 1919, the attendants of a \"dazzle ball\", hosted by the Chelsea Arts Club, wore dazzle-patterned black and white clothing. The ball influenced fashion and art via postcards and magazine articles. The \"Illustrated London News\" announced:\nMore recently, fashion designers have often used camouflage fabric for its striking designs, its \"patterned disorder\" and its symbolism. Camouflage clothing can be worn largely for its symbolic significance rather than for fashion, as when, during the late 1960s and early 1970s in the United States, anti-war protestors often ironically wore military clothing during demonstrations against the American involvement in the Vietnam War.\nModern artists such as Ian Hamilton Finlay have used camouflage to reflect on war. His 1973 screenprint of a tank camouflaged in a leaf pattern, \"Arcadia\", is described by the Tate as drawing \"an ironic parallel between this idea of a natural paradise and the camouflage patterns on a tank\". The title refers to the Utopian Arcadia of poetry and art, and the \"memento mori\" Latin phrase \"Et in Arcadia ego\" which recurs in Hamilton Finlay's work. In science fiction, \"Camouflage\" is a novel about shapeshifting alien beings by Joe Haldeman. The word is used more figuratively in works of literature such as Thaisa Frank's collection of stories of love and loss, \"A Brief History of Camouflage\"."}
{"id": "6449", "revid": "5183450", "url": "https://en.wikipedia.org/wiki?curid=6449", "title": "Clock", "text": "A clock is a device used to measure, verify, keep, and indicate time. The clock is one of the oldest human inventions, meeting the need to measure intervals of time shorter than the natural units: the day, the lunar month, and the year. Devices operating on several physical processes have been used over the millennia.\nSome predecessors to the modern clock may be considered as \"clocks\" that are based on movement in nature: A sundial shows the time by displaying the position of a shadow on a flat surface. There is a range of duration timers, a well-known example being the hourglass. Water clocks, along with the sundials, are possibly the oldest time-measuring instruments. A major advance occurred with the invention of the verge escapement, which made possible the first mechanical clocks around 1300 in Europe, which kept time with oscillating timekeepers like balance wheels.\nTraditionally, in horology, the term \"clock\" was used for a striking clock, while a clock that did not strike the hours audibly was called a timepiece; this distinction is no longer made. Watches and other timepieces that can be carried on one's person are usually not referred to as clocks.\nSpring-driven clocks appeared during the 15th century. During the 15th and 16th centuries, clockmaking flourished. The next development in accuracy occurred after 1656 with the invention of the pendulum clock by Christiaan Huygens. A major stimulus to improving the accuracy and reliability of clocks was the importance of precise time-keeping for navigation. The mechanism of a timepiece with a series of gears driven by a spring or weights is referred to as clockwork; the term is used by extension for a similar mechanism not used in a timepiece. The electric clock was patented in 1840, and electronic clocks were introduced in the 20th century, becoming widespread with the development of small battery-powered semiconductor devices.\nThe timekeeping element in every modern clock is a harmonic oscillator, a physical object (resonator) that vibrates or oscillates at a particular frequency.\nThis object can be a pendulum, a tuning fork, a quartz crystal, or the vibration of electrons in atoms as they emit microwaves.\nClocks have different ways of displaying the time. Analog clocks indicate time with a traditional clock face, with moving hands. Digital clocks display a numeric representation of time. Two numbering systems are in use: 24-hour time notation and 12-hour notation. Most digital clocks use electronic mechanisms and LCD, LED, or VFD displays. For the blind and use over telephones, speaking clocks state the time audibly in words. There are also clocks for the blind that have displays that can be read by touch. The study of timekeeping is known as horology.\nEtymology.\nThe word \"clock\" derives from the medieval Latin word for 'bell'\u2014\u2014and has cognates in many European languages. Clocks spread to England from the Low Countries, so the English word came from the Middle Low German and Middle Dutch \"Klocke\".\nThe word derives from the Middle English \"clokke\", Old North French \"cloque\", or Middle Dutch \"clocke\", all of which mean 'bell'.\nHistory of time-measuring devices.\nSundials.\nThe apparent position of the Sun in the sky moves over the course of each day, reflecting the rotation of the Earth. Shadows cast by stationary objects move correspondingly, so their positions can be used to indicate the time of day. A sundial shows the time by displaying the position of a shadow on a (usually) flat surface, which has markings that correspond to the hours. Sundials can be horizontal, vertical, or in other orientations. Sundials were widely used in ancient times. With the knowledge of latitude, a well-constructed sundial can measure local solar time with reasonable accuracy, within a minute or two. Sundials continued to be used to monitor the performance of clocks until the 1830s, with the use of the telegraph and train to standardize time and time zones between cities.\nDevices that measure duration, elapsed time and intervals.\nMany devices can be used to mark the passage of time without respect to reference time (time of day, hours, minutes, etc.) and can be useful for measuring duration or intervals. Examples of such duration timers are candle clocks, incense clocks and the hourglass. Both the candle clock and the incense clock work on the same principle wherein the consumption of resources is more or less constant allowing reasonably precise and repeatable estimates of time passages. In the hourglass, fine sand pouring through a tiny hole at a constant rate indicates an arbitrary, predetermined passage of time. The resource is not consumed but re-used.\nWater clocks.\nWater clocks, along with the sundials, are possibly the oldest time-measuring instruments, with the only exceptions being the day counting tally stick. Given their great antiquity, where and when they first existed is not known and perhaps unknowable. The bowl-shaped outflow is the simplest form of a water clock and is known to have existed in Babylon and in Egypt around the 16th century BC. Other regions of the world, including India and China, also have early evidence of water clocks, but the earliest dates are less certain. Some authors, however, write about water clocks appearing as early as 4000 BC in these regions of the world.\nGreek astronomer Andronicus of Cyrrhus supervised the construction of the Tower of the Winds in Athens in the 1st century B.C. The Greek and Roman civilizations advanced water clock design with improved accuracy. These advances were passed on through Byzantium and Islamic times, eventually making their way back to Europe. Independently, the Chinese developed their own advanced water clocks\uff08\u6c34\u9418\uff09in 725 AD, passing their ideas on to Korea and Japan.\nSome water clock designs were developed independently and some knowledge was transferred through the spread of trade. Pre-modern societies do not have the same precise timekeeping requirements that exist in modern industrial societies, where every hour of work or rest is monitored, and work may start or finish at any time regardless of external conditions. Instead, water clocks in ancient societies were used mainly for astrological reasons. These early water clocks were calibrated with a sundial. While never reaching the level of accuracy of a modern timepiece, the water clock was the most accurate and commonly used timekeeping device for millennia, until it was replaced by the more accurate pendulum clock in 17th-century Europe.\nIslamic civilization is credited with further advancing the accuracy of clocks with elaborate engineering. In 797 (or possibly 801), the Abbasid caliph of Baghdad, Harun al-Rashid, presented Charlemagne with an Asian elephant named Abul-Abbas together with a \"particularly elaborate example\" of a water clock. Pope Sylvester II introduced clocks to northern and western Europe around 1000 AD.\nMechanical water clocks.\nThe first known geared clock was invented by the great mathematician, physicist, and engineer Archimedes during the 3rd century BC. Archimedes created his astronomical clock that was also a cuckoo clock with birds singing and moving every hour. It is the first carillon clock as it plays music and simultaneously with a person blinking his eyes surprised by the singing birds. Archimedes clock works with a system of four weights, counter weights, and strings regulated by a system of floats in a water container with siphons that regulate the automatic continuation of the clock. The principles of this type of clocks are described by the mathematician and physicist Hero, who says that some of them work with a chain that turns a gear of the mechanism. Another Greek clock probably constructed at the time of Alexander was in Gaza, described by Procopius. The Gaza clock was probably a Meteoroskopeion, i.e. a building showing the celestial phenomena and the time. It had pointer for the time and some automations similar to the Archimedes clock. There were 12 doors opening one every hour with Hercules performing his labors, the Lion at one o'clock, etc, and at night a lamp becomes visible every hour, with 12 windows opening to show the time.\nAnother geared clock was developed in the 11th century by the Arab engineer Ibn Khalaf al-Muradi in Islamic Iberia; it was a water clock that employed a complex gear train mechanism, including both segmental and epicyclic gearing, capable of transmitting high torque. The clock was unrivalled in its use of sophisticated complex gearing, until the mechanical clocks of the mid-14th century. Al-Muradi's clock also employed the use of mercury in its hydraulic linkages, which could function mechanical automata. Al-Muradi's work was known to scholars working under Alfonso X of Castile, hence the mechanism may have played a role in the development of the European mechanical clocks. Other monumental water clocks constructed by medieval Muslim engineers also employed complex gear trains and arrays of automata. Arab engineers at the time also developed a liquid-driven escapement mechanism which they employed in some of their water clocks. Heavy floats were used as weights and a constant-head system was used as an escapement mechanism, which was present in the hydraulic controls they used to make heavy floats descend at a slow and steady rate.\nA water-powered cogwheel clock was created in China by Yi Xing and Liang Lingzan. This is not considered an escapement mechanism clock as it was unidirectional, the Song dynasty polymath and genius Su Song (1020\u20131101) incorporated it into his monumental innovation of the astronomical clock-tower of Kaifeng in 1088. His astronomical clock and rotating armillary sphere still relied on the use of either flowing water during the spring, summer, autumn seasons and liquid mercury during the freezing temperature of winter (i.e. hydraulics). A mercury clock, described in the \"Libros del saber\", a Spanish work from 1277 consisting of translations and paraphrases of Arabic works, is sometimes quoted as evidence for Muslim knowledge of a mechanical clock. A mercury-powered cogwheel clock was created by Ibn Khalaf al-Muradi.\nIn the 13th century, Al-Jazari, an engineer from Mesopotamia (lived 1136\u20131206) who worked for Artuqid king of Diyar-Bakr, Nasir al-Din, made numerous clocks of all shapes and sizes. A book on his work described 50 mechanical devices in 6 categories, including water clocks. The most reputed clocks included the Elephant, Scribe and Castle clocks, all of which have been successfully reconstructed. As well as telling the time, these grand clocks were symbols of status, grandeur and wealth of the Urtuq State.\nFully mechanical.\nThe word (from the Greek \u2014'hour', and \u2014'to tell') was used to describe early mechanical clocks, but the use of this word (still used in several Romance languages) for all timekeepers conceals the true nature of the mechanisms. For example, there is a record that in 1176 Sens Cathedral installed an 'horologe' but the mechanism used is unknown. According to Jocelin of Brakelond, in 1198 during a fire at the abbey of St Edmundsbury (now Bury St Edmunds), the monks 'ran to the clock' to fetch water, indicating that their water clock had a reservoir large enough to help extinguish the occasional fire. The word \"clock\" (via Medieval Latin from Old Irish , both meaning 'bell'), which gradually supersedes \"horologe\", suggests that it was the sound of bells which also characterized the prototype mechanical clocks that appeared during the 13th century in Europe.\nIn Europe, between 1280 and 1320, there was an increase in the number of references to clocks and horologes in church records, and this probably indicates that a new type of clock mechanism had been devised. Existing clock mechanisms that used water power were being adapted to take their driving power from falling weights. This power was controlled by some form of oscillating mechanism, probably derived from existing bell-ringing or alarm devices. This controlled release of power\u2014the escapement\u2014marks the beginning of the true mechanical clock, which differed from the previously mentioned cogwheel clocks. Verge escapement mechanism derived in the surge of true mechanical clocks, which didn't need any kind of fluid power, like water or mercury, to work.\nThese mechanical clocks were intended for two main purposes: for signalling and notification (e.g. the timing of services and public events), and for modeling the solar system. The former purpose is administrative, the latter arises naturally given the scholarly interests in astronomy, science, astrology, and how these subjects integrated with the religious philosophy of the time. The astrolabe was used both by astronomers and astrologers, and it was natural to apply a clockwork drive to the rotating plate to produce a working model of the solar system.\nSimple clocks intended mainly for notification were installed in towers, and did not always require faces or hands. They would have announced the canonical hours or intervals between set times of prayer. Canonical hours varied in length as the times of sunrise and sunset shifted. The more sophisticated astronomical clocks would have had moving dials or hands, and would have shown the time in various time systems, including Italian hours, canonical hours, and time as measured by astronomers at the time. Both styles of clock started acquiring extravagant features such as automata.\nIn 1283, a large clock was installed at Dunstable Priory; its location above the rood screen suggests that it was not a water clock. In 1292, Canterbury Cathedral installed a 'great horloge'. Over the next 30 years there are mentions of clocks at a number of ecclesiastical institutions in England, Italy, and France. In 1322, a new clock was installed in Norwich, an expensive replacement for an earlier clock installed in 1273. This had a large (2 metre) astronomical dial with automata and bells. The costs of the installation included the full-time employment of two clockkeepers for two years.\nAstronomical.\nBesides the Chinese astronomical clock of Su Song in 1088 mentioned above, contemporary Muslim astronomers also constructed a variety of highly accurate astronomical clocks for use in their mosques and observatories, such as the water-powered astronomical clock by Al-Jazari in 1206, and the astrolabic clock by Ibn al-Shatir in the early 14th century. The most sophisticated timekeeping astrolabes were the geared astrolabe mechanisms designed by Ab\u016b Rayh\u0101n B\u012br\u016bn\u012b in the 11th century and by Muhammad ibn Abi Bakr in the 13th century. These devices functioned as timekeeping devices and also as calendars.\nA sophisticated water-powered astronomical clock was built by Al-Jazari in 1206. This castle clock was a complex device that was about high, and had multiple functions alongside timekeeping. It included a display of the zodiac and the solar and lunar paths, and a pointer in the shape of the crescent moon which travelled across the top of a gateway, moved by a hidden cart and causing doors to open, each revealing a mannequin, every hour. It was possible to reset the length of day and night in order to account for the changing lengths of day and night throughout the year. This clock also featured a number of automata including falcons and musicians who automatically played music when moved by levers operated by a hidden camshaft attached to a water wheel.\nIn Europe, there were the clocks constructed by Richard of Wallingford in St Albans by 1336, and by Giovanni de Dondi in Padua from 1348 to 1364. They no longer exist, but detailed descriptions of their design and construction survive, and modern reproductions have been made. They illustrate how quickly the theory of the mechanical clock had been translated into practical constructions, and also that one of the many impulses to their development had been the desire of astronomers to investigate celestial phenomena.\nWallingford's clock had a large astrolabe-type dial, showing the sun, the moon's age, phase, and node, a star map, and possibly the planets. In addition, it had a wheel of fortune and an indicator of the state of the tide at London Bridge. Bells rang every hour, the number of strokes indicating the time. Dondi's clock was a seven-sided construction, 1 metre high, with dials showing the time of day, including minutes, the motions of all the known planets, an automatic calendar of fixed and movable feasts, and an eclipse prediction hand rotating once every 18 years. It is not known how accurate or reliable these clocks would have been. They were probably adjusted manually every day to compensate for errors caused by wear and imprecise manufacture. Water clocks are sometimes still used today, and can be examined in places such as ancient castles and museums. The Salisbury Cathedral clock, built in 1386, is considered to be the world's oldest surviving mechanical clock that strikes the hours.\nSpring-driven.\nClockmakers developed their art in various ways. Building smaller clocks was a technical challenge, as was improving accuracy and reliability. Clocks could be impressive showpieces to demonstrate skilled craftsmanship, or less expensive, mass-produced items for domestic use. The escapement in particular was an important factor affecting the clock's accuracy, so many different mechanisms were tried.\nSpring-driven clocks appeared during the 15th century, although they are often erroneously credited to Nuremberg watchmaker Peter Henlein (or Henle, or Hele) around 1511. The earliest existing spring driven clock is the chamber clock given to Phillip the Good, Duke of Burgundy, around 1430, now in the Germanisches Nationalmuseum. Spring power presented clockmakers with a new problem: how to keep the clock movement running at a constant rate as the spring ran down. This resulted in the invention of the \"stackfreed\" and the fusee in the 15th century, and many other innovations, down to the invention of the modern \"going barrel\" in 1760.\nEarly clock dials did not indicate minutes and seconds. A clock with a dial indicating minutes was illustrated in a 1475 manuscript by Paulus Almanus, and some 15th-century clocks in Germany indicated minutes and seconds.\nAn early record of a seconds hand on a clock dates back to about 1560 on a clock now in the Fremersdorf collection.\nDuring the 15th and 16th centuries, clockmaking flourished, particularly in the metalworking towns of Nuremberg and Augsburg, and in Blois, France. Some of the more basic table clocks have only one time-keeping hand, with the dial between the hour markers being divided into four equal parts making the clocks readable to the nearest 15 minutes. Other clocks were exhibitions of craftsmanship and skill, incorporating astronomical indicators and musical movements. The cross-beat escapement was invented in 1584 by Jost B\u00fcrgi, who also developed the remontoire. B\u00fcrgi's clocks were a great improvement in accuracy as they were correct to within a minute a day. These clocks helped the 16th-century astronomer Tycho Brahe to observe astronomical events with much greater precision than before.\nPendulum.\nThe next development in accuracy occurred after 1656 with the invention of the pendulum clock. Galileo had the idea to use a swinging bob to regulate the motion of a time-telling device earlier in the 17th century. Christiaan Huygens, however, is usually credited as the inventor. He determined the mathematical formula that related pendulum length to time (about 99.4\u00a0cm or 39.1\u00a0inches for the one second movement) and had the first pendulum-driven clock made. The first model clock was built in 1657 in the Hague, but it was in England that the idea was taken up. The longcase clock (also known as the \"grandfather clock\") was created to house the pendulum and works by the English clockmaker William Clement in 1670 or 1671. It was also at this time that clock cases began to be made of wood and clock faces to utilize enamel as well as hand-painted ceramics.\nIn 1670, William Clement created the anchor escapement, an improvement over Huygens' crown escapement. Clement also introduced the pendulum suspension spring in 1671. The concentric minute hand was added to the clock by Daniel Quare, a London clockmaker and others, and the second hand was first introduced.\nHairspring.\nIn 1675, Huygens and Robert Hooke invented the spiral balance spring, or the hairspring, designed to control the oscillating speed of the balance wheel. This crucial advance finally made accurate pocket watches possible. The great English clockmaker Thomas Tompion, was one of the first to use this mechanism successfully in his pocket watches, and he adopted the minute hand which, after a variety of designs were trialled, eventually stabilised into the modern-day configuration. The rack and snail striking mechanism for striking clocks, was introduced during the 17th century and had distinct advantages over the 'countwheel' (or 'locking plate') mechanism. During the 20th century there was a common misconception that Edward Barlow invented \"rack and snail\" striking. In fact, his invention was connected with a repeating mechanism employing the rack and snail. The repeating clock, that chimes the number of hours (or even minutes) on demand was invented by either Quare or Barlow in 1676. George Graham invented the deadbeat escapement for clocks in 1720.\nMarine chronometer.\nA major stimulus to improving the accuracy and reliability of clocks was the importance of precise time-keeping for navigation. The position of a ship at sea could be determined with reasonable accuracy if a navigator could refer to a clock that lost or gained less than about 10 seconds per day. This clock could not contain a pendulum, which would be virtually useless on a rocking ship. In 1714, the British government offered large financial rewards to the value of 20,000 pounds for anyone who could determine longitude accurately. John Harrison, who dedicated his life to improving the accuracy of his clocks, later received considerable sums under the Longitude Act.\nIn 1735, Harrison built his first chronometer, which he steadily improved on over the next thirty years before submitting it for examination. The clock had many innovations, including the use of bearings to reduce friction, weighted balances to compensate for the ship's pitch and roll in the sea and the use of two different metals to reduce the problem of expansion from heat. The chronometer was tested in 1761 by Harrison's son and by the end of 10 weeks the clock was in error by less than 5 seconds.\nMass production.\nThe British had predominated in watch manufacture for much of the 17th and 18th centuries, but maintained a system of production that was geared towards high quality products for the elite. Although there was an attempt to modernise clock manufacture with mass production techniques and the application of duplicating tools and machinery by the British Watch Company in 1843, it was in the United States that this system took off. In 1816, Eli Terry and some other Connecticut clockmakers developed a way of mass-producing clocks by using interchangeable parts. Aaron Lufkin Dennison started a factory in 1851 in Massachusetts that also used interchangeable parts, and by 1861 was running a successful enterprise incorporated as the Waltham Watch Company.\nEarly electric.\nIn 1815, Francis Ronalds published the first electric clock powered by dry pile batteries. Alexander Bain, Scottish clockmaker, patented the electric clock in 1840. The electric clock's mainspring is wound either with an electric motor or with an electromagnet and armature. In 1841, he first patented the electromagnetic pendulum. By the end of the nineteenth century, the advent of the dry cell battery made it feasible to use electric power in clocks. Spring or weight driven clocks that use electricity, either alternating current (AC) or direct current (DC), to rewind the spring or raise the weight of a mechanical clock would be classified as an electromechanical clock. This classification would also apply to clocks that employ an electrical impulse to propel the pendulum. In electromechanical clocks the electricity serves no time keeping function. These types of clocks were made as individual timepieces but more commonly used in synchronized time installations in schools, businesses, factories, railroads and government facilities as a master clock and slave clocks.\nWhere an AC electrical supply of stable frequency is available, timekeeping can be maintained very reliably by using a synchronous motor, essentially counting the cycles. The supply current alternates with an accurate frequency of 50\u00a0hertz in many countries, and 60\u00a0hertz in others. While the frequency may vary slightly during the day as the load changes, generators are designed to maintain an accurate number of cycles over a day, so the clock may be a fraction of a second slow or fast at any time, but will be perfectly accurate over a long time. The rotor of the motor rotates at a speed that is related to the alternation frequency. Appropriate gearing converts this rotation speed to the correct ones for the hands of the analog clock. Time in these cases is measured in several ways, such as by counting the cycles of the AC supply, vibration of a tuning fork, the behaviour of quartz crystals, or the quantum vibrations of atoms. Electronic circuits divide these high-frequency oscillations to slower ones that drive the time display.\nQuartz.\nThe piezoelectric properties of crystalline quartz were discovered by Jacques and Pierre Curie in 1880. The first crystal oscillator was invented in 1917 by Alexander M. Nicholson after which, the first quartz crystal oscillator was built by Walter G. Cady in 1921. In 1927 the first quartz clock was built by Warren Marrison and J.W. Horton at Bell Telephone Laboratories in Canada. The following decades saw the development of quartz clocks as precision time measurement devices in laboratory settings\u2014the bulky and delicate counting electronics, built with vacuum tubes, limited their practical use elsewhere. The National Bureau of Standards (now NIST) based the time standard of the United States on quartz clocks from late 1929 until the 1960s, when it changed to atomic clocks. In 1969, Seiko produced the world's first quartz wristwatch, the Astron. Their inherent accuracy and low cost of production resulted in the subsequent proliferation of quartz clocks and watches.\nAtomic.\nCurrently, atomic clocks are the most accurate clocks in existence. They are considerably more accurate than quartz clocks as they can be accurate to within a few seconds over trillions of years. Atomic clocks were first theorized by Lord Kelvin in 1879. In the 1930s the development of Magnetic resonance created practical method for doing this. A prototype ammonia maser device was built in 1949 at the U.S. National Bureau of Standards (NBS, now NIST). Although it was less accurate than existing quartz clocks, it served to demonstrate the concept. The first accurate atomic clock, a caesium standard based on a certain transition of the caesium-133 atom, was built by Louis Essen in 1955 at the National Physical Laboratory in the UK. Calibration of the caesium standard atomic clock was carried out by the use of the astronomical time scale \"ephemeris time\" (ET). As of 2013, the most stable atomic clocks are ytterbium clocks, which are stable to within less than two parts in 1 quintillion ().\nOperation.\nThe invention of the mechanical clock in the 13th century initiated a change in timekeeping methods from continuous processes, such as the motion of the gnomon's shadow on a sundial or the flow of liquid in a water clock, to periodic oscillatory processes, such as the swing of a pendulum or the vibration of a quartz crystal, which had the potential for more accuracy. All modern clocks use oscillation.\nAlthough the mechanisms they use vary, all oscillating clocks, mechanical, digital and atomic, work similarly and can be divided into analogous parts. They consist of an object that repeats the same motion over and over again, an \"oscillator\", with a precisely constant time interval between each repetition, or 'beat'. Attached to the oscillator is a \"controller\" device, which sustains the oscillator's motion by replacing the energy it loses to friction, and converts its oscillations into a series of pulses. The pulses are then counted by some type of \"counter\", and the number of counts is converted into convenient units, usually seconds, minutes, hours, etc. Finally some kind of \"indicator\" displays the result in human readable form.\nOscillator.\nThe timekeeping element in every modern clock is a harmonic oscillator, a physical object (resonator) that vibrates or oscillates repetitively at a precisely constant frequency.\nThe advantage of a harmonic oscillator over other forms of oscillator is that it employs resonance to vibrate at a precise natural resonant frequency or \"beat\" dependent only on its physical characteristics, and resists vibrating at other rates. The possible precision achievable by a harmonic oscillator is measured by a parameter called its Q, or quality factor, which increases (other things being equal) with its resonant frequency. This is why there has been a long term trend toward higher frequency oscillators in clocks. Balance wheels and pendulums always include a means of adjusting the rate of the timepiece. Quartz timepieces sometimes include a rate screw that adjusts a capacitor for that purpose. Atomic clocks are primary standards, and their rate cannot be adjusted.\nSynchronized or slave clocks.\nSome clocks rely for their accuracy on an external oscillator; that is, they are automatically synchronized to a more accurate clock:\nController.\nThis has the dual function of keeping the oscillator running by giving it 'pushes' to replace the energy lost to friction, and converting its vibrations into a series of pulses that serve to measure the time.\nIn mechanical clocks, the low Q of the balance wheel or pendulum oscillator made them very sensitive to the disturbing effect of the impulses of the escapement, so the escapement had a great effect on the accuracy of the clock, and many escapement designs were tried. The higher Q of resonators in electronic clocks makes them relatively insensitive to the disturbing effects of the drive power, so the driving oscillator circuit is a much less critical component.\nCounter chain.\nThis counts the pulses and adds them up to get traditional time units of seconds, minutes, hours, etc. It usually has a provision for \"setting\" the clock by manually entering the correct time into the counter.\nIndicator.\nThis displays the count of seconds, minutes, hours, etc. in a human readable form.\nTypes.\nClocks can be classified by the type of time display, as well as by the method of timekeeping.\nTime display methods.\nAnalog.\nAnalog clocks usually use a clock face which indicates time using rotating pointers called \"hands\" on a fixed numbered dial or dials. The standard clock face, known universally throughout the world, has a short \"hour hand\" which indicates the hour on a circular dial of 12 hours, making two revolutions per day, and a longer \"minute hand\" which indicates the minutes in the current hour on the same dial, which is also divided into 60 minutes. It may also have a \"second hand\" which indicates the seconds in the current minute. The only other widely used clock face today is the 24 hour analog dial, because of the use of 24 hour time in military organizations and timetables. Before the modern clock face was standardized during the Industrial Revolution, many other face designs were used throughout the years, including dials divided into 6, 8, 10, and 24 hours. During the French Revolution the French government tried to introduce a 10-hour clock, as part of their decimal-based metric system of measurement, but it didn't catch on. An Italian 6 hour clock was developed in the 18th century, presumably to save power (a clock or watch striking 24 times uses more power).\nAnother type of analog clock is the sundial, which tracks the sun continuously, registering the time by the shadow position of its gnomon. Because the sun does not adjust to daylight saving time, users must add an hour during that time. Corrections must also be made for the equation of time, and for the difference between the longitudes of the sundial and of the central meridian of the time zone that is being used (i.e. 15 degrees east of the prime meridian for each hour that the time zone is ahead of GMT). Sundials use some or part of the 24 hour analog dial. There also exist clocks which use a digital display despite having an analog mechanism\u2014these are commonly referred to as flip clocks. Alternative systems have been proposed. For example, the \"Twelv\" clock indicates the current hour using one of twelve colors, and indicates the minute by showing a proportion of a circular disk, similar to a moon phase.\nDigital.\nDigital clocks display a numeric representation of time. Two numeric display formats are commonly used on digital clocks:\nMost digital clocks use electronic mechanisms and LCD, LED, or VFD displays; many other display technologies are used as well (cathode ray tubes, nixie tubes, etc.). After a reset, battery change or power failure, these clocks without a backup battery or capacitor either start counting from 12:00, or stay at 12:00, often with blinking digits indicating that the time needs to be set. Some newer clocks will reset themselves based on radio or Internet time servers that are tuned to national atomic clocks. Since the advent of digital clocks in the 1960s, the use of analog clocks has declined significantly.\nSome clocks, called 'flip clocks', have digital displays that work mechanically. The digits are painted on sheets of material which are mounted like the pages of a book. Once a minute, a page is turned over to reveal the next digit. These displays are usually easier to read in brightly lit conditions than LCDs or LEDs. Also, they do not go back to 12:00 after a power interruption. Flip clocks generally do not have electronic mechanisms. Usually, they are driven by AC-synchronous motors.\nHybrid (analog-digital).\nClocks with analog quadrants, with a digital component, usually minutes and hours displayed analogously and seconds displayed in digital mode.\nAuditory.\nFor convenience, distance, telephony or blindness, auditory clocks present the time as sounds. The sound is either spoken natural language, (e.g. \"The time is twelve thirty-five\"), or as auditory codes (e.g. number of sequential bell rings on the hour represents the number of the hour like the bell, Big Ben). Most telecommunication companies also provide a speaking clock service as well.\nWord.\nWord clocks are clocks that display the time visually using sentences. E.g.: \"It's about three o'clock.\" These clocks can be implemented in hardware or software.\nProjection.\nSome clocks, usually digital ones, include an optical projector that shines a magnified image of the time display onto a screen or onto a surface such as an indoor ceiling or wall. The digits are large enough to be easily read, without using glasses, by persons with moderately imperfect vision, so the clocks are convenient for use in their bedrooms. Usually, the timekeeping circuitry has a battery as a backup source for an uninterrupted power supply to keep the clock on time, while the projection light only works when the unit is connected to an A.C. supply. Completely battery-powered portable versions resembling flashlights are also available.\nTactile.\nAuditory and projection clocks can be used by people who are blind or have limited vision. There are also clocks for the blind that have displays that can be read by using the sense of touch. Some of these are similar to normal analog displays, but are constructed so the hands can be felt without damaging them. Another type is essentially digital, and uses devices that use a code such as Braille to show the digits so that they can be felt with the fingertips.\nMulti-display.\nSome clocks have several displays driven by a single mechanism, and some others have several completely separate mechanisms in a single case. Clocks in public places often have several faces visible from different directions, so that the clock can be read from anywhere in the vicinity; all the faces show the same time. Other clocks show the current time in several time-zones. Watches that are intended to be carried by travellers often have two displays, one for the local time and the other for the time at home, which is useful for making pre-arranged phone calls. Some equation clocks have two displays, one showing mean time and the other solar time, as would be shown by a sundial. Some clocks have both analog and digital displays. Clocks with Braille displays usually also have conventional digits so they can be read by sighted people.\nPurposes.\nClocks are in homes, offices and many other places; smaller ones (watches) are carried on the wrist or in a pocket; larger ones are in public places, e.g. a railway station or church. A small clock is often shown in a corner of computer displays, mobile phones and many MP3 players.\nThe primary purpose of a clock is to \"display\" the time. Clocks may also have the facility to make a loud alert signal at a specified time, typically to waken a sleeper at a preset time; they are referred to as \"alarm clocks\". The alarm may start at a low volume and become louder, or have the facility to be switched off for a few minutes then resume. Alarm clocks with visible indicators are sometimes used to indicate to children too young to read the time that the time for sleep has finished; they are sometimes called \"training clocks\".\nA clock mechanism may be used to \"control\" a device according to time, e.g. a central heating system, a VCR, or a time bomb (see: digital counter). Such mechanisms are usually called timers. Clock mechanisms are also used to drive devices such as solar trackers and astronomical telescopes, which have to turn at accurately controlled speeds to counteract the rotation of the Earth.\nMost digital computers depend on an internal signal at constant frequency to synchronize processing; this is referred to as a clock signal. (A few research projects are developing CPUs based on asynchronous circuits.) Some equipment, including computers, also maintains time and date for use as required; this is referred to as time-of-day clock, and is distinct from the system clock signal, although possibly based on counting its cycles.\nIn Chinese culture, giving a clock () is often taboo, especially to the elderly as the term for this act is a homophone with the term for the act of attending another's funeral (). A UK government official Susan Kramer gave a watch to Taipei mayor Ko Wen-je unaware of such a taboo which resulted in some professional embarrassment and a pursuant apology.\nThis homonymic pair works in both Mandarin and Cantonese, although in most parts of China only clocks and large bells, and not watches, are called \"zhong\", and watches are commonly given as gifts in China.\nHowever, should such a gift be given, the \"unluckiness\" of the gift can be countered by exacting a small monetary payment so the recipient is buying the clock and thereby counteracting the (\"give\") expression of the phrase.\nTime standards.\nFor some scientific work timing of the utmost accuracy is essential. It is also necessary to have a standard of the maximum accuracy against which working clocks can be calibrated. An ideal clock would give the time to unlimited accuracy, but this is not realisable. Many physical processes, in particular including some transitions between atomic energy levels, occur at exceedingly stable frequency; counting cycles of such a process can give a very accurate and consistent time\u2014clocks which work this way are usually called atomic clocks. Such clocks are typically large, very expensive, require a controlled environment, and are far more accurate than required for most purposes; they are typically used in a standards laboratory.\nNavigation.\nUntil advances in the late twentieth century, navigation depended on the ability to measure latitude and longitude. Latitude can be determined through celestial navigation; the measurement of longitude requires accurate knowledge of time. This need was a major motivation for the development of accurate mechanical clocks. John Harrison created the first highly accurate marine chronometer in the mid-18th century. The Noon gun in Cape Town still fires an accurate signal to allow ships to check their chronometers. Many buildings near major ports used to have (some still do) a large ball mounted on a tower or mast arranged to drop at a pre-determined time, for the same purpose. While satellite navigation systems such as the Global Positioning System (GPS) require unprecedentedly accurate knowledge of time, this is supplied by equipment on the satellites; vehicles no longer need timekeeping equipment."}
{"id": "6451", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=6451", "title": "Charles Proteus Steinmetz", "text": "Charles Proteus Steinmetz (born Karl August Rudolph Steinmetz, April 9, 1865\u00a0\u2013 October 26, 1923) was a German-born American mathematician and electrical engineer and professor at Union College. He fostered the development of alternating current that made possible the expansion of the electric power industry in the United States, formulating mathematical theories for engineers. He made ground-breaking discoveries in the understanding of hysteresis that enabled engineers to design better electromagnetic apparatus equipment, especially electric motors for use in industry.\nAt the time of his death, Steinmetz held over 200 patents. A genius in both mathematics and electronics, he did work that earned him the nicknames \"Forger of Thunderbolts\" and \"The Wizard of Schenectady\". Steinmetz's equation, Steinmetz solids, Steinmetz curves, and Steinmetz equivalent circuit theory are all named after him, as are numerous honors and scholarships, including the \"IEEE Charles Proteus Steinmetz Award\", one of the highest technical recognitions given by the Institute of Electrical and Electronics Engineers professional society.\nEarly life and education.\nSteinmetz was born Karl August Rudolph Steinmetz on April 9, 1865 in Breslau, Province of Silesia, Prussia (now Wroc\u0142aw, Poland) the son of Caroline (Neubert) and Karl Heinrich Steinmetz. He was baptized a Lutheran into the Evangelical Church of Prussia. Steinmetz, who stood only four feet tall as an adult, suffered from dwarfism, hunchback, and hip dysplasia, as did his father and grandfather. Steinmetz attended Johannes Gymnasium and astonished his teachers with his proficiency in mathematics and physics.\nFollowing the Gymnasium, Steinmetz went on to the University of Breslau to begin work on his undergraduate degree in 1883. He was on the verge of finishing his doctorate in 1888 when he came under investigation by the German police for activities on behalf of a socialist university group and articles he had written for a local socialist newspaper.\nSocialism and technocracy.\nAs socialist meetings and press had been banned in Germany, Steinmetz fled to Z\u00fcrich in 1888 to escape possible arrest. Cornell University Professor Ronald R. Kline, author of \"Steinmetz: Engineer and Socialist\", contended that other factors were more directly involved in Steinmetz's decision to leave his homeland such as being in arrears with his tuition at the University and life at home with his father, stepmother and their daughters being tension-filled.\nFaced with an expiring visa, he emigrated to the United States in 1889. He changed his first name to \"Charles\" in order to sound more American, and chose the middle name \"Proteus\", a wise hunchbacked character from the \"Odyssey\" who knew many secrets, after a childhood epithet given by classmates Steinmetz felt suited him.\nDespite his earlier efforts and interest in socialism, by 1922 Steinmetz concluded that socialism would never work in the United States, because the country lacked a \"powerful, centralized government of competent men, remaining continuously in office\", and because \"only a small percentage of Americans accept this viewpoint today\".\nA member of the original Technical Alliance, which also included Thorstein Veblen and Leland Olds, Steinmetz had great faith in the ability of machines to eliminate human toil and create abundance for all. He put it this way: \"Some day we make the good things of life for everybody\".\nEngineering wizard.\nSteinmetz is known for his contribution in three major fields of alternating current (AC) systems theory: hysteresis, steady-state analysis, and transients.\nAC hysteresis theory.\nShortly after arriving in the United States, Steinmetz went to work for Rudolf Eickemeyer in Yonkers, New York, and published in the field of magnetic hysteresis, earning worldwide professional recognition. Eickemeyer's firm developed transformers for use in the transmission of electrical power among many other mechanical and electrical devices. In 1893 Eickemeyer's company, along with all of its patents and designs, was bought by the newly formed General Electric Company, where Steinmetz quickly became known as the engineering wizard in GE's engineering community.\nAC steady state circuit theory.\nSteinmetz's work revolutionized AC circuit theory and analysis, which had been carried out using complicated, time-consuming calculus-based methods. In the groundbreaking paper, \"Complex Quantities and Their Use in Electrical Engineering\", presented at a July 1893 meeting published in the American Institute of Electrical Engineers (AIEE), Steinmetz simplified these complicated methods to \"a simple problem of algebra\". He systematized the use of complex number phasor representation in electrical engineering education texts, whereby the lower-case letter \"j\" is used to designate the 90-degree rotation operator in AC system analysis. His seminal books and many other AIEE papers \"taught a whole generation of engineers how to deal with AC phenomena\".\nAC transient theory.\nSteinmetz also greatly advanced the understanding of lightning. His systematic experiments resulted in the first laboratory created \"man-made lightning\", earning him the nickname the \"Forger of Thunderbolts\". These were conducted in a football field-sized laboratory at General Electric, using 120,000 volt generators. He also erected a lightning tower to attract natural lightning in order to study its patterns and effects, which resulted in several theories.\nProfessional life.\nSteinmetz acted in the following professional capacities:\nHe was granted an honorary degree from Harvard University in 1901 and a doctorate from Union College in 1903.\nSteinmetz wrote 13 books and 60 articles, not exclusively about engineering. He was a member and adviser to the fraternity Phi Gamma Delta at Union College, whose chapter house there was one of the first ever electrified residences.\nWhile serving as president of the Schenectady Board of Education, Steinmetz introduced numerous progressive reforms, including extended school hours, school meals, school nurses, special classes for the children of immigrants, and the distribution of free textbooks.\nPersonal life.\nIn spite of his love for children and family life, Steinmetz remained unmarried, to prevent the spinal deformity afflicting himself, his father, and grandfather from being passed on to any offspring.\nWhen Joseph LeRoy Hayden, a loyal and hardworking lab assistant, announced that he would marry and look for his own living quarters, Steinmetz made the unusual proposal of opening his large home, complete with research lab, greenhouse, and office to the Haydens and their prospective family. Hayden favored the idea, but his future wife was very wary of the unorthodox setup. She finally agreed after Steinmetz's assurance that she could run the house as she saw fit.\nAfter an uneasy start, the arrangement worked well for all parties, especially after three Hayden children were born. Steinmetz legally adopted Joseph Hayden as his son, becoming grandfather to the youngsters, entertaining them with fantastic stories and spectacular scientific demonstrations. The unusual but harmonious living arrangements lasted for the rest of Steinmetz's life.\nSteinmetz founded America's first glider club, but none of its prototypes \"could be dignified with the term 'flight'\".\nSteinmetz was a lifelong agnostic. He died on October 26, 1923, and was buried in Vale Cemetery in Schenectady.\nLegacy.\nThe \"Forger of Thunderbolts\" and \"Wizard of Schenectady\" earned wide recognition among the scientific community and numerous awards and honors both during his life and posthumously.\n\"Steinmetz's equation\", derived from his experiments, defines the approximate heat energy due to magnetic hysteresis released, per cycle per unit volume of magnetic material. A Steinmetz solid is the solid body generated by the intersection of two or three cylinders of equal radius at right angles. Steinmetz equivalent circuit theory is still widely used for the design and testing of induction motors.\nOne of the highest technical recognitions given by the Institute of Electrical and Electronics Engineers, the \"IEEE Charles Proteus Steinmetz Award\", is given for major contributions to standardization within the field of electrical and electronics engineering. Other awards include the Certificate of Merit of Franklin Institute, 1908; the Elliott Cresson Medal, 1913; and the Cedergren Medal, 1914.\nThe \"Charles P. Steinmetz Memorial Lecture\" series was begun in his honor in 1925, sponsored by the Schenectady branch of the IEEE. Through 2017 seventy-three gatherings have taken place, held almost exclusively at Union College, featuring notable figures such as Nobel laureate experimental physicist Robert A. Millikan, helicopter inventor Igor Sikorsky, nuclear submarine pioneer Admiral Hyman G. Rickover (1963), Nobel-winning semiconductor inventor William Shockley, and Internet 'founding father' Leonard Kleinrock. The \"Charles P. Steinmetz Scholarship\" is awarded annually by the college, underwritten since its inception in 1923 by the General Electric Company.\nThe \"Charles P. Steinmetz Memorial Scholarship\" was established at Union by Marjorie Hayden, daughter of Joseph and Corrine Hayden, and is awarded to students majoring in engineering or physics.\nSteinmetz's connection to Union is further celebrated with the annual Steinmetz Symposium, a day-long event in which Union undergraduates give presentations on research they have done. Steinmetz Hall, which houses the Union College computer center, is named after him.\nSteinmetz was portrayed in 1959 by the actor Rod Steiger in the CBS television anthology series, \"The Joseph Cotten Show\". The episode focused on his socialist activities in Germany.\nA Chicago public high school, Steinmetz College Prep, is named for him.\nA public park in north Schenectady, New York was named for him in 1931.\nIn popular culture.\nSteinmetz is featured in John Dos Passos' \"U.S.A.\" trilogy in one of the biographies. He also serves as a major character in Starling Lawrence's \"The Lightning Keeper\".\nSteinmetz is a major character in the novel \"Electric City\" by Elizabeth Rosner.\nMoe refers to Curly as a \"Steinmetz\" in the 1944 Three Stooges short \"Busy Buddies\".\nA famous anecdote about Steinmetz concerns a troubleshooting consultation at Henry Ford's River Rouge Plant. A humorous aspect of the story is the \"itemized bill\" he submitted for the work performed.\nBibliography.\nPatents.\nAt the time of his death, Steinmetz held over 200 patents:"}
{"id": "6452", "revid": "2340490", "url": "https://en.wikipedia.org/wiki?curid=6452", "title": "Charles Martel", "text": "Charles Martel (c. 688 \u2013 22 October 741) was a Frankish statesman and military leader who, as Duke and Prince of the Franks and Mayor of the Palace, was the \"de facto\" ruler of Francia from 718 until his death. He was a son of the Frankish statesman Pepin of Herstal and Pepin's mistress, a noblewoman named Alpaida. Charles, also known as \u201cThe Hammer\u201d (in old French, \"Martel\"), successfully asserted his claims to power as successor to his father as the power behind the throne in Frankish politics. Continuing and building on his father's work, he restored centralized government in Francia and began the series of military campaigns that re-established the Franks as the undisputed masters of all Gaul. According to a near-contemporary source, the \"Liber Historiae Francorum\", Charles was \"a warrior who was uncommonly [...] effective in battle\". \nMartel defeated an Umayyad invasion of Aquitaine at the Battle of Tours. The Umayyad Caliphate controlled most of the Iberian Peninsula. Alongside his military endeavours, Charles has been traditionally credited with a seminal role in the development of the Frankish system of feudalism.\nAt the end of his reign, Charles divided Francia between his sons, Carloman and Pepin. The latter became the first king of the Carolingian dynasty. Charles' grandson, Charlemagne, extended the Frankish realms, and became the first emperor in the West since the fall of Rome.\nBackground.\nCharles, nicknamed \"Martel\", or \"Charles the Hammer\" in later chronicles, was the illegitimate son of Pepin of Herstal and his mistress, possible second wife, Alpaida. He had a brother named Childebrand, who later became the Frankish \"dux\" (that is, \"duke\") of Burgundy.\nIn older historiography, it was common to describe Charles as \"illegitimate\". But the dividing line between wives and concubines was not clear-cut in eighth-century Francia, and it is likely that the accusation of \"illegitimacy\" derives from the desire of Pepin's first wife Plectrude to see her progeny as heirs to Pepin's power.\nAfter the reign of Dagobert I (629\u2013639) the Merovingians effectively ceded power to the Pippinid Mayors of the Palace, who ruled the Frankish realm of Austrasia in all but name. They controlled the royal treasury, dispensed patronage, and granted land and privileges in the name of the figurehead king. Charles' father, Pepin of Herstal, was able to unite the Frankish realm by conquering Neustria and Burgundy. Pepin was the first to call himself Duke and Prince of the Franks, a title later taken up by Charles.\nContesting for power.\nIn December 714, Pepin of Herstal died. Prior to his death, he had, at his wife Plectrude's urging, designated Theudoald, his grandson by their late son Grimoald, his heir in the entire realm. This was immediately opposed by the nobles because Theudoald was a child of only eight years of age. To prevent Charles using this unrest to his own advantage, Plectrude had him imprisoned in Cologne, the city which was intended to be her capital. This prevented an uprising on his behalf in Austrasia, but not in Neustria.\nCivil war of 715\u2013718.\nPepin's death occasioned open conflict between his heirs and the Neustrian nobles who sought political independence from Austrasian control. In 715, Dagobert III named Ragenfrid mayor of their palace, effectively declaring political independence. On 26 September 715, Ragenfrid's Neustrians met the young Theudoald's forces at the Battle of Compi\u00e8gne. Theudoald was defeated and fled back to Cologne.\nBefore the end of the year, Charles Martel had escaped from prison and been acclaimed mayor by the nobles of Austrasia. That same year, Dagobert III died and the Neustrians proclaimed Chilperic II, the cloistered son of Childeric II, as king.\nBattle of Cologne.\nIn 716, Chilperic and Ragenfrid together led an army into Austrasia intent on seizing the Pippinid wealth at Cologne. The Neustrians allied with another invading force under Redbad, King of the Frisians and met Charles in battle near Cologne, which was still held by Plectrude. Charles had little time to gather men, or prepare, and the result was inevitable. The Frisians held off Charles, while the king and his mayor besieged Plectrude at Cologne, where she bought them off with a substantial portion of Pepin's treasure. After that they withdrew. The Battle of Cologne is the only defeat of Charles Martel's career.\nBattle of Ambl\u00e8ve.\nCharles retreated to the hills of the Eifel to gather men, and train them. Having made the proper preparations, in April 716, he fell upon the triumphant army near Malmedy as it was returning to its own province. In the ensuing Battle of Ambl\u00e8ve, Martel attacked as the enemy rested at midday. According to one source, he split his forces into several groups which fell at them from many sides. Another suggests that while this was his intention, he then decided, given the enemy's unpreparedness, this was not necessary. In any event, the suddenness of the assault lead them to believe they were facing a much larger host. Many of the enemy fled and Martel's troops gathered the spoils of the camp. Martel's reputation increased considerably as a result, and he attracted more followers. This battle is often considered by historians as the turning point in Charles's struggle.\nBattle of Vincy.\nRichard Gerberding points out that up to this time, much of Martel's support was probably from his mother's kindred in the lands around Liege. After Ambl\u00e8ve, he seems to have won the backing of the influential Willibrord, founder of the Abbey of Echternach. The abbey had been built on land donated by Plectrude's mother, Irmina of Oeren, but most of Willibrord's missionary work had been carried out in Frisia. In joining Chilperic and Ragenfrid, Radbod of Frisia sacked Utrecht, burning churches and killing many missionaries. Willibrord and his monks were forced to flee to Echternach. Gerberding suggests that Willibrord had decided that the chances of preserving his life's work were better with a successful field commander like Martel than with Plectrude in Cologne. Willibrord subsequently baptized Martel's son Pepin. Gerberding suggests a likely date of Easter 716. Martel also received support from bishop Pepo of Verdun.\nCharles took time to rally more men and prepare. By the following spring, Charles had attracted enough support to invade Neustria. Charles sent an envoy who proposed a cessation of hostilities if Chilperic would recognize his rights as mayor of the palace in Austrasia. The refusal was not unexpected but served to impress upon Martel's forces the unreasonableness of the Neustrians. They met near Cambrai at the Battle of Vincy on 21 March 717. The victorious Martel pursued the fleeing king and mayor to Paris, but as he was not yet prepared to hold the city, he turned back to deal with Plectrude and Cologne. He took the city and dispersed her adherents. Plectrude was allowed to retire to a convent. Theudoald lived to 741 under his uncle's protection, a kindness unusual for those times, when mercy to a former gaoler, or a potential rival, was rare.\nConsolidation of power.\nUpon this success, Charles proclaimed Chlothar IV king of Austrasia in opposition to Chilperic and deposed Rigobert, archbishop of Reims, replacing him with Milo, a lifelong supporter.\nIn 718, Chilperic responded to Charles' new ascendancy by making an alliance with Odo the Great (or Eudes, as he is sometimes known), the duke of Aquitaine, who had become independent during the civil war in 715, but was again defeated, at the Battle of Soissons, by Charles. Chilperic fled with his ducal ally to the land south of the Loire and Ragenfrid fled to Angers. Soon Chlotar IV died and Odo surrendered King Chilperic in exchange for Charles recognizing his dukedom. Charles recognized Chilperic as king of the Franks in return for legitimate royal affirmation of his own mayoralty over all the kingdoms.\nWars of 718\u2013732.\nBetween 718 and 732, Charles secured his power through a series of victories. Having unified the Franks under his banner, Charles was determined to punish the Saxons who had invaded Austrasia. Therefore, late in 718, he laid waste their country to the banks of the Weser, the Lippe, and the Ruhr. He defeated them in the Teutoburg Forest and thus secured the Frankish border in the name of King Chlotaire.\nWhen the Frisian leader Radbod died in 719, Charles seized West Frisia without any great resistance on the part of the Frisians, who had been subjected to the Franks but had rebelled upon the death of Pippin. When Chilperic II died in 721, Charles appointed as his successor the son of Dagobert III, Theuderic IV, who was still a minor, and who occupied the throne from 721 to 737. Charles was now appointing the kings whom he supposedly served (\"rois fain\u00e9ants\") although they were mere figureheads. By the end of his reign, he didn't appoint any at all. At this time, Charles again marched against the Saxons. Then the Neustrians rebelled under Ragenfrid, who had left the county of Anjou. They were easily defeated in 724 but Ragenfrid gave up his sons as hostages in turn for keeping his county. This ended the civil wars of Charles' reign.\nThe next six years were devoted in their entirety to assuring Frankish authority over the neighboring political groups. Between 720 and 723, Charles was fighting in Bavaria, where the Agilolfing dukes had gradually evolved into independent rulers, recently in alliance with Liutprand the Lombard. He forced the Alemanni to accompany him, and Duke Hugbert submitted to Frankish suzerainty. In 725 he brought back the Agilolfing Princess Swanachild as a second wife.\nIn 725 and 728, he again entered Bavaria but, in 730, he marched against Lantfrid, Duke of Alemannia, who had also become independent, and killed him in battle. He forced the Alemanni to capitulate to Frankish suzerainty and did not appoint a successor to Lantfrid. Thus, southern Germany once more became part of the Frankish kingdom, as had northern Germany during the first years of the reign.\nAquitaine and the Battle of Tours in 732.\nIn 731, after defeating the Saxons, Charles turned his attention to the rival southern realm of Aquitaine, and crossed the Loire, breaking the treaty with Duke Odo. The Franks ransacked Aquitaine twice, and captured Bourges, although Odo retook it. The \"Continuations of Fredegar\" allege that Odo called on assistance from the recently established emirate of al-Andalus, but there had been Arab raids into Aquitaine from the 720s onwards. Indeed, the anonymous Chronicle of 754 records that in 721 a victory of Odo at the Battle of Toulouse, while the \"Liber Pontificalis\" records that Odo had killed 375,000 Saracens. It is more likely that this invasion or raid took place in revenge for Odo's support for a rebel Berber leader named Munnuza.\nWhatever the precise circumstances were, it is clear that an army under the leadership of Abd al-Rahman al-Ghafiqi headed north, and after some minor engagements marched on the wealthy city of Tours. According to British medieval historian Paul Fouracre, \"Their campaign should perhaps be interpreted as a long-distance raid rather than the beginning of a war\". They were, however, defeated by the army of Charles at a location between Tours and Poitiers, in a victory described by the \"Continuations of Fredegar\". News of this battle spread, and may be recorded in Bede's \"Ecclesiastical History\" (Book V, ch. 23). However, it is not given prominence in Arabic sources from the period.\nDespite his victory, Charles did not gain full control of Aquitaine, and Odo remained duke until 735.\nWars of 732\u2013737.\nBetween his victory of 732 and 735, Charles reorganized the kingdom of Burgundy, replacing the counts and dukes with his loyal supporters, thus strengthening his hold on power. He was forced, by the ventures of Bubo, Duke of the Frisians, to invade independent-minded Frisia again in 734. In that year, he slew the duke at the Battle of the Boarn. Charles ordered the Frisian pagan shrines destroyed, and so wholly subjugated the populace that the region was peaceful for twenty years after.\nIn 735, Duke Odo of Aquitaine died. Though Charles wished to rule the duchy directly and went there to elicit the submission of the Aquitainians, the aristocracy proclaimed Odo's son, Hunald I of Aquitaine, as duke, and Charles and Hunald eventually recognised each other's position.\nInterregnum (737-741).\nIn 737, at the tail end of his campaigning in Provence and Septimania, the Merovingian king, Theuderic IV, died. Charles, titling himself \"maior domus\" and \"princeps et dux Francorum\", did not appoint a new king and nobody acclaimed one. The throne lay vacant until Charles' death. The interregnum, the final four years of Charles' life, was relatively peaceful although in 738 he compelled the Saxons of Westphalia to submit and pay tribute and in 739 he checked an uprising in Provence where some rebels united under the leadership of Maurontus.\nCharles used the relative peace to set about integrating the outlying realms of his empire into the Frankish church. He erected four dioceses in Bavaria (Salzburg, Regensburg, Freising, and Passau) and gave them Boniface as archbishop and metropolitan over all Germany east of the Rhine, with his seat at Mainz. Boniface had been under his protection from 723 on. Indeed, the saint himself explained to his old friend, Daniel of Winchester, that without it he could neither administer his church, defend his clergy nor prevent idolatry.\nIn 739, Pope Gregory III begged Charles for his aid against Liutprand, but Charles was loath to fight his onetime ally and ignored the plea. Nonetheless, the pope's request for Frankish protection showed how far Charles had come from the days he was tottering on excommunication and set the stage for his son and grandson to assert themselves in the peninsula.\nDeath and transition in rule.\nCharles Martel died on 22 October 741, at Quierzy-sur-Oise in what is today the Aisne \"d\u00e9partement\" in the Picardy region of France. He was buried at Saint Denis Basilica in Paris.\nHis territories had been divided among his adult sons a year earlier: to Carloman he gave Austrasia, Alemannia, and Thuringia, and to Pippin the Younger Neustria, Burgundy, Provence, and Metz and Trier in the \"Mosel duchy\". Grifo was given several lands throughout the kingdom, but at a later date, just before Charles died.\nLegacy.\nEarlier in his life Charles Martel had many internal opponents and felt the need to appoint his own kingly claimant, Chlotar IV. Later, however, the dynamics of rulership in Francia had changed, and no hallowed Merovingian ruler was required. Charles divided his realm between his sons without opposition (though he ignored his young son Bernard). For many historians, Charles Martel laid the foundations for his son Pepin's rise to the Frankish throne in 751, and his grandson Charlemagne's imperial acclamation in 800. However, for Paul Fouracre, while Charles was \"the most effective military leader in Francia\", his career \"finished on a note of unfinished business\".\nOrder of the Genet.\nSome historical sources say that Charles Martel formed the first regular order of knights in France. They hold that among the spoils Charles Martel's forces captured after the Battle of Tours were many genets (raised for their fur) and several of their pelts. These were presented to him and found favor in his eyes due to their soft fine fur and pleasant smell (the fur was valued by aristocrats to serve as inner lining for garments). As marks of his favor, Charles Martel distributed some of the genets to leaders among his army. Soon after, to commemorate the great victory, he began the first Order of Knighthood in France \u2013 called the \"Order of the Genet\". The order was limited to fifteen knights at a time. Charles Martel served as its Chief and that office was handed down to heirs in his bloodline. This order of knights continued for little over two centuries, when it was replaced by Robert II of France's new order: the Knights of Our Lady of the Star (named in honor of his devotion to the Virgin Mary). Some historians question if the story of the captured genets is a fabrication and that the order was named after small Arabian horses, while others challenge the historical existence of the order altogether.\nFamily and children.\nCharles Martel married twice, his first wife being Rotrude of Treves, daughter either of Lambert II, Count of Hesbaye, or of Leudwinus, Count of Treves. They had the following children:\nMost of the children married and had issue. Hiltrud married Odilo I (Duke of Bavaria). Landrade was once believed to have married a Sigrand (Count of Hesbania) but Sigrand's wife was more likely the sister of Rotrude. Auda married Thierry IV (Count of Autun and Toulouse).\nCharles also married a second time, to Swanhild and they had a child named Grifo.\nCharles Martel also had a known mistress, Ruodhaid with whom he had Bernard, Hieronymus and Remigius. Remigius became an archbishop of Rouen.\nReputation and historiography.\nFor early medieval authors, Charles Martel was famous for his military victories. Paul the Deacon for instance attributed a victory against the Saracens actually won by Odo of Aquitaine to Charles. However, alongside this there soon developed a darker reputation, for his alleged abuse of church property. A ninth-century text, the \"Visio Eucherii\", possibly written by Hincmar of Reims, portrayed Martel as suffering in hell for this reason. According to British medieval historian Paul Fouracre, this was \"the single most important text in the construction of Charles Martel's reputation as a seculariser or despoiler of church lands\".\nBy the eighteenth century, historians such as Edward Gibbon had begun to portray the Frankish leader as the saviour of Christian Europe from a full-scale Islamic invasion. In Gibbon's \"The Decline And Fall Of The Roman Empire\" he wonders whether without Charles' victory, \"Perhaps the interpretation of the Koran would now be taught in the schools of Oxford\".\nIn the nineteenth century, the German historian Heinrich Brunner argued that Charles had confiscated church lands in order to fund military reforms that allowed him to defeat the Arab conquests, in this way brilliantly combining two traditions about the ruler. However, Fouracre argued that \"...there is not enough evidence to show that there was a decisive change either in the way in which the Franks fought, or in the way in which they organised the resources needed to support their warriors.\"\nMany twentieth-century European historians continued to develop Gibbon's perspectives, such as French medievalist Christian Pfister, who wrote in 1911 that\nSimilarly, William E. Watson who wrote of the battle's importance in Frankish and world history in 1993, suggested that\nOther recent historians however argue that the importance of the battle is dramatically overstated, both for European history in general and for Charles Martel's reign in particular. This view is typified by Alessandro Barbero, who in 2004 wrote,\nSimilarly, in 2002 Toma\u017e Mastnak wrote: \nMore recently, the memory of Charles Martel has been appropriated by far right and white nationalist groups, such as the 'Charles Martel Group' in France, and by Australian Brenton Harrison Tarrant, the perpetrator of the Christchurch mosque shootings at Al Noor Mosque and Linwood Islamic Centre in Christchurch, New Zealand, in 2019."}
{"id": "6456", "revid": "39569616", "url": "https://en.wikipedia.org/wiki?curid=6456", "title": "Charles Edward Jones", "text": "Colonel Charles Edward (\"Chuck\") Jones (November 8, 1952 \u2013 September 11, 2001) was a United States Air Force officer, a computer programmer, and an astronaut in the USAF Manned Spaceflight Engineer Program.\nLife.\nJones was born November 8, 1952, in Clinton, Indiana. He graduated from Wichita East High School in 1970, earned a Bachelor of Science degree in Astronautical Engineering from the United States Air Force Academy in 1974, and received a Master of Science degree in Astronautics from MIT in 1980. He entered the USAF Manned Spaceflight Engineer program in 1982, and was scheduled to fly on mission STS-71-B in December 1986, but the mission was cancelled after the \"Challenger\" Disaster in January 1986. He left the Manned Spaceflight Engineer program in 1987.\nHe later worked for Defense Intelligence Agency, Bolling AFB in Washington D.C., and was Systems Program Director for Intelligence and Information Systems, Hanscom AFB, Massachusetts.\nHe was killed at the age of 48 in the attacks of September 11, 2001, aboard American Airlines Flight 11. He had been living as a retired U.S. Air Force Colonel in Bedford, Massachusetts, at the time of his death. He was survived by his wife Jeanette.\nAt the National 9/11 Memorial, Jones is memorialized at the North Pool, on Panel N-74."}
{"id": "6458", "revid": "33115362", "url": "https://en.wikipedia.org/wiki?curid=6458", "title": "Ceramic", "text": "A ceramic is any of the various hard, brittle, heat-resistant and corrosion-resistant materials made by shaping and then firing a nonmetallic mineral, such as clay, at a high temperature. Common examples are earthenware, porcelain, and brick.\nThe crystallinity of ceramic materials ranges from highly oriented to semi-crystalline, vitrified, and often completely amorphous (glasses). Most often, fired ceramics are either vitrified or semi-vitrified as is the case with earthenware, stoneware, and porcelain. Varying crystallinity and electron composition in the ionic and covalent bonds cause most ceramic materials to be good thermal and electrical insulators (researched in ceramic engineering). With such a large range of possible options for the composition/structure of a ceramic (nearly all of the elements, nearly all types of bonding, and all levels of crystallinity), the breadth of the subject is vast, and identifiable attributes (hardness, toughness, electrical conductivity) are difficult to specify for the group as a whole. General properties such as high melting temperature, high hardness, poor conductivity, high moduli of elasticity, chemical resistance and low ductility are the norm, with known exceptions to each of these rules (piezoelectric ceramics, glass transition temperature, superconductive ceramics). Many composites, such as fiberglass and carbon fiber, while containing ceramic materials are not considered to be part of the ceramic family. \nThe earliest ceramics made by humans were pottery objects (\"pots\" or \"vessels\") or figurines made from clay, either by itself or mixed with other materials like silica, hardened and sintered in fire. Later, ceramics were glazed and fired to create smooth, colored surfaces, decreasing porosity through the use of glassy, amorphous ceramic coatings on top of the crystalline ceramic substrates. Ceramics now include domestic, industrial and building products, as well as a wide range of ceramic art. In the 20th century, new ceramic materials were developed for use in advanced ceramic engineering, such as in semiconductors.\nThe word \"ceramic\" comes from the Greek word (), \"of pottery\" or \"for pottery\", from (), \"potter's clay, tile, pottery\". The earliest known mention of the root \"ceram-\" is the Mycenaean Greek , workers of ceramic written in Linear B syllabic script. The word \"ceramic\" may be used as an adjective to describe a material, product or process, or it may be used as a noun, either singular, or more commonly, as the plural noun \"ceramics\".\nMaterials.\nCeramic material is an inorganic, non-metallic, often crystalline oxide, nitride, or carbide material. Some elements, such as carbon or silicon, may be considered ceramics. Ceramic materials are brittle, hard, strong in compression, and weak in shearing and tension. They withstand chemical erosion that occurs in other materials subjected to acidic or caustic environments. Ceramics generally can withstand very high temperatures, ranging from 1,000\u00a0\u00b0C to 1,600\u00a0\u00b0C (1,800\u00a0\u00b0F to 3,000\u00a0\u00b0F). Glass is often not considered a ceramic because of its amorphous (noncrystalline) character. However, glassmaking involves several steps of the ceramic process, and its mechanical properties are similar to ceramic materials.\nTraditional ceramic raw materials include clay minerals such as kaolinite, whereas more recent materials include aluminum oxide, more commonly known as alumina. The modern ceramic materials, which are classified as advanced ceramics, include silicon carbide and tungsten carbide. Both are valued for their abrasion resistance and hence find use in applications such as the wear plates of crushing equipment in mining operations. Advanced ceramics are also used in the medicine, electrical, electronics industries, and body armor.\nCrystalline ceramics.\nCrystalline ceramic materials are not amenable to a great range of processing. Methods for dealing with them tend to fall into one of two categories \u2013 either make the ceramic in the desired shape, by reaction \"in situ\", or by \"forming\" powders into the desired shape, and then sintering to form a solid body. Ceramic forming techniques include shaping by hand (sometimes including a rotation process called \"throwing\"), slip casting, tape casting (used for making very thin ceramic capacitors), injection molding, dry pressing, and other variations. \nNoncrystalline ceramics.\nNoncrystalline ceramics, being glass, tend to be formed from melts. The glass is shaped when either fully molten, by casting, or when in a state of toffee-like viscosity, by methods such as blowing into a mold. If later heat treatments cause this glass to become partly crystalline, the resulting material is known as a glass-ceramic, widely used as cook-tops, and also as a glass composite material for nuclear waste disposal.\nHistory.\nHuman beings appear to have been making their own ceramics for at least 26,000 years, subjecting clay and silica to intense heat to fuse and form ceramic materials. The earliest found so far were in southern central Europe and were sculpted figures, not dishes. The earliest known pottery was made by mixing animal products with clay and baked in kilns at up to 800\u00b0C. While actual pottery fragments have been found up to 19,000 years old, it was not until about ten thousand years later that regular pottery became common. An early people that spread across much of Europe is named after its use of pottery, the Corded Ware culture. These early Indo-European peoples decorated their pottery by wrapping it with rope, while still wet. When the ceramics were fired, the rope burned off but left a decorative pattern of complex grooves on the surface.\nThe invention of the wheel eventually led to the production of smoother, more even pottery using the wheel-forming technique, like the pottery wheel. Early ceramics were porous, absorbing water easily. It became useful for more items with the discovery of glazing techniques, coating pottery with silicon, bone ash, or other materials that could melt and reform into a glassy surface, making a vessel less pervious to water. \nArchaeology.\nCeramic artifacts have an important role in archaeology for understanding the culture, technology, and behavior of peoples of the past. They are among the most common artifacts to be found at an archaeological site, generally in the form of small fragments of broken pottery called sherds. Processing of collected sherds can be consistent with two main types of analysis: technical and traditional.\nThe traditional analysis involves sorting ceramic artifacts, sherds, and larger fragments into specific types based on style, composition, manufacturing, and morphology. By creating these typologies, it is possible to distinguish between different cultural styles, the purpose of the ceramic, and the technological state of the people among other conclusions. Besides, by looking at stylistic changes of ceramics over time is it possible to separate (seriate) the ceramics into distinct diagnostic groups (assemblages). A comparison of ceramic artifacts with known dated assemblages allows for a chronological assignment of these pieces.\nThe technical approach to ceramic analysis involves a finer examination of the composition of ceramic artifacts and sherds to determine the source of the material and through this the possible manufacturing site. Key criteria are the composition of the clay and the temper used in the manufacture of the article under study: the temper is a material added to the clay during the initial production stage, and it is used to aid the subsequent drying process. Types of temper include shell pieces, granite fragments, and ground sherd pieces called 'grog'. Temper is usually identified by microscopic examination of the tempered material. Clay identification is determined by a process of refiring the ceramic and assigning a color to it using Munsell Soil Color notation. By estimating both the clay and temper compositions, and locating a region where both are known to occur, an assignment of the material source can be made. From the source assignment of the artifact, further investigations can be made into the site of manufacture.\nProperties.\nThe physical properties of any ceramic substance are a direct result of its crystalline structure and chemical composition. Solid-state chemistry reveals the fundamental connection between microstructure and properties, such as localized density variations, grain size distribution, type of porosity, and second-phase content, which can all be correlated with ceramic properties such as mechanical strength \u03c3 by the Hall-Petch equation, hardness, toughness, dielectric constant, and the optical properties exhibited by transparent materials.\nCeramography is the art and science of preparation, examination, and evaluation of ceramic microstructures. Evaluation and characterization of ceramic microstructures are often implemented on similar spatial scales to that used commonly in the emerging field of nanotechnology: from tens of angstroms (A) to tens of micrometers (\u00b5m). This is typically somewhere between the minimum wavelength of visible light and the resolution limit of the naked eye.\nThe microstructure includes most grains, secondary phases, grain boundaries, pores, micro-cracks, structural defects, and hardness micro indentions. Most bulk mechanical, optical, thermal, electrical, and magnetic properties are significantly affected by the observed microstructure. The fabrication method and process conditions are generally indicated by the microstructure. The root cause of many ceramic failures is evident in the cleaved and polished microstructure. Physical properties which constitute the field of materials science and engineering include the following:\nMechanical properties.\nMechanical properties are important in structural and building materials as well as textile fabrics. In modern materials science, fracture mechanics is an important tool in improving the mechanical performance of materials and components. It applies the physics of stress and strain, in particular the theories of elasticity and plasticity, to the microscopic crystallographic defects found in real materials in order to predict the macroscopic mechanical failure of bodies. Fractography is widely used with fracture mechanics to understand the causes of failures and also verify the theoretical failure predictions with real-life failures.\nCeramic materials are usually ionic or covalent bonded materials, and can be crystalline or amorphous. A material held together by either type of bond will tend to fracture before any plastic deformation takes place, which results in poor toughness in these materials. Additionally, because these materials tend to be porous, the pores and other microscopic imperfections act as stress concentrators, decreasing the toughness further, and reducing the tensile strength. These combine to give catastrophic failures, as opposed to the more ductile failure modes of metals.\nThese materials do show plastic deformation. However, because of the rigid structure of the crystalline materials, there are very few available slip systems for dislocations to move, and so they deform very slowly. With the non-crystalline (glassy) materials, viscous flow is the dominant source of plastic deformation, and is also very slow. It is therefore neglected in many applications of ceramic materials.\nTo overcome the brittle behavior, ceramic material development has introduced the class of ceramic matrix composite materials, in which ceramic fibers are embedded and with specific coatings are forming fiber bridges across any crack. This mechanism substantially increases the fracture toughness of such ceramics. Ceramic disc brakes are an example of using a ceramic matrix composite material manufactured with a specific process.\nIce-templating for enhanced mechanical properties.\nIf ceramic is subjected to substantial mechanical loading, it can undergo a process called ice-templating, which allows some control of the microstructure of the ceramic product and therefore some control of the mechanical properties. Ceramic engineers use this technique to tune the mechanical properties to their desired application. Specifically, strength is increased, when this technique is employed. Ice templating allows the creation of macroscopic pores in a unidirectional arrangement. The applications of this oxide strengthening technique are important for solid oxide fuel cells and water filtration devices. \nTo process a sample through ice templating, an aqueous colloidal suspension is prepared to contain the dissolved ceramic powder evenly dispersed throughout the colloid, for example Yttria-stabilized zirconia (YSZ). The solution is then cooled from the bottom to the top on a platform that allows for unidirectional cooling. This forces ice crystals to grow in compliance with the unidirectional cooling and these ice crystals force the dissolved YSZ particles to the solidification front of the solid-liquid interphase boundary, resulting in pure ice crystals lined up unidirectionally alongside concentrated pockets of colloidal particles. The sample is then simultaneously heated and the pressure is reduced enough to force the ice crystals to sublimate and the YSZ pockets begin to anneal together to form macroscopically aligned ceramic microstructures. The sample is then further sintered to complete the evaporation of the residual water and the final consolidation of the ceramic microstructure. \nDuring ice-templating, a few variables can be controlled to influence the pore size and morphology of the microstructure. These important variables are the initial solids loading of the colloid, the cooling rate, the sintering temperature and duration, and the use of certain additives which can influence the microstructural morphology during the process. A good understanding of these parameters is essential to understanding the relationships between processing, microstructure, and mechanical properties of anisotropically porous materials.\nElectrical properties.\nSemiconductors.\nSome ceramics are semiconductors. Most of these are transition metal oxides that are II-VI semiconductors, such as zinc oxide. While there are prospects of mass-producing blue LEDs from zinc oxide, ceramicists are most interested in the electrical properties that show grain boundary effects. One of the most widely used of these is the varistor. These are devices that exhibit the property that resistance drops sharply at a certain threshold voltage. Once the voltage across the device reaches the threshold, there is a breakdown of the electrical structure in the vicinity of the grain boundaries, which results in its electrical resistance dropping from several megohms down to a few hundred ohms. The major advantage of these is that they can dissipate a lot of energy, and they self-reset; after the voltage across the device drops below the threshold, its resistance returns to being high. This makes them ideal for surge-protection applications; as there is control over the threshold voltage and energy tolerance, they find use in all sorts of applications. The best demonstration of their ability can be found in electrical substations, where they are employed to protect the infrastructure from lightning strikes. They have rapid response, are low maintenance, and do not appreciably degrade from use, making them virtually ideal devices for this application. Semiconducting ceramics are also employed as gas sensors. When various gases are passed over a polycrystalline ceramic, its electrical resistance changes. With tuning to the possible gas mixtures, very inexpensive devices can be produced.\nSuperconductivity.\nUnder some conditions, such as extremely low temperature, some ceramics exhibit high-temperature superconductivity. The reason for this is not understood, but there are two major families of superconducting ceramics.\nFerroelectricity and supersets.\nPiezoelectricity, a link between electrical and mechanical response, is exhibited by a large number of ceramic materials, including the quartz used to measure time in watches and other electronics. Such devices use both properties of piezoelectrics, using electricity to produce a mechanical motion (powering the device) and then using this mechanical motion to produce electricity (generating a signal). The unit of time measured is the natural interval required for electricity to be converted into mechanical energy and back again.\nThe piezoelectric effect is generally stronger in materials that also exhibit pyroelectricity, and all pyroelectric materials are also piezoelectric. These materials can be used to inter-convert between thermal, mechanical, or electrical energy; for instance, after synthesis in a furnace, a pyroelectric crystal allowed to cool under no applied stress generally builds up a static charge of thousands of volts. Such materials are used in motion sensors, where the tiny rise in temperature from a warm body entering the room is enough to produce a measurable voltage in the crystal.\nIn turn, pyroelectricity is seen most strongly in materials that also display the ferroelectric effect, in which a stable electric dipole can be oriented or reversed by applying an electrostatic field. Pyroelectricity is also a necessary consequence of ferroelectricity. This can be used to store information in ferroelectric capacitors, elements of ferroelectric RAM.\nThe most common such materials are lead zirconate titanate and barium titanate. Aside from the uses mentioned above, their strong piezoelectric response is exploited in the design of high-frequency loudspeakers, transducers for sonar, and actuators for atomic force and scanning tunneling microscopes.\nPositive thermal coefficient.\nTemperature increases can cause grain boundaries to suddenly become insulating in some semiconducting ceramic materials, mostly mixtures of heavy metal titanates. The critical transition temperature can be adjusted over a wide range by variations in chemistry. In such materials, current will pass through the material until joule heating brings it to the transition temperature, at which point the circuit will be broken and current flow will cease. Such ceramics are used as self-controlled heating elements in, for example, the rear-window defrost circuits of automobiles.\nAt the transition temperature, the material's dielectric response becomes theoretically infinite. While a lack of temperature control would rule out any practical use of the material near its critical temperature, the dielectric effect remains exceptionally strong even at much higher temperatures. Titanates with critical temperatures far below room temperature have become synonymous with \"ceramic\" in the context of ceramic capacitors for just this reason.\nOptical properties.\nOptically transparent materials focus on the response of a material to incoming light waves of a range of wavelengths. Frequency selective optical filters can be utilized to alter or enhance the brightness and contrast of a digital image. Guided lightwave transmission via frequency selective waveguides involves the emerging field of fiber optics and the ability of certain glassy compositions as a transmission medium for a range of frequencies simultaneously (multi-mode optical fiber) with little or no interference between competing wavelengths or frequencies. This resonant mode of energy and data transmission via electromagnetic (light) wave propagation, though low powered, is virtually lossless. Optical waveguides are used as components in Integrated optical circuits (e.g. light-emitting diodes, LEDs) or as the transmission medium in local and long haul optical communication systems. Also of value to the emerging materials scientist is the sensitivity of materials to radiation in the thermal infrared (IR) portion of the electromagnetic spectrum. This heat-seeking ability is responsible for such diverse optical phenomena as Night-vision and IR luminescence.\nThus, there is an increasing need in the military sector for high-strength, robust materials which have the capability to transmit light (electromagnetic waves) in the visible (0.4 \u2013 0.7 micrometers) and mid-infrared (1 \u2013 5 micrometers) regions of the spectrum. These materials are needed for applications requiring transparent armor, including next-generation high-speed missiles and pods, as well as protection against improvised explosive devices (IED).\nIn the 1960s, scientists at General Electric (GE) discovered that under the right manufacturing conditions, some ceramics, especially aluminium oxide (alumina), could be made translucent. These translucent materials were transparent enough to be used for containing the electrical plasma generated in high-pressure sodium street lamps. During the past two decades, additional types of transparent ceramics have been developed for applications such as nose cones for heat-seeking missiles, windows for fighter aircraft, and scintillation counters for computed tomography scanners.\nIn the early 1970s, Thomas Soules pioneered computer modeling of light transmission through translucent ceramic alumina. His model showed that microscopic pores in ceramic, mainly trapped at the junctions of microcrystalline grains, caused the light to scatter and prevented true transparency. The volume fraction of these microscopic pores had to be less than 1% for high-quality optical transmission.\nThis is basically a particle size effect. Opacity results from the incoherent scattering of light at surfaces and interfaces. In addition to pores, most of the interfaces in a typical metal or ceramic object are in the form of grain boundaries which separate tiny regions of crystalline order. When the size of the scattering center (or grain boundary) is reduced below the size of the wavelength of the light being scattered, the scattering no longer occurs to any significant extent.\nIn the formation of polycrystalline materials (metals and ceramics) the size of the crystalline grains is determined largely by the size of the crystalline particles present in the raw material during formation (or pressing) of the object. Moreover, the size of the grain boundaries scales directly with particle size. Thus a reduction of the original particle size below the wavelength of visible light (~ 0.5 micrometers for shortwave violet) eliminates any light scattering, resulting in a transparent material.\nRecently, Japanese scientists have developed techniques to produce ceramic parts that rival the transparency of traditional crystals (grown from a single seed) and exceed the fracture toughness of a single crystal. In particular, scientists at the Japanese firm Konoshima Ltd., a producer of ceramic construction materials and industrial chemicals, have been looking for markets for their transparent ceramics.\nLivermore researchers realized that these ceramics might greatly benefit high-powered lasers used in the National Ignition Facility (NIF) Programs Directorate. In particular, a Livermore research team began to acquire advanced transparent ceramics from Konoshima to determine if they could meet the optical requirements needed for Livermore's Solid-State Heat Capacity Laser (SSHCL). Livermore researchers have also been testing applications of these materials for applications such as advanced drivers for laser-driven fusion power plants.\nExamples.\nA composite material of ceramic and metal is known as cermet.\nOther ceramic materials, generally requiring greater purity in their make-up than those above, include forms of several chemical compounds, including:\nProducts.\nBy usage.\nFor convenience, ceramic products are usually divided into four main types; these are shown below with some examples:\nCeramics made with clay.\nFrequently, the raw materials of modern ceramics do not include clays.\nThose that do are classified as follows:\nClassification.\nCeramics can also be classified into three distinct material categories: \nEach one of these classes can be developed into unique material properties because ceramics tend to be crystalline."}
{"id": "6459", "revid": "1018542544", "url": "https://en.wikipedia.org/wiki?curid=6459", "title": "Wuxing (Chinese philosophy)", "text": "Wuxing (), usually translated as Five Phases, is a fivefold conceptual scheme that many traditional Chinese fields used to explain a wide array of phenomena, from cosmic cycles to the interaction between internal organs, and from the succession of political regimes to the properties of medicinal drugs. The \"Five Phases\" are Fire ( \"hu\u01d2\"), Water ( \"shu\u01d0\"), Wood ( \"m\u00f9\"), Metal or Gold ( \"j\u012bn\"), and Earth or Soil ( \"t\u01d4\"). This order of presentation is known as the \"Days of the Week\" sequence. In the order of \"mutual generation\" ( \"xi\u0101ngsh\u0113ng\"), they are Wood, Fire, Earth, Metal, and Water. In the order of \"mutual overcoming\" ( \"xi\u0101ngk\u00e8\"), they are Wood, Earth, Water, Fire, and Metal.\nThe system of five phases was used for describing interactions and relationships between phenomena. After it came to maturity in the second or first century BCE during the Han dynasty, this device was employed in many fields of early Chinese thought, including seemingly disparate fields such as Yi jing divination, alchemy, feng shui, astrology, traditional Chinese medicine, music, military strategy, and martial arts.\nNames.\n\"X\u00edng\" () of \"w\u01d4x\u00edng\" () means moving; a planet is called a 'moving star' ( \"x\u00edngx\u012bng\") in Chinese. W\u01d4x\u00edng originally refers to the five major planets (Jupiter, Saturn, Mercury, Mars, Venus) that create five dimensions of earth life. \"W\u01d4x\u00edng\" is also widely translated as \"Five Elements\" and this is used extensively by many including practitioners of Five Element acupuncture. This translation arose by false analogy with the Western system of the four elements. Whereas the classical Greek elements were concerned with substances or natural qualities, the Chinese \"x\u00edng\" are \"primarily concerned with process and change,\" hence the common translation as \"phases\" or \"agents\". By the same token, \"M\u00f9\" is thought of as \"Tree\" rather than \"Wood\". The word \"element\" is thus used within the context of Chinese medicine with a different meaning to its usual meaning.\nIt should be recognized that the word \"phase\", although commonly preferred, is not perfect. \"Phase\" is a better translation for the five \"seasons\" ( \"w\u01d4y\u00f9n\") mentioned below, and so \"agents\" or \"processes\" might be preferred for the primary term \"x\u00edng\". Manfred Porkert attempts to resolve this by using \"Evolutive Phase\" for \"w\u01d4x\u00edng\" and \"Circuit Phase\" for \"w\u01d4y\u00f9n\", but these terms are unwieldy.\nSome of the Mawangdui Silk Texts (no later than 168 BC) also present the \"w\u01d4x\u00edng\" as \"five virtues\" or types of activities. Within Chinese medicine texts the \"w\u01d4x\u00edng\" are also referred to as \"w\u01d4y\u01d4n\" () or a combination of the two characters ( w\u01d4x\u00edngy\u01d4n) these emphasise the correspondence of five elements to five 'seasons' (four seasons plus one). Another tradition refers to the \"w\u01d4x\u00edng\" as \"w\u01d4d\u00e9\" (), the .\nThe phases.\nThe five phases are around 73 days each and are usually used to describe the state in nature:\nCycles.\nThe doctrine of five phases describes two cycles, a generating or creation ( \"sh\u0113ng\") cycle, also known as \"mother-son\", and an overcoming or destruction ( \"k\u00e8\") cycle, also known as \"grandfather-grandson\", of interactions between the phases. Within Chinese medicine the effects of these two main relations are further elaborated:\nInter-promoting.\nCommon verbs for the \"sh\u0113ng\" cycle include \"generate\", \"create\" or \"strengthens\", as well as \"grow\" or \"promote\". The phase interactions in the \"sh\u0113ng\" cycle are:\nWeakening.\nA deficient \"sh\u0113ng\" cycle is called the \"xi\u00e8\" cycle and is the reverse of the \"sh\u0113ng\" cycle. Common verbs for the \"xi\u00e8\" include \"weaken\", \"drain\", \"diminish\" or \"exhaust\". The phase interactions in the \"xi\u00e8\" cycle are:\nInter-regulating.\nCommon verbs for the \"k\u00e8\" cycle include \"controls\", \"restrains\" and \"fathers\", as well as \"overcome\" or \"regulate\". The phase interactions in the \"k\u00e8\" cycle are:\nOveracting.\nAn excessive \"k\u00e8\" cycle is called the \"ch\u00e9ng\" cycle. Common verbs for the \"ch\u00e9ng\" cycle include \"restrict\", \"overwhelm\", \"dominate\" or \"destroy\". The phase interactions in the \"ch\u00e9ng\" cycle are:\nCounteracting.\nA deficient \"k\u00e8\" cycle is called the \"w\u01d4\" cycle and is the reverse of the \"k\u00e8\" cycle. Common verbs for the \"w\u01d4\" cycle can include \"insult\" or \"harm\". The phase interactions in the \"w\u01d4\" cycle are:\nCosmology and \"feng shui\".\nAccording to wuxing theory, the structure of the cosmos mirrors the five phases. Each phase has a complex series of associations with different aspects of nature, as can be seen in the following table. In the ancient Chinese form of geomancy, known as Feng Shui, practitioners all based their art and system on the five phases (wuxing). All of these phases are represented within the trigrams. Associated with these phases are colors, seasons and shapes; all of which are interacting with each other.\nBased on a particular directional energy flow from one phase to the next, the interaction can be expansive, destructive, or exhaustive. A proper knowledge of each aspect of energy flow will enable the Feng Shui practitioner to apply certain cures or rearrangement of energy in a way they believe to be beneficial for the receiver of the Feng Shui Treatment.\nDynastic transitions.\nAccording to the Warring States period political philosopher Zou Yan (c. 305\u2013240 BCE), each of the five elements possesses a personified \"virtue\" (\"de\" ), which indicates the foreordained destiny (\"yun\" ) of a dynasty; accordingly, the cyclic succession of the elements also indicates dynastic transitions. Zou Yan claims that the Mandate of Heaven sanctions the legitimacy of a dynasty by sending self-manifesting auspicious signs in the ritual color (yellow, blue, white, red, and black) that matches the element of the new dynasty (Earth, Wood, Metal, Fire, and Water). From the Qin dynasty onward, most Chinese dynasties invoked the theory of the Five Elements to legitimize their reign.\nChinese medicine.\nThe interdependence of zang-fu networks in the body was said to be a circle of five things, and so mapped by the Chinese doctors onto the five phases.\nIn order to explain the integrity and complexity of the human body, Chinese medical scientists and physicians use the Five Elements theory to classify the human body's endogenous influences on organs, physiological activities, pathological reactions, and environmental or exogenous influences.This diagnostic capacity is extensively used in traditional five phase acupunture today, as opposed to the modern eight principal based Traditional Chinese medicine.\nCelestial stem.\nMing neiyin.\nIn Ziwei, \"neiyin\" () or the method of divination is the further classification of the Five Elements into 60 \"ming\" (), or life orders, based on the ganzhi. Similar to the astrology zodiac, the ming is used by fortune-tellers to analyse a person's personality and future fate.\nMusic.\nThe \"Yu\u00e8l\u00ecng\" chapter () of the \"L\u01d0j\u00ec\" () and the \"Hu\u00e1in\u00e1nz\u01d0\" () make the following correlations:\nMartial arts.\nT'ai chi ch'uan uses the five elements to designate different directions, positions or footwork patterns. Either forward, backward, left, right and centre, or three steps forward (attack) and two steps back (retreat).\nThe Five Steps ( w\u01d4 b\u00f9):\nXingyiquan uses the five elements metaphorically to represent five different states of combat.\nWuxing heqidao, Gogyo Aikido (\u4e94\u884c\u5408\u6c14\u9053-Chinese) is an art form with its roots in Confucian, Taoists and Buddhist theory. This art is centralised around applied peace and health studies and not that of defence or material application. The unification of mind, body and environment is emphasised using the anatomy and physiological theory of yin, yang and five-element Traditional Chinese medicine. Its movements, exercises and teachings cultivate, direct and harmonise the QI.\nTea ceremony.\nThere are spring, summer, fall, and winter teas. The perennial tea ceremony includes four tea settings () and a tea master (). Each tea setting is arranged and stands for the four directions (North, South, East, and West). A vase of the seasons' flowers is put on the tea table. The tea settings are:\nGogyo Japanese.\nThe theory of Wuxing in Japanese culture is known as Gogyo-\u4e94\u884c. In the 5th and 6th centuries, the principles of yin-yang and the Five Phases were transmitted to Japan from China, along with Taoism, Chinese Buddhism and Confucianism by Monks and medical physicians. Today the Theory of Gogyo is extensively used in the practice of Japanese Acupunture and traditional Kampo medicine."}
{"id": "6461", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=6461", "title": "Chinese element", "text": ""}
{"id": "6462", "revid": "5846", "url": "https://en.wikipedia.org/wiki?curid=6462", "title": "Church of Christ, Scientist", "text": "The Church of Christ, Scientist was founded in 1879 in Boston, Massachusetts, by Mary Baker Eddy, author of \"Science and Health with Key to the Scriptures,\" and founder of Christian Science. The church was founded \"to commemorate the word and works of [Christ Jesus]\" and \"reinstate primitive Christianity and its lost element of healing\". Sunday services are held throughout the year and weekly testimony meetings are held on Wednesday evenings, where following brief readings from the Bible and the Christian Science textbook, those in attendance are invited to give testimonies of healing brought about through Christian Science prayer.\nIn the early decades of the 20th century, Christian Science churches sprang up in communities around the world, though in the last several decades of that century, there was a marked decline in membership, except in Africa, where there has been growth. Headquartered in Boston, the church does not officially report membership, and estimates as to worldwide membership range between about 400,000 to less than 100,000.\nHistory.\nThe church was incorporated by Mary Baker Eddy in 1879 following a claimed personal healing in 1866, which she said resulted from reading the Bible. The Bible and Eddy's textbook on Christian healing, \"Science and Health with Key to the Scriptures\", are together the church's key doctrinal sources and have been ordained as the church's \"dual impersonal pastor\".\nThe First Church of Christ, Scientist, is widely known for its publications, especially \"The Christian Science Monitor\", a weekly newspaper published internationally in print and online. The seal of Christian Science is a cross and crown with the words, \"Heal the sick, raise the dead, cleanse the lepers, cast out demons,\" and is a registered trademark of the church.\nBeliefs and practices.\nChristian Scientists believe that prayer is effective. The Church has collected over 50,000 testimonies of incidents that it considers healing through Christian Science treatment alone. While most of these testimonies represent ailments neither diagnosed nor treated by medical professionals, the Church requires three other people to vouch for any testimony published in any of its official organ, including the \"Christian Science Journal\", \"Christian Science Sentinel\", and \"Herald of Christian Science\"; verifiers say that they witnessed the healing or know the testifier well enough to vouch for them.\nChristian Scientists may take an intensive two-week \"Primary\" class from an authorized Christian Science teacher. Those who wish to become \"Journal-listed\" (accredited) practitioners, devoting themselves full-time to the practice of healing, must first have Primary class instruction. When they have what the church regards as a record of healing, they may submit their names for publication in the directory of practitioners and teachers in the \"Christian Science Journal.\" A practitioner who has been listed for at least three years' may apply for \"Normal\" class instruction, given once every three years. Those who receive a certificate are authorized to teach. Both Primary and Normal classes are based on the Bible and the writings of Mary Baker Eddy. The Primary class focuses on the chapter, \"Recapitulation\" in \"Science and Health with Key to the Scriptures\". This chapter uses the Socratic method of teaching and contains the \"Scientific Statement of Being\". The \"Normal\" class focuses on the platform of Christian Science, contained on pages 330-340 of \"Science and Health.\"\nOrganization.\nThe First Church of Christ, Scientist is the legal title of the Mother Church and administrative headquarters of the Christian Science Church. The complex is located in a plaza alongside Huntington Avenue in the Back Bay neighborhood of Boston, Massachusetts.\nThe church itself was built in 1894, and an annex larger in footprint than the original structure was added in 1906. It boasts one of the world's largest pipe organs, built by the Aeolian-Skinner Company of Boston. The Mary Baker Eddy Library for the Betterment of Humanity is housed in an 11-story structure originally built for The Christian Science Publishing Society constructed between 1932 and 1934, and the present plaza was constructed in the late 1960s and early 1970s to include a 28 story administration building, a colonnade, and a reflecting pool with fountain, designed by Araldo Cossutta of I. M. Pei and Partners (now Pei Cobb Freed).\nBranch churches of The Mother Church may take the title of \"First Church of Christ, Scientist\"; Second; but the article \"The\" must not be used, presumably to concede the primacy of the Boston Mother Church.\nAn international newspaper, \"The Christian Science Monitor\", founded by Eddy in 1908 and winner of seven Pulitzer prizes, is published by the church through the Christian Science Publishing Society.\nBranch Christian Science churches and Christian Science societies are subordinate to the Mother Church, but are self-governed. They have their own by-laws, bank accounts, assets and officers, but in order to be recognised must abide by the by-laws in the \"Manual of The Mother Church\". Church services are regulated by the \"Manual,\" the set of by-laws written by Eddy, that establishes the church organization and explains the duties and responsibilities of members, officers, practitioners, teachers and nurses; and establishes rules for discipline and other aspects of church business.\nBoard of Directors.\nThe Christian Science Board of Directors is a five-person executive entity created by Mary Baker Eddy to conduct the business of the Christian Science Church under the terms defined in the by-laws of the \"Church Manual\". Its functions and restrictions are defined by the \"Manual.\"\nThe Board (occasionally CSBD or the BoD for short) also includes functions defined by a Deed of Trust written by Eddy (one of several, in fact) under which it consisted of four persons, though she later expanded the Board to five persons, thus in effect leaving one of its members out of Deed functions. This later bore on a dispute during the 1920s, known as the Great Litigation in CS circles, pivoting on whether the CSBD could remove trustees of the Christian Science Publishing Society or whether the CSPS trustees were established independently.\nWhile Eddy's Manual established limited executive functions under the rule of law in place of a traditional hierarchy, the controversial 1991 publication of a book by Bliss Knapp led the then Board of Directors to make the unusual affidavit during a suit over Knapp's estate that neither acts by it violating the \"Manual,\" nor acts refraining from required action, constituted violations of the \"Manual\". A traditionally-minded minority held that the Board's act in publishing Knapp's book constituted a fundamental violation of several by-laws and its legal trust, automatically mandating the offending Board members' resignations under Article I, Section 9.\nAnother minority believed that Eddy intended various requirements for her consent (in their view, \"estoppels\") to effect the church's dissolution on her death, since they could no longer be followed literally. Ironically, one of the stronger arguments against this position came from an individual highly respected by their theological quarter, Bliss Knapp, who claimed that Eddy understood through her lawyer that these consent clauses would not hinder normal operation after her decease.\nServices.\nChurches worldwide hold a one-hour service each Sunday, consisting of hymns, prayer, and currently, readings from the \"King James Version\" (KJV) of the Bible (although there is no requirement that this version of the Bible be used) and \"Science and Health with Key to the Scriptures\". These readings are the weekly Lesson-Sermon, which is read aloud at all Sunday services in all Christian Science churches worldwide, and is studied by individuals at home throughout the preceding week. The Lesson, as it is informally called, is compiled by a committee at The Mother Church, and is usually made up of six sections, each of which consists of passages from the Bible (read by the Second Reader) and passages from \"Science and Health\" (read by the First Reader).\nEddy selected 26 subjects for the Lesson-Sermon. These Lessons run in continuous rotation in the order she established, hence each subject is studied twice a year. In years in which there are 53 Sundays, the topic \"Christ Jesus\" occurs a third time, in December. In addition, there is a special, shortened Lesson-Sermon for Thanksgiving Day. Branch churches outside the United States may schedule their Thanksgiving service when convenient for them, most choosing a day in October or November, and the Thanksgiving Day proclamation by the United States president, may be omitted.\nBecause there are no clergy in the church, branch church Sunday services are conducted by two Readers: the First Reader, who reads passages from Science and Health, and the Second Reader, who reads passages from the Bible. First Readers determine the beginning \"scriptural selection\", hymns to be sung on Sundays, and the benediction. The vast majority of the service is the reading of the weekly Bible lesson supplied by Boston, and the order of the service set out by the Manual. To be elected the First Reader in one's branch church is one of the highest and most important positions the lay Christian Scientist may aspire to.\nChurches also hold a one-hour Wednesday evening testimony meeting, with similar readings, after which, those in attendance are invited to share accounts of healing through prayer. At these services, the First Reader reads passages from the Bible and Science and Health. Departing from denominational practice for over 120 years, English language churches may now choose alternate Bible translations at these services (i.e. Phillips).\nBranch churches also sponsor annual public talks (called lectures) given by speakers selected annually by the Board of Lectureship in Boston.\nRecent problems.\nBroadcasting.\nBeginning in the mid-1980s, church executives undertook a controversial and ambitious foray into electronic broadcast media. The first significant effort was to create a weekly half-hour syndicated television program, \"The Christian Science Monitor\" Reports. \"Monitor Reports\" was anchored in its first season by newspaper veteran Rob Nelson. He was replaced in the second by the \"Christian Science Monitor\"'s former Moscow correspondent, David Willis. The program was usually broadcast by independent stations\u00a0\u2014 often at odd hours.\nIn 1988, Monitor Reports was supplanted by a nightly half-hour news show, World Monitor, which was broadcast by the Discovery Channel. The program was anchored by veteran journalist John Hart. The Church then purchased a Boston cable television station for elaborate in-house programming production. In parallel, the church purchased a shortwave radio station and syndicated radio production to National Public Radio. However, revenues fell far short of optimistic predictions by church managers, who had ignored early warnings by members and media experts.\nIn October 1991, after a series of conflicts over the boundaries between Christian Science teachings and his journalistic independence, John Hart resigned. The Monitor Channel went off the air in June 1992. Most of the other operations closed in well under a decade. Public accounts in both the mainstream and trade media reported that the church lost approximately $250 million on these ventures.\nThe hundreds of millions lost on broadcasting brought the church to the brink of bankruptcy. However, with the 1991 publication of \"The Destiny of The Mother Church\" by the late Bliss Knapp, the church secured a $90 million bequest from the Knapp trust. The trust dictated that the book be published as \"Authorized Literature,\" with neither modification nor comment. Historically, the church had censured Knapp for deviating at several points from Eddy's teaching, and had refused to publish the work. The church's archivist, fired in anticipation of the book's publication, wrote to branch churches to inform them of the book's history. Many Christian Scientists thought the book violated the church's by-laws, and the editors of the church's religious periodicals and several other church employees resigned in protest. Alternate beneficiaries subsequently sued to contest the church's claim it had complied fully with the will's terms, and the church ultimately received only half of the original sum.\nThe fallout of the broadcasting debacle also sparked a minor revolt among some prominent church members. In late 1993, a group of Christian Scientists filed suit against the Board of Directors, alleging a willful disregard for the Manual of the Mother Church in its financial dealings. The suit was thrown out by the Supreme Judicial Court of Massachusetts in 1997, but a lingering discontent with the church's financial matters persists to this day.\nMembership decline and financial setbacks.\nIn spite of its early meteoric rise, church membership has declined over the past eight decades, according to the church's former treasurer, J. Edward Odegaard. Though the Church is prohibited by the Manual from publishing membership figures, the number of branch churches in the United States has fallen steadily since World War II. In 2009, for the first time in church history, more new members came from Africa than the United States.\nIn 2005, \"The Boston Globe\" reported that the church was considering consolidating Boston operations into fewer buildings and leasing out space in buildings it owned. Church official Philip G. Davis noted that the administration and Colonnade buildings had not been fully used for many years and that vacancy increased after staff reductions in 2004. The church posted an $8 million financial loss in fiscal 2003, and in 2004 cut 125 jobs, a quarter of the staff, at the \"Christian Science Monitor\". Conversely, Davis noted that \"the financial situation right now is excellent\" and stated that the church was not facing financial problems."}
{"id": "6464", "revid": "8390765", "url": "https://en.wikipedia.org/wiki?curid=6464", "title": "Cell phone", "text": ""}
{"id": "6466", "revid": "41521721", "url": "https://en.wikipedia.org/wiki?curid=6466", "title": "Connecticut", "text": "Connecticut () is the southernmost state in the New England region of the United States. As of the 2010 Census, it has the highest per-capita income, second-highest level of human development behind Massachusetts, and highest median household income in the United States. It is bordered by Rhode Island to the east, Massachusetts to the north, New York to the west, and the Long Island Sound to the south. Its capital is Hartford and its most populous city is Bridgeport. Historically the state is part of New England as well as the tri-state area with New York and New Jersey, which together make up metropolitan New York City. The state is named for the Connecticut River which approximately bisects the state. The word \"Connecticut\" is derived from various anglicized spellings of \u201cQuononoquett (Conanicut),\u201d a Mohegan-Pequot word for \"long tidal river\".\nConnecticut's first European settlers were Dutchmen who established a small, short-lived settlement called Fort Hoop in Hartford at the confluence of the Park and Connecticut Rivers. Half of Connecticut was initially claimed by the Dutch colony New Netherland, which included much of the land between the Connecticut and Delaware Rivers, although the first major settlements were established in the 1630s by the English. Thomas Hooker led a band of followers from the Massachusetts Bay Colony and founded the Connecticut Colony; other settlers from Massachusetts founded the Saybrook Colony and the New Haven Colony. The Connecticut and New Haven colonies established documents of Fundamental Orders, considered the first constitutions in America. In 1662, the three colonies were merged under a royal charter, making Connecticut a crown colony. This was one of the Thirteen Colonies which rejected British rule in the American Revolution.\nConnecticut is the third smallest state by area, the 29th most populous, and the fourth most densely populated of the fifty states. It is known as the \"Constitution State\", the \"Nutmeg State\", the \"Provisions State\", and the \"Land of Steady Habits\". It was influential in the development of the federal government of the United States (see Connecticut Compromise). The Connecticut River, Thames River, and ports along Long Island Sound have given Connecticut a strong maritime tradition which continues today. The state also has a long history of hosting the financial services industry, including insurance companies in Hartford and hedge funds in Fairfield County.\nGeography.\nConnecticut is bordered on the south by Long Island Sound, on the west by New York, on the north by Massachusetts, and on the east by Rhode Island. The state capital and fourth largest city is Hartford, and other major cities and towns (by population) include Bridgeport, New Haven, Stamford, Waterbury, Norwalk, Danbury, New Britain, Greenwich, and Bristol. Connecticut is slightly larger than the country of Montenegro. There are 169 incorporated towns in Connecticut.\nThe highest peak in Connecticut is Bear Mountain in Salisbury in the northwest corner of the state. The highest point is just east of where Connecticut, Massachusetts, and New York meet (42\u00b03\u2032 N, 73\u00b029\u2032 W), on the southern slope of Mount Frissell, whose peak lies nearby in Massachusetts. At the opposite extreme, many of the coastal towns have areas that are less than 20 feet (6 m) above sea level.\nConnecticut has a long maritime history and a reputation based on that history\u2014yet the state has no direct oceanfront (technically speaking). The coast of Connecticut sits on Long Island Sound, which is an estuary. The state's access to the open Atlantic Ocean is both to the west (toward New York City) and to the east (toward the \"race\" near Rhode Island). This situation provides many safe harbors from ocean storms, and many transatlantic ships seek anchor inside Long Island Sound when tropical cyclones pass off the upper East Coast.\nThe Connecticut River cuts through the center of the state, flowing into Long Island Sound. The most populous metropolitan region centered within the state lies in the Connecticut River Valley. Despite Connecticut's relatively small size, it features wide regional variations in its landscape; for example, in the northwestern Litchfield Hills, it features rolling mountains and horse farms, whereas in areas to the east of New Haven along the coast, the landscape features coastal marshes, beaches, and large scale maritime activities.\nConnecticut's rural areas and small towns in the northeast and northwest corners of the state contrast sharply with its industrial cities such as Stamford, Bridgeport, and New Haven, located along the coastal highways from the New York border to New London, then northward up the Connecticut River to Hartford. Many towns in northeastern and northwestern Connecticut center around a green. Near the green typically stand historical visual symbols of New England towns, such as a white church, a colonial meeting house, a colonial tavern or inn, several colonial houses, and so on, establishing a scenic historical appearance maintained for both historic preservation and tourism. Many of the areas in southern and coastal Connecticut have been built up and rebuilt over the years, and look less visually like traditional New England.\nThe northern boundary of the state with Massachusetts is marked by the Southwick Jog or Granby Notch, an approximately square detour into Connecticut. The origin of this anomaly is clearly established in a long line of disputes and temporary agreements which were finally concluded in 1804, when southern Southwick's residents sought to leave Massachusetts, and the town was split in half.\nThe southwestern border of Connecticut where it abuts New York State is marked by a panhandle in Fairfield County, containing the towns of Greenwich, Stamford, New Canaan, Darien, and parts of Norwalk and Wilton. This irregularity in the boundary is the result of territorial disputes in the late 17th century, culminating with New York giving up its claim to the area, whose residents considered themselves part of Connecticut, in exchange for an equivalent area extending northwards from Ridgefield to the Massachusetts border, as well as undisputed claim to Rye, New York.\nAreas maintained by the National Park Service include Appalachian National Scenic Trail, Quinebaug and Shetucket Rivers Valley National Heritage Corridor, and Weir Farm National Historic Site.\nClimate.\nConnecticut lies at the rough transition zone between the southern end of the humid continental climate, and the northern portion of the humid subtropical climate. Northern Connecticut generally experiences a climate with cold winters with moderate snowfall and hot, humid summers. Far southern and coastal Connecticut has a climate with cool winters with a mix of rain and infrequent snow, and the long hot and humid summers typical of the middle and lower East Coast.\nConnecticut sees a fairly even precipitation pattern with rainfall/snowfall spread throughout the 12 months. Connecticut averages 56% of possible sunshine (higher than the U.S. national average), averaging 2,400 hours of sunshine annually.\nEarly spring (April) can range from slightly cool (40s to low 50s F) to warm (65 to 70 F), while mid and late spring (late April/May) is warm. By late May, the building Bermuda High creates a southerly flow of warm and humid tropical air, bringing hot weather conditions throughout the state, with average highs in New London of and in Windsor Locks at the peak of summer in late July. On occasion, heat waves with highs from 90 to occur across Connecticut. Although summers are sunny in Connecticut, quick moving summer thunderstorms can bring brief downpours with thunder and lightning. Occasionally these thunderstorms can be severe, and the state usually averages one tornado per year. During hurricane season, the remains of tropical cyclones occasionally affect the region, though a direct hit is rare.\nWeather commonly associated with the fall season typically begins in October and lasts to the first days of December. Daily high temperatures in October and November range from the 50s to 60s (Fahrenheit) with nights in the 40s and upper 30s. Colorful foliage begins across northern parts of the state in early October and moves south and east reaching southeast Connecticut by early November. Far southern and coastal areas, however, have more oak and hickory trees (and fewer maples) and are often less colorful than areas to the north. By December daytime highs are in the 40s \u00b0F for much of the state, and average overnight lows are below freezing.\nWinters (December through mid-March) are generally cold from south to north in Connecticut. The coldest month (January) has average high temperatures ranging from in the coastal lowlands to in the inland and northern portions on the state. The average yearly snowfall ranges from about in the higher elevations of the northern portion of the state to only along the southeast coast of Connecticut (Branford to Groton). Generally, any locale north or west of Interstate 84 receives the most snow, during a storm, and throughout the season. Most of Connecticut has less than 60 days of snow cover. Snow usually falls from late November to late March in the northern part of the state, and from early December to mid-March in the southern and coastal parts of the state.\nConnecticut's record high temperature is which occurred in Danbury on July 15, 1995; the record low is which occurred in the Northwest Hills Falls Village on February 16, 1943, and Coventry on January 22, 1961.\nFlora.\nForests consist of a mix of Northeastern coastal forests of Oak in southern areas of the state, to the upland New England-Acadian forests in the northwestern parts of the state. Mountain Laurel (Kalmia latifolia) is the state flower and is native to low ridges in several parts of Connecticut. Rosebay Rhododendron (Rhododendron maximum) is also native to eastern uplands of Connecticut and Pachaug State Forest is home to the Rhododendron Sanctuary Trail. Atlantic white cedar (Chamaecyparis thyoides), is found in wetlands in the southern parts of the state. Connecticut has one native cactus (Opuntia humifusa), found in sandy coastal areas and low hillsides. Several types of beach grasses and wildflowers are also native to Connecticut. Connecticut spans USDA Plant Hardiness Zones 5b to 7a. Coastal Connecticut is the broad transition zone where more southern and subtropical plants are cultivated. In some coastal communities, Magnolia grandiflora (southern magnolia), Crape Myrtles, scrub palms (Sabal minor), Needle Palms (Rhapidophyllum hystrix), and other broadleaved evergreens are cultivated in small numbers.\nHistory.\nFirst people.\nThe name Connecticut is derived from the Mohegan-Pequot word that has been translated as \"long tidal river\" and \"upon the long river\", referring to the Connecticut River. Evidence of human presence in the Connecticut region dates to as much as 10,000 years ago. Stone tools were used for hunting, fishing, and woodworking. Semi-nomadic in lifestyle, these peoples moved seasonally to take advantage of various resources in the area. They shared languages based on Algonquian. The Connecticut region was inhabited by multiple Native American tribes which can be grouped into the Nipmuc, the Sequin or \"River Indians\" (which included the Tunxis, Schaghticoke, Podunk, Wangunk, Hammonasset, and Quinnipiac), the Mattabesec or \"Wappinger Confederacy\" and the Pequot-Mohegan. Some of these groups continue to abide in Connecticut, including the Mohegans, the Pequots, and the Paugusetts.\nColonial period.\nThe first European explorer in Connecticut was Dutchman Adriaen Block, who explored the region in 1614. Dutch fur traders then sailed up the Connecticut River, which they called Versche Rivier (\"Fresh River\"), and built a fort at Dutch Point in Hartford that they named \"House of Hope\" ().\nThe Connecticut Colony was originally a number of separate, smaller settlements at Windsor, Wethersfield, Saybrook, Hartford, and New Haven. The first English settlers came in 1633 and settled at Windsor, and then at Wethersfield the following year. John Winthrop the Younger of Massachusetts received a commission to create Saybrook Colony at the mouth of the Connecticut River in 1635.\nThe main body of settlers came in one large group in 1636. They were Puritans from Massachusetts Bay Colony led by Thomas Hooker, who established the Connecticut Colony at Hartford. The Quinnipiack Colony was established by John Davenport, Theophilus Eaton, and others at New Haven in March 1638. The New Haven Colony had its own constitution called \"The Fundamental Agreement of the New Haven Colony\", signed on June 4, 1639.\nThe settlements were established without official sanction of the English Crown, and each was an independent political entity. In 1662, Winthrop traveled to England and obtained a charter from CharlesII which united the settlements of Connecticut. Historically important colonial settlements included Windsor (1633), Wethersfield (1634), Saybrook (1635), Hartford (1636), New Haven (1638), Fairfield (1639), Guilford (1639), Milford (1639), Stratford (1639), Farmington (1640), Stamford (1641), and New London (1646).\nThe Pequot War marked the first major clash between colonists and Native Americans in New England. The Pequots reacted with increasing aggression to Colonial settlements in their territory\u2014while simultaneously taking lands from the Narragansett and Mohegan tribes. Settlers responded to a murder in 1636 with a raid on a Pequot village on Block Island; the Pequots laid siege to Saybrook Colony's garrison that autumn, then raided Wethersfield in the spring of 1637. Colonists declared war on the Pequots, organized a band of militia and allies from the Mohegan and Narragansett tribes, and attacked a Pequot village on the Mystic River, with death toll estimates ranging between 300 and 700 Pequots. After suffering another major loss at a battle in Fairfield, the Pequots asked for a truce and peace terms.\nThe western boundaries of Connecticut have been subject to change over time. The Hartford Treaty with the Dutch was signed on September 19, 1650, but it was never ratified by the British. According to it, the western boundary of Connecticut ran north from Greenwich Bay for a distance of , \"provided the said line come not within 10 miles of Hudson River\". This agreement was observed by both sides until war erupted between England and The Netherlands in 1652. Conflict continued concerning colonial limits until the Duke of York captured New Netherland in 1664.\nOn the other hand, Connecticut's original Charter in 1662 granted it all the land to the \"South Sea\"\u2014that is, to the Pacific Ocean. Most Colonial royal grants were for long east\u2013west strips. Connecticut took its grant seriously and established a ninth county between the Susquehanna River and Delaware River named Westmoreland County. This resulted in the brief Pennamite Wars with Pennsylvania.\nYale College was established in 1701, providing Connecticut with an important institution to educate clergy and civil leaders. The Congregational church dominated religious life in the colony and, by extension, town affairs in many parts.\nWith more than 600 miles of coastline including along its navigable rivers, during the colonial years Connecticut developed the antecedents of a maritime tradition that would later produce booms in shipbuilding, marine transport, naval support, seafood production, and leisure boating.\nHistorical records list the Tryall as the first vessel built in Connecticut Colony, in 1649 at a site on the Connecticut River in present-day Wethersfield. In the two decades leading up to 1776 and the American Revolution, Connecticut boatyards launched about 100 sloops, schooners and brigs according to a database of U.S. customs records maintained online by the Mystic Seaport Museum, the largest being the 180-ton Patient Mary launched in New Haven in 1763. Connecticut's first lighthouse was constructed in 1760 at the mouth of the Thames River with the New London Harbor Lighthouse.\nAmerican Revolution.\nConnecticut designated four delegates to the Second Continental Congress who signed the Declaration of Independence: Samuel Huntington, Roger Sherman, William Williams, and Oliver Wolcott. Connecticut's legislature authorized the outfitting of six new regiments in 1775, in the wake of the clashes between British regulars and Massachusetts militia at Lexington and Concord. There were some 1,200 Connecticut troops on hand at the Battle of Bunker Hill in June 1775. In 1775, David Bushnell invented the Turtle which the following year launched the first submarine attack in history, unsuccessfully against a British warship at anchor in New York Harbor.\nIn 1777, the British got word of Continental Army supplies in Danbury, and they landed an expeditionary force of some 2,000 troops in Westport. This force then marched to Danbury and destroyed homes and much of the depot. Continental Army troops and militia led by General David Wooster and General Benedict Arnold engaged them on their return march at Ridgefield in 1777. For the winter of 1778\u201379, General George Washington decided to split the Continental Army into three divisions encircling New York City, where British General Sir Henry Clinton had taken up winter quarters. Major General Israel Putnam chose Redding as the winter encampment quarters for some 3,000 regulars and militia under his command. The Redding encampment allowed Putnam's soldiers to guard the replenished supply depot in Danbury and to support any operations along Long Island Sound and the Hudson River Valley. Some of the men were veterans of the winter encampment at Valley Forge, Pennsylvania, the previous winter. Soldiers at the Redding camp endured supply shortages, cold temperatures, and significant snow, with some historians dubbing the encampment \"Connecticut's Valley Forge\".\nThe state was also the launching site for a number of raids against Long Island orchestrated by Samuel Holden Parsons and Benjamin Tallmadge, and provided men and material for the war effort, especially to Washington's army outside New York City. General William Tryon raided the Connecticut coast in July 1779, focusing on New Haven, Norwalk, and Fairfield. New London and Groton Heights were raided in September 1781 by Benedict Arnold, who had turned traitor to the British.\nAt the outset of the American Revolution, the Continental Congress assigned Nathaniel Shaw Jr. of New London as its naval agent in charge of recruiting privateers to seize British vessels as opportunities presented, with nearly 50 operating out of the Thames River which eventually drew the reprisal from the British force led by Arnold.\n19th century.\nEarly national period and industrial revolution.\nConnecticut ratified the U.S. Constitution on January 9, 1788, becoming the fifth state.\nThe state prospered during the era following the American Revolution, as mills and textile factories were built and seaports flourished from trade and fisheries. After Congress established in 1790 the predecessor to the U.S. Revenue Cutter Service that would evolve into the U.S. Coast Guard, President Washington assigned Jonathan Maltbie as one of seven masters to enforce customs regulations, with Maltbie monitoring the southern New England coast with a 48-foot cutter sloop named Argus.\nIn 1786, Connecticut ceded territory to the U.S. government that became part of the Northwest Territory. The state retained land extending across the northern part of present-day Ohio called the Connecticut Western Reserve. The Western Reserve section was settled largely by people from Connecticut, and they brought Connecticut place names to Ohio.\nConnecticut made agreements with Pennsylvania and New York which extinguished the land claims within those states' boundaries and created the Connecticut Panhandle. The state then ceded the Western Reserve in 1800 to the federal government, which brought it to its present boundaries (other than minor adjustments with Massachusetts).\nFor the first time in 1800, Connecticut shipwrights launched more than 100 vessels in a single year. Over the following decade to the doorstep of renewed hostilities with Britain that sparked the War of 1812, Connecticut boatyards constructed close to 1,000 vessels, the most productive stretch of any decade in the 19th century.\nDuring the war, the British launched raids in Stonington and Essex and blockaded vessels in the Thames River. Derby native Isaac Hull became Connecticut's best-known naval figure to win renown during the conflict, as captain of the .\nThe British blockade during the War of 1812 hurt exports and bolstered the influence of Federalists who opposed the war. The cessation of imports from Britain stimulated the construction of factories to manufacture textiles and machinery. Connecticut came to be recognized as a major center for manufacturing, due in part to the inventions of Eli Whitney and other early innovators of the Industrial Revolution.\nThe war led to the development of fast clippers that helped extend the reach of New England merchants to the Pacific and Indian oceans. The first half of the 19th century saw as well a rapid rise in whaling, with New London emerging as one of the New England industry's three biggest home ports after Nantucket and New Bedford.\nThe state was known for its political conservatism, typified by its Federalist party and the Yale College of Timothy Dwight. The foremost intellectuals were Dwight and Noah Webster, who compiled his great dictionary in New Haven. Religious tensions polarized the state, as the Congregational Church struggled to maintain traditional viewpoints, in alliance with the Federalists. The failure of the Hartford Convention in 1814 hurt the Federalist cause, with the Democratic-Republican Party gaining control in 1817.\nConnecticut had been governed under the \"Fundamental Orders\" since 1639, but the state adopted a new constitution in 1818.\nCivil War era.\nConnecticut manufacturers played a major role in supplying the Union forces with weapons and supplies during the Civil War. The state furnished 55,000 men, formed into thirty full regiments of infantry, including two in the U.S. Colored Troops, with several Connecticut men becoming generals. The Navy attracted 250 officers and 2,100 men, and Glastonbury native Gideon Welles was Secretary of the Navy. James H. Ward of Hartford was the first U.S. Naval Officer killed in the Civil War. Connecticut casualties included 2,088 killed in combat, 2,801 dying from disease, and 689 dying in Confederate prison camps.\nA surge of national unity in 1861 brought thousands flocking to the colors from every town and city. However, as the war became a crusade to end slavery, many Democrats (especially Irish Catholics) pulled back. The Democrats took a pro-slavery position and included many Copperheads willing to let the South secede. The intensely fought 1863 election for governor was narrowly won by the Republicans.\nSecond industrial revolution.\nConnecticut's extensive industry, dense population, flat terrain, and wealth encouraged the construction of railroads starting in 1839. By 1840, of line were in operation, growing to in 1850 and in 1860.\nThe New York, New Haven and Hartford Railroad, called the \"New Haven\" or \"The Consolidated\", became the dominant Connecticut railroad company after 1872. J. P. Morgan began financing the major New England railroads in the 1890s, dividing territory so that they would not compete. The New Haven purchased 50 smaller companies, including steamship lines, and built a network of light rails (electrified trolleys) that provided inter-urban transportation for all of southern New England. By 1912, the New Haven operated over of track with 120,000 employees.\nAs steam-powered passenger ships proliferated after the Civil War, Noank would produce the two largest built in Connecticut during the 19th century, with the 332-foot wooden steam paddle wheeler Rhode Island launched in 1882, and the 345-foot paddle wheeler Connecticut seven years later. Connecticut shipyards would launch more than 165 steam-powered vessels in the 19th century.\nIn 1875, the first telephone exchange in the world was established in New Haven.\n20th century.\nWorld War I.\nWhen World War I broke out in 1914, Connecticut became a major supplier of weaponry to the U.S. military; by 1918, 80% of the state's industries were producing goods for the war effort. Remington Arms in Bridgeport produced half the small-arms cartridges used by the U.S. Army, with other major suppliers including Winchester in New Haven and Colt in Hartford.\nConnecticut was also an important U.S. Navy supplier, with Electric Boat receiving orders for 85 submarines, Lake Torpedo Boat building more than 20 subs, and the Groton Iron Works building freighters. On June 21, 1916, the Navy made Groton the site for its East Coast submarine base and school.\nThe state enthusiastically supported the American war effort in 1917 and 1918 with large purchases of war bonds, a further expansion of industry, and an emphasis on increasing food production on the farms. Thousands of state, local, and volunteer groups mobilized for the war effort and were coordinated by the Connecticut State Council of Defense. Manufacturers wrestled with manpower shortages; Waterbury's American Brass and Manufacturing Company was running at half capacity, so the federal government agreed to furlough soldiers to work there.\nInterwar period.\nIn 1919, J. Henry Roraback started the Connecticut Light &amp; Power Co. which became the state's dominant electric utility. In 1925, Frederick Rentschler spurred the creation of Pratt &amp; Whitney in Hartford to develop engines for aircraft; the company became an important military supplier in World WarII and one of the three major manufacturers of jet engines in the world.\nOn September 21, 1938, the most destructive storm in New England history struck eastern Connecticut, killing hundreds of people. The eye of the \"Long Island Express\" passed just west of New Haven and devastated the Connecticut shoreline between Old Saybrook and Stonington from the full force of wind and waves, even though they had partial protection by Long Island. The hurricane caused extensive damage to infrastructure, homes, and businesses. In New London, a 500-foot (150 m) sailing ship was driven into a warehouse complex, causing a major fire. Heavy rainfall caused the Connecticut River to flood downtown Hartford and East Hartford. An estimated 50,000 trees fell onto roadways.\nWorld War II.\nThe advent of lend-lease in support of Britain helped lift Connecticut from the Great Depression, with the state a major production center for weaponry and supplies used in World WarII. Connecticut manufactured 4.1% of total U.S. military armaments produced during the war, ranking ninth among the 48 states, with major factories including Colt for firearms, Pratt &amp; Whitney for aircraft engines, Chance Vought for fighter planes, Hamilton Standard for propellers, and Electric Boat for submarines and PT boats. In Bridgeport, General Electric produced a significant new weapon to combat tanks: the bazooka.\nOn May 13, 1940, Igor Sikorsky made an untethered flight of the first practical helicopter. The helicopter saw limited use in World War II, but future military production made Sikorsky Aircraft's Stratford plant Connecticut's largest single manufacturing site by the start of the 21st century.\nPost-World War II economic expansion.\nConnecticut lost some wartime factories following the end of hostilities, but the state shared in a general post-war expansion that included the construction of highways and resulting in middle-class growth in suburban areas.\nPrescott Bush represented Connecticut in the U.S. Senate from 1952 to 1963; his son George H.W. Bush and grandson George W. Bush both became presidents of the United States. In 1965, Connecticut ratified its current constitution, replacing the document that had served since 1818.\nIn 1968, commercial operation began for the Connecticut Yankee Nuclear Power Plant in East Haddam; in 1970, the Millstone Nuclear Power Station began operations in Waterford. In 1974, Connecticut elected Democratic Governor Ella T. Grasso, who became the first woman in any state to be elected governor without being the wife or widow of a previous governor.\nLate 20th century.\nConnecticut's dependence on the defense industry posed an economic challenge at the end of the Cold War. The resulting budget crisis helped elect Lowell Weicker as governor on a third-party ticket in 1990. Weicker's remedy was a state income tax which proved effective in balancing the budget, but only for the short-term. He did not run for a second term, in part because of this politically unpopular move.\nIn 1992, initial construction was completed on Foxwoods Casino at the Mashantucket Pequots reservation in eastern Connecticut, which became the largest casino in the Western Hemisphere. Mohegan Sun followed four years later.\nEarly 21st century.\nIn 2000, presidential candidate Al Gore chose Senator Joe Lieberman as his running mate, marking the first time that a major party presidential ticket included someone of the Jewish faith. Gore and Lieberman fell five votes short of George W. Bush and Dick Cheney in the Electoral College.\nIn the terrorist attacks of September 11, 2001, 65 state residents were killed, mostly Fairfield County residents who were working in the World Trade Center.\nIn 2004, Republican Governor John G. Rowland resigned during a corruption investigation, later pleading guilty to federal charges.\nConnecticut was hit by three major storms in just over 14 months in 2011 and 2012, with all three causing extensive property damage and electric outages. Hurricane Irene struck Connecticut August 28, and damage totaled $235 million. Two months later, the \"Halloween nor'easter\" dropped extensive snow onto trees, resulting in snapped branches and trunks that damaged power lines; some areas were without electricity for 11 days. Hurricane Sandy had tropical storm-force winds when it reached Connecticut October 29, 2012. Sandy's winds drove storm surges into streets and cut power to 98% of homes and businesses, with more than $360 million in damage.\nOn December 14, 2012, Adam Lanza shot and killed 26 people at Sandy Hook Elementary School in Newtown, and then killed himself. The massacre spurred renewed efforts by activists for tighter laws on gun ownership nationally.\nIn the summer and fall of 2016, Connecticut experienced a drought in many parts of the state, causing some water-use bans. As of , 45% of the state was listed at Severe Drought by the U.S. Drought Monitor, including almost all of Hartford and Litchfield counties. All the rest of the state was in Moderate Drought or Severe Drought, including Middlesex, Fairfield, New London, New Haven, Windham, and Tolland counties. This affected the agricultural economy in the state.\nDemographics.\nThe United States Census Bureau estimates that the population of Connecticut was 3,565,287 on July 1, 2019, a 0.25% decrease since the 2010 United States Census.\n, Connecticut had an estimated population of 3,565,287, which is a decrease of 7,378 (0.25%) from the prior year and a decrease of 8,810 (0.25%) since 2010. This includes a natural increase since the last census of 67,427 (222,222 births minus 154,795 deaths) and an increase due to net migration of 41,718 people into the state. Immigration from outside the United States resulted in a net increase of 75,991 people, and migration within the country produced a net loss of 34,273. Based on the 2005 estimates, Connecticut moved from 29th most populous state to 30th. 2018 estimates put Connecticut's population at 3,572,665.\n6.6% of its population was reported as being under5 years old, 24.7% under 18 years old, and 13.8% were 65 years of age or older. Females made up approximately 51.6% of the population, with 48.4% male.\nIn 1790, 97% of the population in Connecticut was classified as \"rural\". The first census in which less than half the population was classified as rural was 1890. In the 2000 census, only 12.3% was considered rural. Most of western and southern Connecticut (particularly the Gold Coast) is strongly associated with New York City; this area is the most affluent and populous region of the state and has high property costs and high incomes. The center of population of Connecticut is located in the town of Cheshire.\nPopulation.\nAs of the 2010 United States Census, Connecticut's race and ethnic percentages were:\nHispanics and Latinos of any race made up 13.4% of the population in the 2010 Census.\nThe state's most populous ethnic group is Non-Hispanic White, but this has declined from 98% in 1940 to 71% in 2010.\nAs of 2004, 11.4% of the population (400,000) was foreign-born. In 1870, native-born Americans had accounted for 75% of the state's population, but that had dropped to 35% by 1918.\nAs of 2000, 81.69% of Connecticut residents age5 and older spoke English at home and 8.42% spoke Spanish, followed by Italian at 1.59%, French at 1.31%, and Polish at 1.20%.\nThe largest European ancestry groups are:\nBirth data.\n, 46.1% of Connecticut's population younger than age1 were minorities.\n\"Note: Births in table do not add up, because Hispanics are counted both by their ethnicity and by their race, giving a higher overall number.\"\nReligion.\nThe religious affiliations of the people of Connecticut :\nA Pew survey of Connecticut residents' religious self-identification showed the following distribution of affiliations: Protestant 35%, Mormonism 1%, Jewish 3%, Roman Catholic 33%, Orthodox 1%, Non-religious 28%, Jehovah's Witness 1%, Hinduism 1%, Buddhism 1% and Islam 1%. Jewish congregations had 108,280 (3.2%) members in 2000. The Jewish population is concentrated in the towns near Long Island Sound between Greenwich and New Haven, in Greater New Haven and in Greater Hartford, especially the suburb of West Hartford. According to the Association of Religion Data Archives, the largest Christian denominations, by number of adherents, in 2010 were: the Catholic Church, with 1,252,936; the United Church of Christ, with 96,506; and non-denominational Evangelical Protestants, with 72,863.\nRecent immigration has brought other non-Christian religions to the state, but the numbers of adherents of other religions are still low. Connecticut is also home to New England's largest Protestant Church: The First Cathedral in Bloomfield, Connecticut, located in Hartford County. Hartford is seat to the Roman Catholic Archdiocese of Hartford, which is sovereign over the Diocese of Bridgeport and the Diocese of Norwich.\nEconomy.\nConnecticut's economic output in 2019 as measured by gross domestic product was $289 billion, up from $277.9 billion in 2018.\nConnecticut's per capita personal income in 2019 was estimated at $79,087, the highest of any state. There is, however, a great disparity in incomes throughout the state; after New York, Connecticut had the second largest gap nationwide between the average incomes of the top 1% and the average incomes of the bottom 99%. According to a 2018 study by Phoenix Marketing International, Connecticut had the third-largest number of millionaires per capita in the United States, with a ratio of 7.75%. New Canaan is the wealthiest town in Connecticut, with a per capita income of $85,459. Hartford is the poorest municipality in Connecticut, with a per capita income of $13,428 in 2000.\nAs of December 2019, Connecticut's seasonally adjusted unemployment rate was 3.8%, with U.S. unemployment at 3.5% that month. Dating back to 1982, Connecticut recorded its lowest unemployment in 2000 between August and October, at 2.2%. The highest unemployment rate during that period occurred in November and December 2010 at 9.3%, but economists expect record new levels of layoffs as a result of business closures in the spring of 2020 as the result of the coronavirus pandemic.\nTaxation.\nTax is collected by the Connecticut Department of Revenue Services and by local municipalities.\nAs of 2012, Connecticut residents had the second highest rate in the nation of combined state and local taxes after New York, at 12.6% of income compared to the national average of 9.9% as reported by the Tax Foundation.\nBefore 1991, Connecticut had an investment-only income tax system. Income from employment was untaxed, but income from investments was taxed at 13%, the highest rate in the U.S., with no deductions allowed for costs of producing the investment income, such as interest on borrowing.\nIn 1991, under Governor Lowell P. Weicker Jr., an independent, the system was changed to one in which the taxes on employment income and investment income were equalized at a maximum rate of 4%. The new tax policy drew investment firms to Connecticut; , Fairfield County was home to the headquarters for 16 of the 200 largest hedge funds in the world.\n, the income tax rates on Connecticut individuals were divided into seven tax brackets of 3% (on income up to $10,000); 5% ($10,000-$50,000); 5.5% ($50,000-$100,000); 6% ($100,000-$200,000); 6.5% ($200,000-$250,000); 6.9% ($250,000-$500,000); and 6.99% above $500,000, with additional amounts owed depending on the bracket.\nAll wages of Connecticut residents are subject to the state's income tax, even if earned outside the state. However, in those cases, Connecticut income tax must be withheld only to the extent the Connecticut tax exceeds the amount withheld by the other jurisdiction. Since New York has higher income tax rates than Connecticut, this effectively means that Connecticut residents who work in New York have no Connecticut income tax withheld. Connecticut permits a credit for taxes paid to other jurisdictions, but since residents who work in other states are still subject to Connecticut income taxation, they may owe taxes if the jurisdictional credit does not fully offset the Connecticut tax amount.\nConnecticut levies a 6.35% state sales tax on the retail sale, lease, or rental of most goods. Some items and services in general are not subject to sales and use taxes unless specifically enumerated as taxable by statute. A provision excluding clothing under $50 from sales tax was repealed . There are no additional sales taxes imposed by local jurisdictions. In 2001, Connecticut instituted what became an annual sales tax \"holiday\" each August lasting one week, when retailers do not have to remit sales tax on certain items and quantities of clothing that has varied from year to year.\nState law authorizes municipalities to tax property, including real estate, vehicles and other personal property, with state statute providing varying exemptions, credits and abatements. All assessments are at 70% of fair market value. The maximum property tax credit is $200 per return and any excess may not be refunded or carried forward. According to the Tax Foundation, on a per capita basis in the 2017 fiscal year Connecticut residents paid the 3rd highest average property taxes in the nation after New Hampshire and New Jersey.\n, gasoline taxes and fees in Connecticut were 40.13 cents per gallon, 11th highest in the United States which had a nationwide average of 36.13 cents a gallon excluding federal taxes. Diesel taxes and fees as of January 2020 in Connecticut were 46.50 cents per gallon, ninth highest nationally with the U.S. average at 37.91 cents.\nReal estate.\nIn 2019, sales of single-family homes in Connecticut totaled 33,146 units, a 2.1 percent decline from the 2018 transaction total. The median home sold in 2019 recorded a transaction amount of $260,000, up 0.4 percent from 2018.\nConnecticut had the seventh highest rate of home foreclosure activity in the country in 2019 at 0.53 percent of the total housing stock.\nIndustries.\nFinance, insurance and real estate was Connecticut's largest industry in 2018 as ranked by gross domestic product, generating $75.7 billion in GDP that year. Major financial industry employers include The Hartford, Travelers, Cigna, the Aetna subsidiary of CVS Health, Mass Mutual, People's United Financial, Bank of America, Realogy, Bridgewater Associates, GE Capital, William Raveis Real Estate, and Berkshire Hathaway through reinsurance and residential real estate subsidiaries.\nThe combined educational, health and social services sector was the largest single industry as ranked by employment, with a combined workforce of 342,600 people at the end of 2019, ranking fourth the year before in GDP at $28.3 billion.\nThe broad business and professional services sector had the second highest GDP total in Connecticut in 2018 at an estimated $33.7 billion.\nManufacturing was the third biggest industry in 2018 with GDP of $30.8 billion, dominated by Raytheon Technologies formed in the March 2020 merger of Hartford-based United Technologies and Waltham, Mass.-based Raytheon Co. As of the merger, Raytheon Technologies employed about 19,000 people in Connecticut through subsidiaries Pratt &amp; Whitney and Collins Aerospace. Lockheed Martin subsidiary Sikorsky Aircraft operates Connecticut's single largest manufacturing plant in Stratford, where it makes helicopters.\nOther major manufacturers include the Electric Boat division of General Dynamics, which makes submarines in Groton, Boehringer Ingelheim, a pharmaceuticals manufacturer with its U.S. headquarters in Ridgefield, and ASML, which in Wilton makes precision lithography machines used to create circuitry on semiconductors and flat-screen displays.\nConnecticut historically was a center of gun manufacturing, and four gun-manufacturing firms continued to operate in the state , employing 2,000 people: Colt, Stag, Ruger, and Mossberg. Marlin, owned by Remington, closed in April 2011.\nOther large components of the Connecticut economy in 2018 included wholesale trade ($18.1 billion in GDP); information services ($13.8 billion); retail ($13.7 billion); arts, entertainment and food services ($9.1 billion); and construction ($8.3 billion).\nTourists spent $9.3 billion in Connecticut in 2017 according to estimates as part of a series of studies commissioned by the state of Connecticut. Foxwoods Resort Casino and Mohegan Sun are the two biggest tourist draws and number among the state's largest employers; both are located on Native American reservations in the eastern part of Connecticut.\nConnecticut's agricultural production totaled $580 million in 2017, with just over half of that revenue the result of nursery stock production. Milk production totaled $81 million that year, with other major product categories including eggs, vegetables and fruit, tobacco and shellfish.\nEnergy.\nConnecticut's economy uses less energy to produce each dollar of GDP than all other states except California, Massachusetts, and New York. It uses less energy on a per-capita basis than all but six other states. It has no fossil-fuel resources, but does have renewable resources. Average retail electricity prices are the highest among the 48 contiguous states. While the vast majority of state's overall energy consumption is fossil fuels, nuclear power delivered over 40% of state's electricity generation in 2019. Refuse-derived fuels and other biomass provided the largest share of renewable electricity at about a 3% share. Solar and wind generation has grown in recent years. More than three-quarters of solar generation came from distributed small-scale installations such as rooftop solar in 2019, and there is planning underway to significantly increase renewable generation with the state's offshore wind resource.\nTransportation.\nRoads.\nThe Interstate highways in the state are Interstate 95 (I-95) traveling southwest to northeast along the coast, I-84 traveling southwest to northeast in the center of the state, I-91 traveling north to south in the center of the state, and I-395 traveling north to south near the eastern border of the state. The other major highways in Connecticut are the Merritt Parkway and Wilbur Cross Parkway, which together form Connecticut Route 15 (Route 15), traveling from the Hutchinson River Parkway in New York parallel to I-95 before turning north of New Haven and traveling parallel to I-91, finally becoming a surface road in Berlin. I-95 and Route 15 were originally toll roads; they relied on a system of toll plazas at which all traffic stopped and paid fixed tolls. A series of major crashes at these plazas eventually contributed to the decision to remove the tolls in 1988. Other major arteries in the state include U.S. Route7 (US7) in the west traveling parallel to the New York state line, Route8 farther east near the industrial city of Waterbury and traveling north\u2013south along the Naugatuck River Valley nearly parallel with US7, and Route9 in the east.\nBetween New Haven and New York City, I-95 is one of the most congested highways in the United States. Although I-95 has been widened in several spots, some areas are only three lanes and this strains traffic capacity, resulting in frequent and lengthy rush hour delays. Frequently, the congestion spills over to clog the parallel Merritt Parkway and even US1. The state has encouraged traffic reduction schemes, including rail use and ride-sharing.\nConnecticut also has a very active bicycling community, with one of the highest rates of bicycle ownership and use in the United States, particularly in New Haven. According to the U.S. Census 2006 American Community Survey, New Haven has the highest percentage of commuters who bicycle to work of any major metropolitan center on the East Coast.\nRail.\nRail is a popular travel mode between New Haven and New York City's Grand Central Terminal. Southwestern Connecticut is served by the Metro-North Railroad's New Haven Line, operated by the Metropolitan Transportation Authority and providing commuter service to New York City and New Haven, with branches servicing New Canaan, Danbury, and Waterbury. Connecticut lies along Amtrak's Northeast Corridor which features frequent Northeast Regional and Acela Express service from New Haven south to New York City, Philadelphia, Baltimore, Washington, DC, and Norfolk, VA.\nCoastal cities and towns between New Haven and New London are also served by the Shore Line East commuter line. Several new stations were completed along the Connecticut shoreline recently, and a commuter rail service called the Hartford Line between New Haven and Springfield on Amtrak's New Haven-Springfield Line began operating in June 2018.\nA proposed commuter rail service, the Central Corridor Rail Line, will connect New London with Norwich, Willimantic, Storrs, and Stafford Springs, with service continuing into Massachusetts and Brattleboro. Amtrak also operates a shuttle service (CTRail) between New Haven and Springfield, Massachusetts, serving Wallingford, Meriden, Berlin, Hartford, Windsor Locks, and Springfield, MA and the Vermonter runs from Washington to St. Albans, Vermont via the same line.\nBus.\nStatewide bus service is supplied by Connecticut Transit, owned by the Connecticut Department of Transportation, with smaller municipal authorities providing local service. Bus networks are an important part of the transportation system in Connecticut, especially in urban areas like Hartford, Stamford, Norwalk, Bridgeport and New Haven. Connecticut Transit also operates CTfastrak, a bus rapid transit service between New Britain and Hartford. The bus route opened to the public on March 28, 2015.\nAir.\nBradley International Airport, is located in Windsor Locks, north of Hartford. Many residents of central and southern Connecticut also make heavy use of JFK International Airport and Newark International Airports, especially for international travel. Smaller regional air service is provided at Tweed New Haven Regional Airport. Larger civil airports include Danbury Municipal Airport and Waterbury-Oxford Airport in western Connecticut, Hartford\u2013Brainard Airport in central Connecticut, and Groton-New London Airport in eastern Connecticut. Sikorsky Memorial Airport is located in Stratford and mostly services cargo, helicopter and private aviation.\nFerry.\nThe Bridgeport &amp; Port Jefferson Ferry travels between Bridgeport, Connecticut and Port Jefferson, New York by crossing Long Island Sound. Ferry service also operates out of New London to Orient, New York; Fishers Island, New York; and Block Island, Rhode Island, which are popular tourist destinations. Small local services operate the Rocky Hill\u2013Glastonbury Ferry and the Chester\u2013Hadlyme Ferry which cross the Connecticut River.\nLaw and government.\nHartford has been the sole capital of Connecticut since 1875. Before then, New Haven and Hartford alternated as capitals.\nConstitutional history.\nConnecticut is known as the \"Constitution State\". The origin of this nickname is uncertain, but it likely comes from Connecticut's pivotal role in the federal constitutional convention of 1787, during which Roger Sherman and Oliver Ellsworth helped to orchestrate what became known as the Connecticut Compromise, or the Great Compromise. This plan combined the Virginia Plan and the New Jersey Plan to form a bicameral legislature, a form copied by almost every state constitution since the adoption of the federal constitution. Variations of the bicameral legislature had been proposed by Virginia and New Jersey, but Connecticut's plan was the one that was in effect until the early 20th century, when Senators ceased to be selected by their state legislatures and were instead directly elected. Otherwise, it is still the design of Congress.\nThe nickname also might refer to the Fundamental Orders of 1638\u201339. These Fundamental Orders represent the framework for the first formal Connecticut state government written by a representative body in Connecticut. The State of Connecticut government has operated under the direction of four separate documents in the course of the state's constitutional history. After the Fundamental Orders, Connecticut was granted governmental authority by King Charles II of England through the Connecticut Charter of 1662.\nSeparate branches of government did not exist during this period, and the General Assembly acted as the supreme authority. A constitution similar to the modern U.S. Constitution was not adopted in Connecticut until 1818. Finally, the current state constitution was implemented in 1965. The 1965 constitution absorbed a majority of its 1818 predecessor, but incorporated a handful of important modifications.\nExecutive.\nThe governor heads the executive branch. , Ned Lamont is the Governor and Susan Bysiewicz is the Lieutenant Governor; both are Democrats. From 1639 until the adoption of the 1818 constitution, the governor presided over the General Assembly. In 1974, Ella Grasso was elected as the governor of Connecticut. This was the first time in United States history when a woman was a governor without her husband being governor first.\nThere are several executive departments: Administrative Services, Agriculture, Banking, Children and Families, Consumer Protection, Correction, Economic and Community Development, Developmental Services, Construction Services, Education, Emergency Management and Public Protection, Energy &amp; Environmental Protection, Higher Education, Insurance, Labor, Mental Health and Addiction Services, Military, Motor Vehicles, Public Health, Public Utility Regulatory Authority, Public Works, Revenue Services, Social Services, Transportation, and Veterans Affairs. In addition to these departments, there are other independent bureaus, offices and commissions.\nIn addition to the Governor and Lieutenant Governor, there are four other executive officers named in the state constitution that are elected directly by voters: Secretary of the State, Treasurer, Comptroller, and Attorney General. All executive officers are elected to four-year terms.\nLegislative.\nThe legislature is the General Assembly. The General Assembly is a bicameral body consisting of an upper body, the State Senate (36 senators); and a lower body, the House of Representatives (151 representatives). Bills must pass each house in order to become law. The governor can veto the bill, but this veto can be overridden by a two-thirds majority in each house. Per Article XV of the state constitution, Senators and Representatives must be at least 18 years of age and are elected to two-year terms in November on even-numbered years. There also must always be between 30 and 50 senators and 125 to 225 representatives. The Lieutenant Governor presides over the Senate, except when absent from the chamber, when the President pro tempore presides. The Speaker of the House presides over the House. , Joe Aresimowicz is the Speaker of the House of Connecticut.\n, Connecticut's United States Senators are Richard Blumenthal (Democrat) and Chris Murphy (Democrat). Connecticut has five representatives in the U.S. House, all of whom are Democrats.\nLocally elected representatives also develop Local ordinances to govern cities and towns. The town ordinances often include noise control and zoning guidelines. However, the State of Connecticut also provides statewide ordinances for noise control as well.\nJudicial.\nThe highest court of Connecticut's judicial branch is the Connecticut Supreme Court, headed by the Chief Justice of Connecticut. The Supreme Court is responsible for deciding on the constitutionality of the law or cases as they relate to the law. Its proceedings are similar to those of the United States Supreme Court, with no testimony given by witnesses, and the lawyers of the two sides each present oral arguments no longer than thirty minutes. Following a court proceeding, the court may take several months to arrive at a judgment. the Chief Justice is Richard A. Robinson.\nIn 1818, the court became a separate entity, independent of the legislative and executive branches. The Appellate Court is a lesser statewide court and the Superior Courts are lower courts that resemble county courts of other states.\nThe State of Connecticut also offers access to Arrest warrant enforcement statistics through the Office of Policy and Management.\nLocal government.\nConnecticut does not have county government, unlike all other states except Rhode Island. Connecticut county governments were mostly eliminated in 1960, with the exception of sheriffs elected in each county. In 2000, the county sheriff was abolished and replaced with the state marshal system, which has districts that follow the old county territories. The judicial system is divided into judicial districts at the trial-court level which largely follow the old county lines. The eight counties are still widely used for purely geographical and statistical purposes, such as weather reports and census reporting.\nConnecticut shares with the rest of New England a governmental institution called the New England town. The state is divided into 169 towns which serve as the fundamental political jurisdictions. There are also 21 cities, most of which simply follow the boundaries of their namesake towns and have a merged city-town government. There are two exceptions: the City of Groton, which is a subsection of the Town of Groton, and the City of Winsted in the Town of Winchester. There are also nine incorporated boroughs which may provide additional services to a section of town. Naugatuck is a consolidated town and borough.\nThe state is also divided into nine regional councils of government defined by the state Office of Planning and Management, which facilitate regional planning and coordination of services between member towns. The Intragovernmental Policy Division of this Office coordinates regional planning with the administrative bodies of these regions. Each region has an administrative body made up chief executive officers of the member towns. The regions are established for the purpose of planning \"coordination of regional and state planning activities; redesignation of logical planning regions and promotion of the continuation of regional planning organizations within the state; and provision for technical aid and the administration of financial assistance to regional planning organizations\".\nPolitics.\nRegistered voters.\nConnecticut residents who register to vote may declare an affiliation to a political party, may become unaffiliated at will, and may change affiliations subject to certain waiting periods. around 59% of registered voters are enrolled (1.7% total in 28 third parties minor parties), and ratios among unaffiliated voters and the two major parties are about eight unaffiliated for every seven in the Democratic Party of Connecticut and for every four in the Connecticut Republican Party.\nMany Connecticut towns and cities show a marked preference for moderate candidates of either party.\nVoting.\nIn July 2009, the Connecticut legislature overrode a veto by Governor M. Jodi Rell to pass SustiNet, the first significant public-option health care reform legislation in the nation.\nIn April 2012, both houses of the Connecticut state legislature passed a bill (20 to 16 and 86 to 62) that abolished the capital punishment for all future crimes, while 11 inmates who were waiting on the death row at the time could still be executed.\nEducation.\nConnecticut ranked third in the nation for educational performance, according to Education Week's Quality Counts 2018 report. It earned an overall score of 83.5 out of 100 points. On average, the country received a score of 75.2.\nConnecticut posted a B-plus in the Chance-for-Success category, ranking fourth on factors that contribute to a person's success both within and outside the K-12 education system. Connecticut received a mark of B-plus and finished fourth for School Finance. It ranked 12th with a grade of C on the K-12 Achievement Index.\nK\u201312.\nPublic schools.\nThe Connecticut State Board of Education manages the public school system for children in grades K\u201312. Board of Education members are appointed by the Governor of Connecticut. Statistics for each school are made available to the public through an online database system called \"CEDAR\". The CEDAR database also provides statistics for \"ACES\" or \"RESC\" schools for children with behavioral disorders.\nColleges and universities.\nConnecticut was home to the nation's first law school, Litchfield Law School, which operated from 1773 to 1833 in Litchfield. Hartford Public High School (1638) is the third-oldest secondary school in the nation after the Collegiate School (1628) in Manhattan and the Boston Latin School (1635).\nPublic community colleges.\nThe state also has many noted private day schools, and its boarding schools draw students from around the world.\nSports.\nThere are two Connecticut teams in the American Hockey League. The Bridgeport Sound Tigers is a farm team for the New York Islanders which competes at the Webster Bank Arena in Bridgeport. The Hartford Wolf Pack is the affiliate of the New York Rangers; they play in the XL Center in Hartford.\nThe Hartford Yard Goats of the Eastern League are a AA affiliate of the Colorado Rockies. Also, the Norwich Sea Unicorns play in the New York-Penn League and are an A affiliate of the Detroit Tigers. The New Britain Bees play in the Atlantic League of Professional Baseball. The Connecticut Sun of the WNBA currently play at the Mohegan Sun Arena in Uncasville. In soccer, Hartford Athletic began play in the USL Championship in 2019, serving as the reserve team for the New England Revolution of Major League Soccer.\nThe state hosts several major sporting events. Since 1952, a PGA Tour golf tournament has been played in the Hartford area. It was originally called the \"Insurance City Open\" and later the \"Greater Hartford Open\" and is now known as the Travelers Championship. The Connecticut Open tennis tournament is held annually in the Cullman-Heyman Tennis Center at Yale University in New Haven.\nLime Rock Park in Salisbury is a road racing course, home to the International Motor Sports Association, SCCA, United States Auto Club, and K&amp;N Pro Series East races. Thompson International Speedway, Stafford Motor Speedway, and Waterford Speedbowl are oval tracks holding weekly races for NASCAR Modifieds and other classes, including the NASCAR Whelen Modified Tour. The state also hosts several major mixed martial arts events for Bellator MMA and the Ultimate Fighting Championship.\nProfessional sports teams.\nThe Hartford Whalers of the National Hockey League played in Hartford from 1975 to 1997 at the Hartford Civic Center. They departed to Raleigh, North Carolina, after disputes with the state over the construction of a new arena, and they are now known as the Carolina Hurricanes. In 1926, Hartford had a franchise in the National Football League known as the Hartford Blues. They joined the National League for one season in 1876, making them the state's only Major League baseball franchise before moving to Brooklyn, New York, and then disbanding one season later. From 2000 until 2006 the city was home to the Hartford FoxForce of World TeamTennis.\nCollege sports.\nThe Connecticut Huskies are the team of the University of Connecticut (UConn); they play NCAA Division I sports. Both the men's basketball and women's basketball teams have won multiple national championships. In 2004, UConn became the first school in NCAA DivisionI history to have its men's and women's basketball programs win the national title in the same year; they repeated the feat in 2014 and are still the only DivisionI school to win both titles in the same year. The UConn women's basketball team holds the record for the longest consecutive winning streak in NCAA college basketball at 111 games, a streak that ended in 2017. The UConn Huskies football team has played in the Football Bowl Subdivision since 2002, and has played in four bowl games.\nNew Haven biennially hosts \"The Game\" between the Yale Bulldogs and the Harvard Crimson, the country's second-oldest college football rivalry. Yale alumnus Walter Camp is deemed the \"Father of American Football\", and he helped develop modern football while living in New Haven. Other Connecticut universities which feature DivisionI sports teams are Quinnipiac University, Fairfield University, Central Connecticut State University, Sacred Heart University, and the University of Hartford.\nThe Constitution State Rivalry is an in-state college football rivalry between Sacred Heart University and Central Connecticut State University. Both teams compete at the NCAA Division 1 Football Championship Subdivision level in the Northeast Conference. Since 1998, the game has been played annually with the location of the matchup determined on a yearly basis.\nEtymology and symbols.\nThe name \"Connecticut\" originated with the Mohegan word \"quonehtacut\", meaning \"place of long tidal river\". Connecticut's official nickname is \"The Constitution State\", adopted in 1959 and based on its colonial constitution of 1638\u20131639 which was the first in America and, arguably, the world. Connecticut is also unofficially known as \"The Nutmeg State,\" whose origin is unknown. It may have come from its sailors returning from voyages with nutmeg, which was a very valuable spice in the 18th and 19th centuries. It may have originated in the early machined sheet tin nutmeg grinders sold by early Connecticut peddlers. It is also facetiously said to come from Yankee peddlers from Connecticut who would sell small carved nobs of wood shaped to look like nutmeg to unsuspecting customers. George Washington gave Connecticut the title of \"The Provisions State\" because of the material aid that the state rendered to the American Revolutionary War effort. Connecticut is also known as \"The Land of Steady Habits\".\nAccording to \"Webster's New International Dictionary\" (1993), a person who is a native or resident of Connecticut is a \"Connecticuter\". There are numerous other terms coined in print but not in use, such as \"Connecticotian\" (Cotton Mather in 1702) and \"Connecticutensian\" (Samuel Peters in 1781). Linguist Allen Walker Read suggests the more playful term \"connecticutie\". \"Nutmegger\" is sometimes used, as is \"Yankee\".\nThe official state song is \"Yankee Doodle\". The traditional abbreviation of the state's name is \"Conn\".; the official postal abbreviation is CT.\nCommemorative stamps issued by the United States Postal Service with Connecticut themes include Nathan Hale, Eugene O'Neill, Josiah Willard Gibbs, Noah Webster, Eli Whitney, the whaling ship the \"Charles W. Morgan\", which is docked at Mystic Seaport, and a decoy of a broadbill duck."}
{"id": "6468", "revid": "32389086", "url": "https://en.wikipedia.org/wiki?curid=6468", "title": "Country Liberal Party", "text": "The Country Liberal Party (CLP), officially the Country Liberals (Northern Territory), is a liberal conservative political party in Australia founded in 1974, which operates solely in the Northern Territory, however due to Christmas Island and the Cocos (Keeling) Islands forming part of the Division of Lingiari they also vote for the Country Liberal Party.\nThe CLP first fielded candidates at the 1975 federal election, winning one seat in the Senate and the non-voting seat in the House of Representatives. Since 1979, the CLP has been formally affiliated with both the federal Liberal Party of Australia and the National Party of Australia (previously the Country Party and National Country Party). The Liberal Party, National Party, Liberal National Party of Queensland, and CLP form the Coalition of Australian centre-right parties, with the CLP alone contesting seats for the Coalition in the Northern Territory. The CLP has full voting rights within the National Party, and observer status with the Liberal Party. Currently, the CLP has one representative in federal parliament, Senator Sam McMahon.\nThe CLP dominated the Northern Territory Legislative Assembly from its establishment in 1974 until the 2001 general election, when the CLP lost government winning only 10 of the 25 seats, and was reduced further to four parliamentary members at the 2005 election. At the 2008 election it increased its numbers, winning 11 seats.\nThe CLP returned to office following the 2012 election, winning 16 of 25 seats, and leader Terry Mills became Chief Minister of the Northern Territory. Less than a year later, Mills was replaced as Chief Minister and CLP leader by Adam Giles at the 2013 CLP leadership ballot on 13 March. Giles was the first indigenous Australian to lead a state or territory government in Australia. Giles was defeated at the 2015 CLP leadership ballot but managed to survive in the aftermath. Multiple defections saw the CLP reduced to minority government a few months later. At the 27 August 2016 Territory election, the CLP was resoundingly defeated, winning just two of 25 seats. Gary Higgins became CLP leader and opposition leader on 2 September, with Lia Finocchiaro as his deputy. On 20 January 2020, Higgins stood down as party leader and announced his retirement at the next election. Finocchiaro became CLP leader and leader of the opposition on 1 February 2020.\nHistory.\nOrigins.\nThe Territory Country Party members first contested the 1919 federal election, with a newly established federal Country Party contesting the 1922 federal election. The 1922 election saw the main opposition party to the Australian Labor Party, the Nationalist Party of Australia deprived of a majority, and were required to form a coalition in order to command a majority on the floor of parliament. The price for such support was the resignation of Nationalist (ex-Labor) Prime Minister, Billy Hughes, who was replaced by Stanley Bruce.\nIn 1922, the federal Division of Northern Territory was created, with one non-voting Member in the House of Representatives. Harold George Nelson was the inaugural member serving between 16 December 1922 and 15 September 1934. He was elected as an Independent but later joined the Australian Labor Party (ALP). Between 15 September 1934 and 10 December 1949 the Division of Northern Territory was held by Adair Blain, an independent member. Between 10 December 1949 and 31 October 1966 the Division was held by Jock Nelson, a member of the ALP. The Territory seat was won by the Country Party's Sam Calder at the 1966 federal election, who held the seat from 26 November 1966 to 19 September 1980.\nIn 1966, the Country Party was established in the Northern Territory, while the Liberal Party was a small party. In recognition of this, the local Liberals supported the Country Party's Calder for the sole NT seat from 1969 to 1972. An alliance had formed, primarily against the conservatives' main opponent, the ALP. After the gradual extension of limited voting rights, in 1968 the federal Coalition government gave the Member for Northern Territory full voting rights.\n1974\u20132001: Foundation and early dominance.\nAfter the 1974 federal election and the subsequent Joint Sitting of parliament, legislation was passed to give the Australian Capital Territory and the Northern Territory representation in the Australian Senate, with two senators being elected.\nThe Whitlam Government passed legislation in 1974 to establish a fully elected unicameral Northern Territory Legislative Assembly to replace the previous partly elected Northern Territory Legislative Council, which had been in existence since 1947. The term of the Legislative Assembly was four years. Initially, the Legislative Assembly consisted of 19 members, which was increased in 1982 to 25 members, the present number. The Northern Territory was granted self-government in 1978.\nFollowing the creation of the Legislative Assembly in 1974, the Territory's branches of the Country and Liberal parties merged to form the \"Country Liberal Party\" (CLP) to field candidates at the 1974 general election for the Legislative Assembly, going on to win 17 out of 19 seats. Calder was largely responsible for the push to unite the non-Labor forces in the Territory.\nThe CLP fielded candidates at the 1975 federal election, winning one seat each in the Senate and in the House of Representatives. Its first two federal MPs, Sam Calder and Bernie Kilgariff, both sat with the National Country Party (NCP) in federal parliament. However, on 3 February 1979 a special conference of the CLP resolved that \"the Federal CLP Parliamentarians be permitted to sit in the Party Rooms of their choice in Canberra\". Despite personal misgivings, Kilgariff chose to sit with the Liberal Party from 8 March 1979 in order that the CLP have representation in both parties, a practice which has been maintained where possible.\nThe CLP governed the Northern Territory from 1974 until the 2001 election. During this time, it never faced more than nine opposition members. Indeed, the CLP's dominance was so absolute that its internal politics were seen as a bigger threat than any opposition party. This was especially pronounced in the mid-1980s, when a series of party-room coups resulted in the Territory having three Chief Ministers in four years and also saw the creation of the Northern Territory Nationals as a short-lived splinter group under the leadership of former CLP chief minister Ian Tuxworth.\n2001\u20132012: In opposition.\nAt the 2001 election the Australian Labor Party won government by one seat, ending 27 years of CLP government. The loss marked a major turning point in Northern Territory politics, a result which was exacerbated when, at the 2005 election, the ALP won the second-largest majority government in the history of the Territory, reducing the once-dominant party to just four members in the Legislative Assembly. This result was only outdone by the 1974 election, in which the CLP faced only two independents as opposition. The CLP even lost two seats in Palmerston, an area where the ALP had never come close to winning any seats before.\nIn the 2001 federal election, the CLP won the newly formed seat of Solomon, based on Darwin/Palmerston, in the House of Representatives.\nIn the 2004 federal election, the CLP held one seat in the House of Representatives, and one seat in the Senate. The CLP lost its federal lower house seat in the 2007 federal election, but regained it when Palmerston deputy mayor Natasha Griggs won back Solomon for the CLP. She sat with the Liberals in the House.\nThe 2008 election saw the CLP recover from the severe loss it suffered three years earlier, increasing its representation from four to 11 members. Following the 2011 decision of ALP-turned-independent member Alison Anderson to join the CLP, this increased CLP's representation to 12 in the Assembly, leaving the incumbent Henderson Government to govern in minority with the support of Independent MP Gerry Wood.\nHistorically, the CLP has been particularly dominant in the Territory's two major cities, Darwin/Palmerston and Alice Springs. However, in recent years the ALP has pulled even with the CLP in the Darwin area; indeed, its 2001 victory was fueled by an unexpected swing in Darwin.\n2012\u20132016: Return to government and internal conflict.\nThe CLP under the leadership of Terry Mills returned to power in the 2012 election with 16 of 25 seats, defeating the incumbent Labor Government led by Paul Henderson. In the lead up to the Territory election, CLP Senator Nigel Scullion sharply criticised the Federal Labor Government for its suspension of the live cattle trade to Indonesia - an economic mainstay of the territory.\nThe election victory ended 11 years of ALP rule in the Northern Territory. The victory was also notable for the support it achieved from indigenous people in pastoral and remote electorates. Large swings were achieved in remote Territory electorates (where the indigenous population comprised around two-thirds of voters) and a total of five Aboriginal CLP candidates won election to the Assembly. Among the indigenous candidates elected were high-profile Aboriginal activist Bess Price and former ALP member Alison Anderson. Anderson was appointed Minister for Indigenous Advancement. In a nationally reported speech in November 2012, Anderson condemned welfare dependency and a culture of entitlement in her first ministerial statement on the status of Aboriginal communities in the Territory and said the CLP would focus on improving education and on helping create real jobs for indigenous people.\nLeadership spills.\nAdam Giles replaced Mills as Chief Minister of the Northern Territory and party leader at the 2013 CLP leadership ballot on 13 March while Mills was on a trade mission in Japan. Giles was sworn in as Chief Minister on 14 March, becoming the first indigenous head of government of an Australian state or territory.\nWillem Westra van Holthe challenged Giles at the 2015 CLP leadership ballot on 2 February and was elected leader by the party room in a late night vote conducted by phone. However, Giles refused to resign as Chief Minister following the vote. On 3 February, \"ABC News\" reported that officials were preparing an instrument for Giles' removal by the Administrator. The swearing-in of Westra van Holthe, which had been scheduled for 11:00 local time (01:30 UTC), was delayed. After a meeting of the parliamentary wing of the CLP, Giles announced that he would remain as party leader and Chief Minister, and that Westra van Holthe would be his deputy.\nDefections and minority government.\nAfter four defections during the parliamentary term, the CLP was reduced to minority government by July 2015. Giles raised the possibility of an early election on 20 July stating that he would \"love\" to call a snap poll, but that it was \"pretty much impossible to do\". Crossbenchers dismissed the notion of voting against a confidence motion to bring down the government.\n2016\u2013present: In opposition.\nTerritory government legislation passed in February 2016 changed the voting method of single-member electorates from full-preferential voting to optional preferential voting ahead of the 2016 territory election held on 27 August.\nFederally, a MediaReach seat-level opinion poll of 513 voters in the seat of Solomon conducted 22\u221223 June ahead of the 2016 federal election held on 2 July surprisingly found Labor candidate Luke Gosling heavily leading two-term CLP incumbent Natasha Griggs 61\u201339 on the two-party vote from a large 12.4 percent swing. The CLP lost Solomon to Labor at the election, with Gosling defeating Griggs 56\u201344 on the two-party vote from a 7.4 percent swing.\nPolling ahead of the 2016 Territory election indicated a large swing against the CLP, including a near-total collapse in Darwin/Palmerston. By the time the writs were dropped, commentators had almost universally written off the CLP. At 27 August Territory election, the CLP was swept from power in a massive Labor landslide, suffering easily the worst defeat of a sitting government in Territory history and one of the worst defeats a governing party has ever suffered at the state or territory level in Australia. The party not only lost all of the bush seats it picked up in 2012, but was all but shut out of Darwin/Palmerston, winning only one seat there. All told, the CLP only won two seats, easily its worst showing in an election. Giles himself lost his own seat, becoming the second Majority Leader/Chief Minister to lose his own seat. Even before Giles' defeat was confirmed, second-term MP Gary Higgins\u2014the only surviving member of the Giles cabinet\u2014was named the party's new leader, with Lia Finocchiaro as his deputy. On 20 January 2020, Higgins announced his resignation as party leader and announced his retirement at the next election. Finocchiaro succeeded him as CLP leader and leader of the opposition on 1 February 2020.\nFinocchiaro led the CLP to a modest recovery at the 2020 Territory election. The CLP picked up a six-seat swing, boosting its seat count to eight. However, it failed to make significant inroads in Darwin/Palmerston, winning only two seats there, including that of Finocchiaro.\nIdeology.\nThe CLP stands for office in the Northern Territory Assembly and Federal Parliament of Australia and primarily concerns itself with representing Territory interests. It is a regionally based party, that has parliamentary representation in both the Federal Parliament and at the Territory level. It brands as a party with strong roots in the Territory.\nThe CLP competes against the Australian Labor Party (Northern Territory Branch) (the local branch of Australia's social-democratic party). It is closely affiliated with, but is independent from the Liberal Party of Australia (a mainly urban, pro-business party comprising mainly liberal membership) and the National Party of Australia (a conservative agrarian and regional interests party).\nThe party promotes traditional Liberal Party values such as individualism and private enterprise, and what it describes as \"progressive\" political policy such as full statehood for the Northern Territory.\nOrganisation.\nBranch delegates and members of the party's Central Council attend the Annual Conference of the Country Liberal Party to decide the party's platform. The Central Council is composed of the party's office bearers, its leaders from the Territory Assembly and the Federal Parliament and representatives of party branches.\nThe Annual Conference of the Country Liberal Party, attended by branch delegates and members of the party's Central Council, decides matters relating to the party's platform and philosophy. The Central Council administers the party and makes decisions on pre-selections. It is composed of the party's office bearers, its leaders in the Northern Territory Legislative Assembly, members in the Federal Parliament, and representation from each of the party's branches.\nThe CLP president has full voting rights with the National Party and observer status with the Liberal Party. Both the Liberals and Nationals receive Country Liberal delegations at their conventions. After federal elections, the CLP directs its federal members and senators as to which of the two other parties they should sit with in the parliamentary chamber. In practice, CLP House members usually sit with the Liberals, while CLP Senators sit with the Nationals."}
